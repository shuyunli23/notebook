# å¼ºåŒ–å­¦ä¹ å®Œæ•´ç¬”è®°

## ç›®å½•
1. [åŸºç¡€æ¦‚å¿µ](#ä¸€åŸºç¡€æ¦‚å¿µ)
2. [é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹](#äºŒé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹mdp)
3. [åŠ¨æ€è§„åˆ’](#ä¸‰åŠ¨æ€è§„åˆ’)
4. [è’™ç‰¹å¡æ´›æ–¹æ³•](#å››è’™ç‰¹å¡æ´›æ–¹æ³•)
5. [æ—¶åºå·®åˆ†å­¦ä¹ ](#äº”æ—¶åºå·®åˆ†å­¦ä¹ )
6. [å€¼å‡½æ•°é€¼è¿‘](#å…­å€¼å‡½æ•°é€¼è¿‘)
7. [ç­–ç•¥æ¢¯åº¦æ–¹æ³•](#ä¸ƒç­–ç•¥æ¢¯åº¦æ–¹æ³•)
8. [Actor-Critic](#å…«actor-criticæ–¹æ³•)
9. [æ·±åº¦å¼ºåŒ–å­¦ä¹ ](#ä¹æ·±åº¦å¼ºåŒ–å­¦ä¹ )
10. [é«˜çº§ç®—æ³•](#åé«˜çº§ç®—æ³•)

---

## ä¸€ã€åŸºç¡€æ¦‚å¿µ

### 1.1 å¼ºåŒ–å­¦ä¹ ç®€ä»‹
```
å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning, RLï¼‰æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œ
ç ”ç©¶æ™ºèƒ½ä½“ï¼ˆAgentï¼‰å¦‚ä½•åœ¨ç¯å¢ƒï¼ˆEnvironmentï¼‰ä¸­é€šè¿‡è¯•é”™å­¦ä¹ ï¼Œ
ä»¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚

æ ¸å¿ƒç‰¹ç‚¹ï¼š
- å»¶è¿Ÿå¥–åŠ±ï¼šè¡ŒåŠ¨çš„ç»“æœå¯èƒ½åœ¨å¾ˆä¹…ä¹‹åæ‰æ˜¾ç°
- æ¢ç´¢ä¸åˆ©ç”¨ï¼šéœ€è¦å¹³è¡¡å°è¯•æ–°ç­–ç•¥å’Œåˆ©ç”¨å·²çŸ¥å¥½ç­–ç•¥
- åºè´¯å†³ç­–ï¼šå½“å‰å†³ç­–å½±å“æœªæ¥çŠ¶æ€
```

### 1.2 åŸºæœ¬è¦ç´ 
```python
# å¼ºåŒ–å­¦ä¹ çš„äº”å…ƒç»„
class RLSystem:
    def __init__(self):
        # çŠ¶æ€ç©ºé—´ (State Space)
        self.states = S
        
        # åŠ¨ä½œç©ºé—´ (Action Space)
        self.actions = A
        
        # å¥–åŠ±å‡½æ•° (Reward Function)
        # R: S Ã— A â†’ â„
        self.reward_function = R
        
        # çŠ¶æ€è½¬ç§»æ¦‚ç‡ (Transition Probability)
        # P: S Ã— A Ã— S â†’ [0,1]
        self.transition_prob = P
        
        # æŠ˜æ‰£å› å­ (Discount Factor)
        # Î³ âˆˆ [0,1]
        self.gamma = 0.99

# æ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’
def agent_environment_interaction():
    """
    æ™ºèƒ½ä½“-ç¯å¢ƒäº¤äº’å¾ªç¯ï¼š
    1. æ™ºèƒ½ä½“è§‚å¯ŸçŠ¶æ€ s_t
    2. æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ a_t
    3. ç¯å¢ƒè¿”å›å¥–åŠ± r_t å’Œæ–°çŠ¶æ€ s_{t+1}
    4. é‡å¤
    """
    state = env.reset()
    
    for t in range(max_steps):
        # æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ
        action = agent.select_action(state)
        
        # ç¯å¢ƒå“åº”
        next_state, reward, done, info = env.step(action)
        
        # æ™ºèƒ½ä½“å­¦ä¹ 
        agent.learn(state, action, reward, next_state, done)
        
        state = next_state
        
        if done:
            break
```

### 1.3 å›æŠ¥ä¸ä»·å€¼å‡½æ•°
```python
import numpy as np

# 1. å›æŠ¥ (Return)
def compute_return(rewards, gamma=0.99):
    """
    è®¡ç®—ç´¯ç§¯æŠ˜æ‰£å›æŠ¥
    G_t = r_t + Î³r_{t+1} + Î³Â²r_{t+2} + ... = Î£ Î³^k * r_{t+k}
    """
    G = 0
    for t in range(len(rewards)-1, -1, -1):
        G = rewards[t] + gamma * G
    return G

# ç¤ºä¾‹
rewards = [1, 0, 0, 1, 1]
print(f"å›æŠ¥: {compute_return(rewards)}")  # 2.9701

# 2. çŠ¶æ€ä»·å€¼å‡½æ•° (State Value Function)
def state_value_function():
    """
    V^Ï€(s) = ğ”¼_Ï€[G_t | s_t = s]
           = ğ”¼_Ï€[r_t + Î³V^Ï€(s_{t+1}) | s_t = s]
    
    è¡¨ç¤ºåœ¨çŠ¶æ€sä¸‹ï¼Œéµå¾ªç­–ç•¥Ï€èƒ½è·å¾—çš„æœŸæœ›å›æŠ¥
    """
    pass

# 3. åŠ¨ä½œä»·å€¼å‡½æ•° (Action Value Function)
def action_value_function():
    """
    Q^Ï€(s,a) = ğ”¼_Ï€[G_t | s_t = s, a_t = a]
             = ğ”¼[r_t + Î³Q^Ï€(s_{t+1}, a_{t+1}) | s_t = s, a_t = a]
    
    è¡¨ç¤ºåœ¨çŠ¶æ€sä¸‹é‡‡å–åŠ¨ä½œaï¼Œç„¶åéµå¾ªç­–ç•¥Ï€çš„æœŸæœ›å›æŠ¥
    """
    pass

# 4. ä¼˜åŠ¿å‡½æ•° (Advantage Function)
def advantage_function(Q, V, state, action):
    """
    A^Ï€(s,a) = Q^Ï€(s,a) - V^Ï€(s)
    
    è¡¨ç¤ºåœ¨çŠ¶æ€sä¸‹é‡‡å–åŠ¨ä½œaç›¸æ¯”å¹³å‡æ°´å¹³çš„ä¼˜åŠ¿
    """
    return Q[state, action] - V[state]
```

### 1.4 ç­–ç•¥
```python
# 1. ç¡®å®šæ€§ç­–ç•¥ (Deterministic Policy)
class DeterministicPolicy:
    """
    Ï€: S â†’ A
    æ¯ä¸ªçŠ¶æ€æ˜ å°„åˆ°å”¯ä¸€çš„åŠ¨ä½œ
    """
    def __init__(self, policy_dict):
        self.policy = policy_dict
    
    def select_action(self, state):
        return self.policy[state]

# 2. éšæœºç­–ç•¥ (Stochastic Policy)
class StochasticPolicy:
    """
    Ï€: S Ã— A â†’ [0,1]
    Ï€(a|s) è¡¨ç¤ºåœ¨çŠ¶æ€sä¸‹é€‰æ‹©åŠ¨ä½œaçš„æ¦‚ç‡
    """
    def __init__(self, policy_probs):
        self.policy = policy_probs
    
    def select_action(self, state):
        actions = list(self.policy[state].keys())
        probs = list(self.policy[state].values())
        return np.random.choice(actions, p=probs)

# 3. Îµ-è´ªå¿ƒç­–ç•¥ (Îµ-greedy Policy)
class EpsilonGreedyPolicy:
    """
    ä»¥æ¦‚ç‡Îµéšæœºæ¢ç´¢ï¼Œä»¥æ¦‚ç‡1-Îµé€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
    """
    def __init__(self, Q, epsilon=0.1):
        self.Q = Q
        self.epsilon = epsilon
    
    def select_action(self, state, actions):
        if np.random.random() < self.epsilon:
            return np.random.choice(actions)  # æ¢ç´¢
        else:
            return np.argmax(self.Q[state])   # åˆ©ç”¨

# 4. Softmaxç­–ç•¥ (Boltzmannç­–ç•¥)
class SoftmaxPolicy:
    """
    Ï€(a|s) = exp(Q(s,a)/Ï„) / Î£_a' exp(Q(s,a')/Ï„)
    Ï„æ˜¯æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶æ¢ç´¢ç¨‹åº¦
    """
    def __init__(self, Q, temperature=1.0):
        self.Q = Q
        self.tau = temperature
    
    def select_action(self, state, actions):
        q_values = self.Q[state]
        probs = np.exp(q_values / self.tau)
        probs = probs / np.sum(probs)
        return np.random.choice(actions, p=probs)
```

### 1.5 æ¢ç´¢ç­–ç•¥
```python
# 1. Îµè¡°å‡
class EpsilonDecay:
    """éšç€è®­ç»ƒè¿›è¡Œï¼Œå‡å°‘æ¢ç´¢"""
    def __init__(self, epsilon_start=1.0, epsilon_end=0.01, decay_steps=10000):
        self.epsilon_start = epsilon_start
        self.epsilon_end = epsilon_end
        self.decay_steps = decay_steps
    
    def get_epsilon(self, step):
        epsilon = self.epsilon_start - (self.epsilon_start - self.epsilon_end) * \
                  min(step / self.decay_steps, 1.0)
        return epsilon

# 2. ä¸Šç½®ä¿¡ç•Œ (UCB - Upper Confidence Bound)
class UCBExploration:
    """
    é€‰æ‹© a = argmax_a [Q(s,a) + c * sqrt(ln(t) / N(s,a))]
    å¹³è¡¡åˆ©ç”¨ï¼ˆQå€¼ï¼‰å’Œæ¢ç´¢ï¼ˆè®¿é—®æ¬¡æ•°å°‘çš„åŠ¨ä½œï¼‰
    """
    def __init__(self, c=2.0):
        self.c = c
        self.N = {}  # è®¿é—®è®¡æ•°
        self.t = 0   # æ€»æ­¥æ•°
    
    def select_action(self, state, Q, actions):
        self.t += 1
        if state not in self.N:
            self.N[state] = {a: 0 for a in actions}
        
        ucb_values = []
        for a in actions:
            if self.N[state][a] == 0:
                return a  # ä¼˜å…ˆé€‰æ‹©æœªè®¿é—®çš„åŠ¨ä½œ
            
            ucb = Q[state][a] + self.c * np.sqrt(np.log(self.t) / self.N[state][a])
            ucb_values.append(ucb)
        
        action = actions[np.argmax(ucb_values)]
        self.N[state][action] += 1
        return action

# 3. Thompsoné‡‡æ ·
class ThompsonSampling:
    """
    åŸºäºè´å¶æ–¯æ¨æ–­çš„æ¢ç´¢ç­–ç•¥
    ä¸ºæ¯ä¸ªåŠ¨ä½œç»´æŠ¤ä¸€ä¸ªåˆ†å¸ƒï¼Œä»åˆ†å¸ƒä¸­é‡‡æ ·
    """
    def __init__(self):
        self.alpha = {}  # æˆåŠŸè®¡æ•°
        self.beta = {}   # å¤±è´¥è®¡æ•°
    
    def select_action(self, state, actions):
        if state not in self.alpha:
            self.alpha[state] = {a: 1 for a in actions}
            self.beta[state] = {a: 1 for a in actions}
        
        samples = {}
        for a in actions:
            # ä»Betaåˆ†å¸ƒé‡‡æ ·
            samples[a] = np.random.beta(self.alpha[state][a], self.beta[state][a])
        
        return max(samples, key=samples.get)
    
    def update(self, state, action, reward):
        if reward > 0:
            self.alpha[state][action] += 1
        else:
            self.beta[state][action] += 1
```

---

## äºŒã€é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)

### 2.1 MDPå®šä¹‰
```python
class MDP:
    """
    é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹äº”å…ƒç»„: (S, A, P, R, Î³)
    
    - S: çŠ¶æ€ç©ºé—´
    - A: åŠ¨ä½œç©ºé—´
    - P: çŠ¶æ€è½¬ç§»æ¦‚ç‡ P(s'|s,a)
    - R: å¥–åŠ±å‡½æ•° R(s,a,s')
    - Î³: æŠ˜æ‰£å› å­
    """
    def __init__(self, states, actions, transitions, rewards, gamma=0.99):
        self.states = states
        self.actions = actions
        self.P = transitions  # P[s][a][s'] = æ¦‚ç‡
        self.R = rewards      # R[s][a][s'] = å¥–åŠ±
        self.gamma = gamma
    
    def get_transition_prob(self, state, action, next_state):
        """è·å–çŠ¶æ€è½¬ç§»æ¦‚ç‡"""
        return self.P[state][action].get(next_state, 0)
    
    def get_reward(self, state, action, next_state):
        """è·å–å¥–åŠ±"""
        return self.R[state][action].get(next_state, 0)

# ç¤ºä¾‹ï¼šç½‘æ ¼ä¸–ç•ŒMDP
class GridWorldMDP:
    """
    ç®€å•çš„4Ã—4ç½‘æ ¼ä¸–ç•Œ
    ç›®æ ‡ï¼šä»èµ·ç‚¹(0,0)åˆ°è¾¾ç»ˆç‚¹(3,3)
    """
    def __init__(self):
        self.grid_size = 4
        self.actions = ['up', 'down', 'left', 'right']
        self.gamma = 0.9
        
    def get_next_state(self, state, action):
        """ç¡®å®šæ€§çŠ¶æ€è½¬ç§»"""
        x, y = state
        
        if action == 'up':
            x = max(0, x - 1)
        elif action == 'down':
            x = min(self.grid_size - 1, x + 1)
        elif action == 'left':
            y = max(0, y - 1)
        elif action == 'right':
            y = min(self.grid_size - 1, y + 1)
        
        return (x, y)
    
    def get_reward(self, state, action, next_state):
        """å¥–åŠ±å‡½æ•°"""
        if next_state == (3, 3):  # ç»ˆç‚¹
            return 1.0
        return -0.01  # æ¯æ­¥å°æƒ©ç½šï¼Œé¼“åŠ±å¿«é€Ÿåˆ°è¾¾ç»ˆç‚¹
    
    def is_terminal(self, state):
        """åˆ¤æ–­æ˜¯å¦ä¸ºç»ˆæ­¢çŠ¶æ€"""
        return state == (3, 3)
```

### 2.2 è´å°”æ›¼æ–¹ç¨‹
```python
# 1. è´å°”æ›¼æœŸæœ›æ–¹ç¨‹ (Bellman Expectation Equation)
def bellman_expectation_v(V, mdp, policy, state):
    """
    çŠ¶æ€ä»·å€¼å‡½æ•°çš„è´å°”æ›¼æœŸæœ›æ–¹ç¨‹
    V^Ï€(s) = Î£_a Ï€(a|s) Î£_{s'} P(s'|s,a)[R(s,a,s') + Î³V^Ï€(s')]
    """
    value = 0
    for action in mdp.actions:
        action_prob = policy.get_action_prob(state, action)
        
        for next_state in mdp.states:
            trans_prob = mdp.get_transition_prob(state, action, next_state)
            reward = mdp.get_reward(state, action, next_state)
            
            value += action_prob * trans_prob * (reward + mdp.gamma * V[next_state])
    
    return value

def bellman_expectation_q(Q, mdp, policy, state, action):
    """
    åŠ¨ä½œä»·å€¼å‡½æ•°çš„è´å°”æ›¼æœŸæœ›æ–¹ç¨‹
    Q^Ï€(s,a) = Î£_{s'} P(s'|s,a)[R(s,a,s') + Î³ Î£_{a'} Ï€(a'|s')Q^Ï€(s',a')]
    """
    value = 0
    for next_state in mdp.states:
        trans_prob = mdp.get_transition_prob(state, action, next_state)
        reward = mdp.get_reward(state, action, next_state)
        
        next_value = 0
        for next_action in mdp.actions:
            action_prob = policy.get_action_prob(next_state, next_action)
            next_value += action_prob * Q[next_state, next_action]
        
        value += trans_prob * (reward + mdp.gamma * next_value)
    
    return value

# 2. è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ (Bellman Optimality Equation)
def bellman_optimality_v(V, mdp, state):
    """
    æœ€ä¼˜çŠ¶æ€ä»·å€¼å‡½æ•°
    V*(s) = max_a Î£_{s'} P(s'|s,a)[R(s,a,s') + Î³V*(s')]
    """
    max_value = float('-inf')
    
    for action in mdp.actions:
        value = 0
        for next_state in mdp.states:
            trans_prob = mdp.get_transition_prob(state, action, next_state)
            reward = mdp.get_reward(state, action, next_state)
            value += trans_prob * (reward + mdp.gamma * V[next_state])
        
        max_value = max(max_value, value)
    
    return max_value

def bellman_optimality_q(Q, mdp, state, action):
    """
    æœ€ä¼˜åŠ¨ä½œä»·å€¼å‡½æ•°
    Q*(s,a) = Î£_{s'} P(s'|s,a)[R(s,a,s') + Î³ max_{a'} Q*(s',a')]
    """
    value = 0
    
    for next_state in mdp.states:
        trans_prob = mdp.get_transition_prob(state, action, next_state)
        reward = mdp.get_reward(state, action, next_state)
        max_next_q = max(Q[next_state, a] for a in mdp.actions)
        value += trans_prob * (reward + mdp.gamma * max_next_q)
    
    return value
```

### 2.3 é©¬å°”å¯å¤«æ€§è´¨
```python
class MarkovProperty:
    """
    é©¬å°”å¯å¤«æ€§è´¨ï¼šæœªæ¥åªä¾èµ–äºå½“å‰çŠ¶æ€ï¼Œä¸å†å²æ— å…³
    P(s_{t+1} | s_t, s_{t-1}, ..., s_0) = P(s_{t+1} | s_t)
    """
    
    @staticmethod
    def check_markov_property(trajectory, transition_counts):
        """
        æ£€æŸ¥è½¨è¿¹æ˜¯å¦æ»¡è¶³é©¬å°”å¯å¤«æ€§è´¨
        é€šè¿‡æ¯”è¾ƒ P(s'|s) å’Œ P(s'|s,history)
        """
        # è®¡ç®—å•æ­¥è½¬ç§»æ¦‚ç‡
        single_step = {}
        for i in range(len(trajectory) - 1):
            s, s_next = trajectory[i], trajectory[i+1]
            if s not in single_step:
                single_step[s] = {}
            single_step[s][s_next] = single_step[s].get(s_next, 0) + 1
        
        # å½’ä¸€åŒ–
        for s in single_step:
            total = sum(single_step[s].values())
            for s_next in single_step[s]:
                single_step[s][s_next] /= total
        
        # è®¡ç®—å¸¦å†å²çš„è½¬ç§»æ¦‚ç‡
        history_step = {}
        for i in range(2, len(trajectory)):
            history = tuple(trajectory[:i])
            s_next = trajectory[i]
            if history not in history_step:
                history_step[history] = {}
            history_step[history][s_next] = history_step[history].get(s_next, 0) + 1
        
        # æ¯”è¾ƒå·®å¼‚
        # ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦æ›´ä¸¥æ ¼çš„ç»Ÿè®¡æµ‹è¯•ï¼‰
        return single_step, history_step

# éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (POMDP)
class POMDP:
    """
    å½“æ™ºèƒ½ä½“æ— æ³•å®Œå…¨è§‚æµ‹çŠ¶æ€æ—¶ï¼Œä½¿ç”¨POMDP
    
    ä¸ƒå…ƒç»„: (S, A, P, R, Î©, O, Î³)
    - Î©: è§‚æµ‹ç©ºé—´
    - O: è§‚æµ‹æ¦‚ç‡ O(o|s,a)
    """
    def __init__(self, states, actions, observations, 
                 transitions, rewards, obs_probs, gamma=0.99):
        self.states = states
        self.actions = actions
        self.observations = observations
        self.P = transitions
        self.R = rewards
        self.O = obs_probs  # O[s][a][o] = æ¦‚ç‡
        self.gamma = gamma
        
        # ä¿¡å¿µçŠ¶æ€ (Belief State)
        self.belief = self.initialize_belief()
    
    def initialize_belief(self):
        """åˆå§‹åŒ–å‡åŒ€ä¿¡å¿µ"""
        n = len(self.states)
        return {s: 1.0/n for s in self.states}
    
    def update_belief(self, belief, action, observation):
        """
        è´å¶æ–¯ä¿¡å¿µæ›´æ–°
        b'(s') âˆ O(o|s',a) Î£_s P(s'|s,a)b(s)
        """
        new_belief = {}
        
        for s_next in self.states:
            prob = 0
            for s in self.states:
                prob += self.P[s][action].get(s_next, 0) * belief[s]
            prob *= self.O[s_next][action].get(observation, 0)
            new_belief[s_next] = prob
        
        # å½’ä¸€åŒ–
        total = sum(new_belief.values())
        if total > 0:
            new_belief = {s: p/total for s, p in new_belief.items()}
        
        return new_belief
```

---

## ä¸‰ã€åŠ¨æ€è§„åˆ’

### 3.1 ç­–ç•¥è¯„ä¼°
```python
import numpy as np

class PolicyEvaluation:
    """
    ç­–ç•¥è¯„ä¼°ï¼šè®¡ç®—ç»™å®šç­–ç•¥Ï€çš„ä»·å€¼å‡½æ•°V^Ï€
    ä½¿ç”¨è¿­ä»£æ–¹æ³•æ±‚è§£è´å°”æ›¼æœŸæœ›æ–¹ç¨‹
    """
    def __init__(self, mdp, policy, theta=1e-6):
        self.mdp = mdp
        self.policy = policy
        self.theta = theta  # æ”¶æ•›é˜ˆå€¼
    
    def evaluate(self, max_iterations=1000):
        """
        è¿­ä»£ç­–ç•¥è¯„ä¼°
        V_{k+1}(s) = Î£_a Ï€(a|s) Î£_{s'} P(s'|s,a)[R(s,a,s') + Î³V_k(s')]
        """
        # åˆå§‹åŒ–ä»·å€¼å‡½æ•°
        V = {s: 0 for s in self.mdp.states}
        
        for iteration in range(max_iterations):
            delta = 0
            new_V = V.copy()
            
            for state in self.mdp.states:
                if self.mdp.is_terminal(state):
                    continue
                
                v = V[state]
                
                # è´å°”æ›¼æœŸæœ›æ›´æ–°
                new_value = 0
                for action in self.mdp.actions:
                    action_prob = self.policy.get_action_prob(state, action)
                    
                    for next_state in self.mdp.states:
                        trans_prob = self.mdp.get_transition_prob(state, action, next_state)
                        reward = self.mdp.get_reward(state, action, next_state)
                        new_value += action_prob * trans_prob * \
                                   (reward + self.mdp.gamma * V[next_state])
                
                new_V[state] = new_value
                delta = max(delta, abs(v - new_value))
            
            V = new_V
            
            # æ£€æŸ¥æ”¶æ•›
            if delta < self.theta:
                print(f"ç­–ç•¥è¯„ä¼°æ”¶æ•›äºç¬¬ {iteration+1} æ¬¡è¿­ä»£")
                break
        
        return V

# ç¤ºä¾‹ï¼šç½‘æ ¼ä¸–ç•Œç­–ç•¥è¯„ä¼°
def example_policy_evaluation():
    # åˆ›å»º4Ã—4ç½‘æ ¼ä¸–ç•Œ
    mdp = GridWorldMDP()
    
    # å®šä¹‰éšæœºç­–ç•¥ï¼ˆæ¯ä¸ªæ–¹å‘æ¦‚ç‡ç›¸ç­‰ï¼‰
    policy = UniformPolicy(mdp.actions)
    
    # è¯„ä¼°ç­–ç•¥
    evaluator = PolicyEvaluation(mdp, policy)
    V = evaluator.evaluate()
    
    # æ‰“å°ä»·å€¼å‡½æ•°
    print("çŠ¶æ€ä»·å€¼å‡½æ•°:")
    for i in range(4):
        for j in range(4):
            print(f"{V[(i,j)]:.2f}", end="  ")
        print()
```

### 3.2 ç­–ç•¥æ”¹è¿›
```python
class PolicyImprovement:
    """
    ç­–ç•¥æ”¹è¿›ï¼šæ ¹æ®ä»·å€¼å‡½æ•°æ”¹è¿›ç­–ç•¥
    Ï€'(s) = argmax_a Q^Ï€(s,a)
    """
    def __init__(self, mdp):
        self.mdp = mdp
    
    def improve(self, V):
        """
        è´ªå¿ƒç­–ç•¥æ”¹è¿›
        """
        new_policy = {}
        policy_stable = True
        
        for state in self.mdp.states:
            if self.mdp.is_terminal(state):
                continue
            
            # è®¡ç®—æ¯ä¸ªåŠ¨ä½œçš„Qå€¼
            q_values = {}
            for action in self.mdp.actions:
                q = 0
                for next_state in self.mdp.states:
                    trans_prob = self.mdp.get_transition_prob(state, action, next_state)
                    reward = self.mdp.get_reward(state, action, next_state)
                    q += trans_prob * (reward + self.mdp.gamma * V[next_state])
                q_values[action] = q
            
            policy[state] = max(q_values, key=q_values.get)
        
        return policy

# å¼‚æ­¥åŠ¨æ€è§„åˆ’
class AsynchronousDP:
    """
    å¼‚æ­¥åŠ¨æ€è§„åˆ’ï¼šä¸éœ€è¦å®Œæ•´æ‰«ææ‰€æœ‰çŠ¶æ€
    - åŸåœ°æ›´æ–° (In-place)
    - ä¼˜å…ˆçº§æ‰«æ
    - å®æ—¶åŠ¨æ€è§„åˆ’
    """
    def __init__(self, mdp):
        self.mdp = mdp
    
    def prioritized_sweeping(self, V, theta=1e-6, max_iterations=1000):
        """
        ä¼˜å…ˆçº§æ‰«æï¼šä¼˜å…ˆæ›´æ–°Bellmanè¯¯å·®å¤§çš„çŠ¶æ€
        """
        import heapq
        
        # åˆå§‹åŒ–ä¼˜å…ˆé˜Ÿåˆ—ï¼ˆæœ€å¤§å †ï¼Œç”¨è´Ÿå€¼å®ç°ï¼‰
        priority_queue = []
        
        # è®¡ç®—åˆå§‹ä¼˜å…ˆçº§
        for state in self.mdp.states:
            if not self.mdp.is_terminal(state):
                bellman_error = self.compute_bellman_error(state, V)
                if bellman_error > theta:
                    heapq.heappush(priority_queue, (-bellman_error, state))
        
        for iteration in range(max_iterations):
            if not priority_queue:
                break
            
            # å–å‡ºè¯¯å·®æœ€å¤§çš„çŠ¶æ€
            _, state = heapq.heappop(priority_queue)
            
            # æ›´æ–°è¯¥çŠ¶æ€
            old_value = V[state]
            V[state] = self.bellman_backup(state, V)
            
            # æ›´æ–°å‰é©±çŠ¶æ€çš„ä¼˜å…ˆçº§
            for prev_state in self.get_predecessors(state):
                if not self.mdp.is_terminal(prev_state):
                    bellman_error = self.compute_bellman_error(prev_state, V)
                    if bellman_error > theta:
                        heapq.heappush(priority_queue, (-bellman_error, prev_state))
        
        return V
    
    def bellman_backup(self, state, V):
        """æ‰§è¡ŒBellmanæ›´æ–°"""
        max_value = float('-inf')
        
        for action in self.mdp.actions:
            value = 0
            for next_state in self.mdp.states:
                trans_prob = self.mdp.get_transition_prob(state, action, next_state)
                reward = self.mdp.get_reward(state, action, next_state)
                value += trans_prob * (reward + self.mdp.gamma * V[next_state])
            max_value = max(max_value, value)
        
        return max_value
    
    def compute_bellman_error(self, state, V):
        """è®¡ç®—Bellmanè¯¯å·®"""
        new_value = self.bellman_backup(state, V)
        return abs(V[state] - new_value)
    
    def get_predecessors(self, state):
        """è·å–å¯ä»¥è½¬ç§»åˆ°è¯¥çŠ¶æ€çš„å‰é©±çŠ¶æ€"""
        predecessors = []
        for s in self.mdp.states:
            for a in self.mdp.actions:
                if self.mdp.get_transition_prob(s, a, state) > 0:
                    predecessors.append(s)
        return predecessors
```

---

## å››ã€è’™ç‰¹å¡æ´›æ–¹æ³•

### 4.1 è’™ç‰¹å¡æ´›é¢„æµ‹
```python
class MonteCarloPredictor:
    """
    è’™ç‰¹å¡æ´›ç­–ç•¥è¯„ä¼°
    é€šè¿‡é‡‡æ ·å®Œæ•´è½¨è¿¹ä¼°è®¡ä»·å€¼å‡½æ•°
    ä¸éœ€è¦ç¯å¢ƒæ¨¡å‹ï¼ˆmodel-freeï¼‰
    """
    def __init__(self, gamma=0.99):
        self.gamma = gamma
        self.returns = {}  # è®°å½•æ¯ä¸ªçŠ¶æ€çš„å›æŠ¥
        self.V = {}        # çŠ¶æ€ä»·å€¼å‡½æ•°
    
    def first_visit_mc(self, episodes):
        """
        é¦–æ¬¡è®¿é—®MCï¼šåªç»Ÿè®¡çŠ¶æ€ç¬¬ä¸€æ¬¡å‡ºç°æ—¶çš„å›æŠ¥
        """
        for episode in episodes:
            # episode = [(s0,a0,r0), (s1,a1,r1), ..., (sT,aT,rT)]
            states_visited = set()
            G = 0
            
            # ä»åå‘å‰è®¡ç®—å›æŠ¥
            for t in range(len(episode)-1, -1, -1):
                state, action, reward = episode[t]
                G = reward + self.gamma * G
                
                # é¦–æ¬¡è®¿é—®
                if state not in states_visited:
                    states_visited.add(state)
                    
                    if state not in self.returns:
                        self.returns[state] = []
                    self.returns[state].append(G)
                    
                    # æ›´æ–°ä»·å€¼å‡½æ•°ï¼ˆå¹³å‡ï¼‰
                    self.V[state] = np.mean(self.returns[state])
        
        return self.V
    
    def every_visit_mc(self, episodes):
        """
        æ¯æ¬¡è®¿é—®MCï¼šç»Ÿè®¡çŠ¶æ€æ‰€æœ‰å‡ºç°æ—¶çš„å›æŠ¥
        """
        for episode in episodes:
            G = 0
            
            for t in range(len(episode)-1, -1, -1):
                state, action, reward = episode[t]
                G = reward + self.gamma * G
                
                if state not in self.returns:
                    self.returns[state] = []
                self.returns[state].append(G)
                self.V[state] = np.mean(self.returns[state])
        
        return self.V
    
    def incremental_mc(self, episodes, alpha=None):
        """
        å¢é‡å¼MCï¼šä½¿ç”¨å¢é‡å¹³å‡æ›´æ–°
        V(s) â† V(s) + Î±[G - V(s)]
        """
        for state in self.V:
            if state not in self.returns:
                self.returns[state] = []
        
        for episode in episodes:
            G = 0
            states_visited = set()
            
            for t in range(len(episode)-1, -1, -1):
                state, action, reward = episode[t]
                G = reward + self.gamma * G
                
                if state not in states_visited:
                    states_visited.add(state)
                    
                    if state not in self.V:
                        self.V[state] = 0
                    
                    # å¢é‡æ›´æ–°
                    if alpha is None:
                        # è‡ªé€‚åº”å­¦ä¹ ç‡
                        n = len(self.returns.get(state, [])) + 1
                        step_size = 1.0 / n
                    else:
                        step_size = alpha
                    
                    self.V[state] += step_size * (G - self.V[state])
        
        return self.V

# é‡è¦æ€§é‡‡æ ·
class ImportanceSamplingMC:
    """
    é‡è¦æ€§é‡‡æ ·ï¼šä½¿ç”¨è¡Œä¸ºç­–ç•¥é‡‡æ ·ï¼Œè¯„ä¼°ç›®æ ‡ç­–ç•¥
    é€‚ç”¨äºç¦»ç­–ç•¥ï¼ˆoff-policyï¼‰å­¦ä¹ 
    """
    def __init__(self, gamma=0.99):
        self.gamma = gamma
        self.V = {}
        self.C = {}  # ç´¯ç§¯æƒé‡
    
    def weighted_importance_sampling(self, episodes, behavior_policy, target_policy):
        """
        åŠ æƒé‡è¦æ€§é‡‡æ ·
        Ï_t = Ï€(a_t|s_t) / b(a_t|s_t)
        """
        for episode in episodes:
            G = 0
            W = 1.0  # é‡è¦æ€§é‡‡æ ·æ¯”ç‡
            
            for t in range(len(episode)-1, -1, -1):
                state, action, reward = episode[t]
                G = reward + self.gamma * G
                
                if state not in self.C:
                    self.C[state] = 0
                    self.V[state] = 0
                
                self.C[state] += W
                
                # åŠ æƒæ›´æ–°
                self.V[state] += (W / self.C[state]) * (G - self.V[state])
                
                # æ›´æ–°é‡è¦æ€§é‡‡æ ·æ¯”ç‡
                pi_prob = target_policy.get_action_prob(state, action)
                b_prob = behavior_policy.get_action_prob(state, action)
                
                if b_prob == 0:
                    break
                
                W *= pi_prob / b_prob
                
                if W == 0:
                    break
        
        return self.V
```

### 4.2 è’™ç‰¹å¡æ´›æ§åˆ¶
```python
class MonteCarloControl:
    """
    è’™ç‰¹å¡æ´›æ§åˆ¶ï¼šé€šè¿‡é‡‡æ ·å­¦ä¹ æœ€ä¼˜ç­–ç•¥
    """
    def __init__(self, env, gamma=0.99, epsilon=0.1):
        self.env = env
        self.gamma = gamma
        self.epsilon = epsilon
        self.Q = {}  # Q(s,a)
        self.returns = {}
        self.policy = {}
    
    def on_policy_mc_control(self, num_episodes=1000):
        """
        åŒç­–ç•¥MCæ§åˆ¶ï¼ˆÎµ-è´ªå¿ƒç­–ç•¥æ”¹è¿›ï¼‰
        1. ä½¿ç”¨Îµ-è´ªå¿ƒç­–ç•¥ç”Ÿæˆè½¨è¿¹
        2. è¯„ä¼°Qå‡½æ•°
        3. æ”¹è¿›ç­–ç•¥
        """
        for episode_num in range(num_episodes):
            # ç”Ÿæˆè½¨è¿¹
            episode = self.generate_episode()
            
            # æ›´æ–°Qå€¼ï¼ˆé¦–æ¬¡è®¿é—®ï¼‰
            states_actions_visited = set()
            G = 0
            
            for t in range(len(episode)-1, -1, -1):
                state, action, reward = episode[t]
                G = reward + self.gamma * G
                
                sa_pair = (state, action)
                if sa_pair not in states_actions_visited:
                    states_actions_visited.add(sa_pair)
                    
                    if sa_pair not in self.returns:
                        self.returns[sa_pair] = []
                    self.returns[sa_pair].append(G)
                    
                    # æ›´æ–°Qå€¼
                    if state not in self.Q:
                        self.Q[state] = {}
                    self.Q[state][action] = np.mean(self.returns[sa_pair])
                    
                    # ç­–ç•¥æ”¹è¿›ï¼ˆÎµ-è´ªå¿ƒï¼‰
                    self.update_epsilon_greedy_policy(state)
        
        return self.Q, self.policy
    
    def off_policy_mc_control(self, num_episodes=1000):
        """
        ç¦»ç­–ç•¥MCæ§åˆ¶
        è¡Œä¸ºç­–ç•¥ï¼šÎµ-è´ªå¿ƒï¼ˆç”¨äºæ¢ç´¢ï¼‰
        ç›®æ ‡ç­–ç•¥ï¼šè´ªå¿ƒï¼ˆè¦å­¦ä¹ çš„ç­–ç•¥ï¼‰
        """
        # åˆå§‹åŒ–ç›®æ ‡ç­–ç•¥ä¸ºè´ªå¿ƒ
        target_policy = {}
        
        # ç´¯ç§¯æƒé‡
        C = {}
        
        for episode_num in range(num_episodes):
            # ä½¿ç”¨è¡Œä¸ºç­–ç•¥ç”Ÿæˆè½¨è¿¹
            episode = self.generate_episode()
            
            G = 0
            W = 1.0
            
            for t in range(len(episode)-1, -1, -1):
                state, action, reward = episode[t]
                G = reward + self.gamma * G
                
                sa_pair = (state, action)
                
                if sa_pair not in C:
                    C[sa_pair] = 0
                    if state not in self.Q:
                        self.Q[state] = {}
                    if action not in self.Q[state]:
                        self.Q[state][action] = 0
                
                C[sa_pair] += W
                
                # åŠ æƒæ›´æ–°Qå€¼
                self.Q[state][action] += (W / C[sa_pair]) * \
                                        (G - self.Q[state][action])
                
                # æ›´æ–°ç›®æ ‡ç­–ç•¥ï¼ˆè´ªå¿ƒï¼‰
                if state in self.Q and len(self.Q[state]) > 0:
                    target_policy[state] = max(self.Q[state], 
                                              key=self.Q[state].get)
                
                # å¦‚æœåŠ¨ä½œä¸æ˜¯è´ªå¿ƒåŠ¨ä½œï¼Œç»ˆæ­¢
                if state not in target_policy or action != target_policy[state]:
                    break
                
                # æ›´æ–°é‡è¦æ€§é‡‡æ ·æ¯”ç‡
                # è¡Œä¸ºç­–ç•¥æ˜¯Îµ-è´ªå¿ƒï¼Œç›®æ ‡ç­–ç•¥æ˜¯è´ªå¿ƒ
                num_actions = len(self.env.action_space)
                b_prob = self.epsilon / num_actions + \
                        (1 - self.epsilon) * (action == target_policy.get(state))
                pi_prob = 1.0  # è´ªå¿ƒç­–ç•¥
                
                W *= pi_prob / b_prob
        
        return self.Q, target_policy
    
    def generate_episode(self):
        """ä½¿ç”¨å½“å‰ç­–ç•¥ç”Ÿæˆä¸€ä¸ªè½¨è¿¹"""
        episode = []
        state = self.env.reset()
        done = False
        
        while not done:
            # Îµ-è´ªå¿ƒé€‰æ‹©åŠ¨ä½œ
            action = self.select_epsilon_greedy_action(state)
            next_state, reward, done, _ = self.env.step(action)
            episode.append((state, action, reward))
            state = next_state
        
        return episode
    
    def select_epsilon_greedy_action(self, state):
        """Îµ-è´ªå¿ƒåŠ¨ä½œé€‰æ‹©"""
        if np.random.random() < self.epsilon:
            return self.env.action_space.sample()
        else:
            if state not in self.Q or len(self.Q[state]) == 0:
                return self.env.action_space.sample()
            return max(self.Q[state], key=self.Q[state].get)
    
    def update_epsilon_greedy_policy(self, state):
        """æ›´æ–°Îµ-è´ªå¿ƒç­–ç•¥"""
        if state in self.Q and len(self.Q[state]) > 0:
            self.policy[state] = max(self.Q[state], key=self.Q[state].get)
```

---

## äº”ã€æ—¶åºå·®åˆ†å­¦ä¹ 

### 5.1 TDé¢„æµ‹
```python
class TDPredictor:
    """
    æ—¶åºå·®åˆ†ï¼ˆTDï¼‰é¢„æµ‹
    ç»“åˆäº†MCå’ŒDPçš„ä¼˜ç‚¹ï¼š
    - åƒMCä¸€æ ·æ˜¯model-freeçš„
    - åƒDPä¸€æ ·å¯ä»¥bootstrapï¼ˆç”¨ä¼°è®¡æ›´æ–°ä¼°è®¡ï¼‰
    """
    def __init__(self, gamma=0.99, alpha=0.1):
        self.gamma = gamma
        self.alpha = alpha
        self.V = {}
    
    def td_0(self, env, policy, num_episodes=1000):
        """
        TD(0)ï¼šå•æ­¥æ—¶åºå·®åˆ†
        V(s_t) â† V(s_t) + Î±[r_t + Î³V(s_{t+1}) - V(s_t)]
        
        TDç›®æ ‡ï¼šr_t + Î³V(s_{t+1})
        TDè¯¯å·®ï¼šÎ´_t = r_t + Î³V(s_{t+1}) - V(s_t)
        """
        for episode in range(num_episodes):
            state = env.reset()
            done = False
            
            while not done:
                # æ ¹æ®ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
                action = policy.select_action(state)
                next_state, reward, done, _ = env.step(action)
                
                # åˆå§‹åŒ–Vå€¼
                if state not in self.V:
                    self.V[state] = 0
                if next_state not in self.V:
                    self.V[next_state] = 0
                
                # TDæ›´æ–°
                td_target = reward + self.gamma * self.V[next_state] * (not done)
                td_error = td_target - self.V[state]
                self.V[state] += self.alpha * td_error
                
                state = next_state
        
        return self.V
    
    def td_lambda(self, env, policy, lambda_=0.9, num_episodes=1000):
        """
        TD(Î»)ï¼šä½¿ç”¨èµ„æ ¼è¿¹
        ç»“åˆäº†å¤šæ­¥TDçš„ä¼˜ç‚¹
        
        Î» = 0: TD(0)
        Î» = 1: MC
        """
        for episode in range(num_episodes):
            state = env.reset()
            done = False
            
            # èµ„æ ¼è¿¹
            eligibility_trace = {}
            
            while not done:
                action = policy.select_action(state)
                next_state, reward, done, _ = env.step(action)
                
                if state not in self.V:
                    self.V[state] = 0
                if next_state not in self.V:
                    self.V[next_state] = 0
                
                # TDè¯¯å·®
                td_error = reward + self.gamma * self.V[next_state] * (not done) - self.V[state]
                
                # æ›´æ–°èµ„æ ¼è¿¹
                if state not in eligibility_trace:
                    eligibility_trace[state] = 0
                eligibility_trace[state] += 1
                
                # æ›´æ–°æ‰€æœ‰çŠ¶æ€çš„ä»·å€¼ï¼ˆæ ¹æ®èµ„æ ¼è¿¹ï¼‰
                for s in eligibility_trace:
                    self.V[s] += self.alpha * td_error * eligibility_trace[s]
                    eligibility_trace[s] *= self.gamma * lambda_
                
                state = next_state
        
        return self.V
    
    def n_step_td(self, env, policy, n=5, num_episodes=1000):
        """
        næ­¥TD
        G_t^{(n)} = r_t + Î³r_{t+1} + ... + Î³^{n-1}r_{t+n-1} + Î³^nV(s_{t+n})
        """
        for episode in range(num_episodes):
            # å­˜å‚¨è½¨è¿¹
            states = [env.reset()]
            actions = []
            rewards = [0]  # å¡«å……ï¼Œä½¿ç´¢å¼•å¯¹é½
            
            T = float('inf')
            t = 0
            
            while True:
                if t < T:
                    action = policy.select_action(states[t])
                    next_state, reward, done, _ = env.step(action)
                    
                    states.append(next_state)
                    actions.append(action)
                    rewards.append(reward)
                    
                    if done:
                        T = t + 1
                
                # æ›´æ–°æ—¶åˆ»
                tau = t - n + 1
                
                if tau >= 0:
                    # è®¡ç®—næ­¥å›æŠ¥
                    G = sum([self.gamma**(i-tau-1) * rewards[i] 
                            for i in range(tau+1, min(tau+n, T)+1)])
                    
                    if tau + n < T:
                        state_tau_n = states[tau + n]
                        if state_tau_n not in self.V:
                            self.V[state_tau_n] = 0
                        G += self.gamma**n * self.V[state_tau_n]
                    
                    # æ›´æ–°
                    state_tau = states[tau]
                    if state_tau not in self.V:
                        self.V[state_tau] = 0
                    self.V[state_tau] += self.alpha * (G - self.V[state_tau])
                
                if tau == T - 1:
                    break
                
                t += 1
        
        return self.V
```

### 5.2 SARSA
```python
class SARSA:
    """
    SARSAï¼šåŒç­–ç•¥TDæ§åˆ¶ç®—æ³•
    State-Action-Reward-State-Action
    
    Q(s,a) â† Q(s,a) + Î±[r + Î³Q(s',a') - Q(s,a)]
    """
    def __init__(self, env, gamma=0.99, alpha=0.1, epsilon=0.1):
        self.env = env
        self.gamma = gamma
        self.alpha = alpha
        self.epsilon = epsilon
        self.Q = {}
    
    def train(self, num_episodes=1000):
        """SARSAè®­ç»ƒ"""
        episode_rewards = []
        
        for episode in range(num_episodes):
            state = self.env.reset()
            action = self.select_action(state)
            
            total_reward = 0
            done = False
            
            while not done:
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, _ = self.env.step(action)
                total_reward += reward
                
                # é€‰æ‹©ä¸‹ä¸€ä¸ªåŠ¨ä½œï¼ˆåŒç­–ç•¥ï¼‰
                next_action = self.select_action(next_state)
                
                # SARSAæ›´æ–°
                self.update_q(state, action, reward, next_state, next_action, done)
                
                state = next_state
                action = next_action
            
            episode_rewards.append(total_reward)
            
            # Îµè¡°å‡
            self.epsilon = max(0.01, self.epsilon * 0.995)
        
        return self.Q, episode_rewards
    
    def update_q(self, state, action, reward, next_state, next_action, done):
        """SARSA Qå€¼æ›´æ–°"""
        if state not in self.Q:
            self.Q[state] = {}
        if action not in self.Q[state]:
            self.Q[state][action] = 0
        
        if not done:
            if next_state not in self.Q:
                self.Q[next_state] = {}
            if next_action not in self.Q[next_state]:
                self.Q[next_state][next_action] = 0
            
            td_target = reward + self.gamma * self.Q[next_state][next_action]
        else:
            td_target = reward
        
        td_error = td_target - self.Q[state][action]
        self.Q[state][action] += self.alpha * td_error
    
    def select_action(self, state):
        """Îµ-è´ªå¿ƒåŠ¨ä½œé€‰æ‹©"""
        if np.random.random() < self.epsilon:
            return self.env.action_space.sample()
        else:
            if state not in self.Q or len(self.Q[state]) == 0:
                return self.env.action_space.sample()
            return max(self.Q[state], key=self.Q[state].get)

# SARSA(Î»)ï¼šå¸¦èµ„æ ¼è¿¹çš„SARSA
class SARSALambda(SARSA):
    """
    SARSA(Î»)ï¼šç»“åˆèµ„æ ¼è¿¹çš„SARSA
    å¯ä»¥æ›´å¿«åœ°ä¼ æ’­å¥–åŠ±ä¿¡å·
    """
    def __init__(self, env, gamma=0.99, alpha=0.1, epsilon=0.1, lambda_=0.9):
        super().__init__(env, gamma, alpha, epsilon)
        self.lambda_ = lambda_
    
    def train(self, num_episodes=1000):
        """SARSA(Î»)è®­ç»ƒ"""
        episode_rewards = []
        
        for episode in range(num_episodes):
            state = self.env.reset()
            action = self.select_action(state)
            
            # èµ„æ ¼è¿¹
            eligibility_trace = {}
            
            total_reward = 0
            done = False
            
            while not done:
                next_state, reward, done, _ = self.env.step(action)
                total_reward += reward
                
                next_action = self.select_action(next_state)
                
                # åˆå§‹åŒ–Qå€¼
                if state not in self.Q:
                    self.Q[state] = {}
                if action not in self.Q[state]:
                    self.Q[state][action] = 0
                
                # TDè¯¯å·®
                if not done:
                    if next_state not in self.Q:
                        self.Q[next_state] = {}
                    if next_action not in self.Q[next_state]:
                        self.Q[next_state][next_action] = 0
                    td_error = reward + self.gamma * self.Q[next_state][next_action] - \
                              self.Q[state][action]
                else:
                    td_error = reward - self.Q[state][action]
                
                # æ›´æ–°èµ„æ ¼è¿¹
                if state not in eligibility_trace:
                    eligibility_trace[state] = {}
                if action not in eligibility_trace[state]:
                    eligibility_trace[state][action] = 0
                eligibility_trace[state][action] += 1
                
                # æ›´æ–°æ‰€æœ‰çŠ¶æ€-åŠ¨ä½œå¯¹
                for s in list(eligibility_trace.keys()):
                    for a in list(eligibility_trace[s].keys()):
                        if s not in self.Q:
                            self.Q[s] = {}
                        if a not in self.Q[s]:
                            self.Q[s][a] = 0
                        
                        self.Q[s][a] += self.alpha * td_error * eligibility_trace[s][a]
                        eligibility_trace[s][a] *= self.gamma * self.lambda_
                        
                        # æ¸…é™¤å¾ˆå°çš„èµ„æ ¼è¿¹
                        if eligibility_trace[s][a] < 1e-5:
                            del eligibility_trace[s][a]
                
                state = next_state
                action = next_action
            
            episode_rewards.append(total_reward)
            self.epsilon = max(0.01, self.epsilon * 0.995)
        
        return self.Q, episode_rewards
```

### 5.3 Q-Learning
```python
class QLearning:
    """
    Q-Learningï¼šç¦»ç­–ç•¥TDæ§åˆ¶ç®—æ³•
    
    Q(s,a) â† Q(s,a) + Î±[r + Î³ max_a' Q(s',a') - Q(s,a)]
    
    å…³é”®ç‰¹ç‚¹ï¼š
    - è¡Œä¸ºç­–ç•¥ï¼šÎµ-è´ªå¿ƒï¼ˆç”¨äºæ¢ç´¢ï¼‰
    - ç›®æ ‡ç­–ç•¥ï¼šè´ªå¿ƒï¼ˆmax_a'ï¼‰
    """
    def __init__(self, env, gamma=0.99, alpha=0.1, epsilon=0.1):
        self.env = env
        self.gamma = gamma
        self.alpha = alpha
        self.epsilon = epsilon
        self.Q = {}
    
    def train(self, num_episodes=1000):
        """Q-Learningè®­ç»ƒ"""
        episode_rewards = []
        
        for episode in range(num_episodes):
            state = self.env.reset()
            total_reward = 0
            done = False
            
            while not done:
                # ä½¿ç”¨Îµ-è´ªå¿ƒé€‰æ‹©åŠ¨ä½œï¼ˆè¡Œä¸ºç­–ç•¥ï¼‰
                action = self.select_action(state)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, _ = self.env.step(action)
                total_reward += reward
                
                # Q-Learningæ›´æ–°
                self.update_q(state, action, reward, next_state, done)
                
                state = next_state
            
            episode_rewards.append(total_reward)
            
            # Îµè¡°å‡
            self.epsilon = max(0.01, self.epsilon * 0.995)
        
        return self.Q, episode_rewards
    
    def update_q(self, state, action, reward, next_state, done):
        """Q-Learningæ›´æ–°è§„åˆ™"""
        if state not in self.Q:
            self.Q[state] = {}
        if action not in self.Q[state]:
            self.Q[state][action] = 0
        
        if not done:
            # ä½¿ç”¨maxï¼ˆè´ªå¿ƒï¼Œç›®æ ‡ç­–ç•¥ï¼‰
            if next_state not in self.Q or len(self.Q[next_state]) == 0:
                max_next_q = 0
            else:
                max_next_q = max(self.Q[next_state].values())
            
            td_target = reward + self.gamma * max_next_q
        else:
            td_target = reward
        
        td_error = td_target - self.Q[state][action]
        self.Q[state][action] += self.alpha * td_error
    
    def select_action(self, state):
        """Îµ-è´ªå¿ƒåŠ¨ä½œé€‰æ‹©"""
        if np.random.random() < self.epsilon:
            return self.env.action_space.sample()
        else:
            if state not in self.Q or len(self.Q[state]) == 0:
                return self.env.action_space.sample()
            return max(self.Q[state], key=self.Q[state].get)
    
    def get_greedy_action(self, state):
        """è·å–è´ªå¿ƒåŠ¨ä½œï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        if state not in self.Q or len(self.Q[state]) == 0:
            return self.env.action_space.sample()
        return max(self.Q[state], key=self.Q[state].get)

# Double Q-Learning
class DoubleQLearning:
    """
    Double Q-Learningï¼šè§£å†³Q-Learningçš„è¿‡ä¼°è®¡é—®é¢˜
    ç»´æŠ¤ä¸¤ä¸ªQå‡½æ•°ï¼šQ1å’ŒQ2
    
    æ›´æ–°Q1æ—¶ï¼Œç”¨Q1é€‰æ‹©åŠ¨ä½œï¼Œç”¨Q2è¯„ä¼°
    æ›´æ–°Q2æ—¶ï¼Œç”¨Q2é€‰æ‹©åŠ¨ä½œï¼Œç”¨Q1è¯„ä¼°
    """
    def __init__(self, env, gamma=0.99, alpha=0.1, epsilon=0.1):
        self.env = env
        self.gamma = gamma
        self.alpha = alpha
        self.epsilon = epsilon
        self.Q1 = {}
        self.Q2 = {}
    
    def train(self, num_episodes=1000):
        """Double Q-Learningè®­ç»ƒ"""
        episode_rewards = []
        
        for episode in range(num_episodes):
            state = self.env.reset()
            total_reward = 0
            done = False
            
            while not done:
                action = self.select_action(state)
                next_state, reward, done, _ = self.env.step(action)
                total_reward += reward
                
                # éšæœºé€‰æ‹©æ›´æ–°Q1æˆ–Q2
                if np.random.random() < 0.5:
                    self.update_q1(state, action, reward, next_state, done)
                else:
                    self.update_q2(state, action, reward, next_state, done)
                
                state = next_state
            
            episode_rewards.append(total_reward)
            self.epsilon = max(0.01, self.epsilon * 0.995)
        
        return self.Q1, self.Q2, episode_rewards
    
    def update_q1(self, state, action, reward, next_state, done):
        """ä½¿ç”¨Q2è¯„ä¼°Q1é€‰æ‹©çš„åŠ¨ä½œ"""
        if state not in self.Q1:
            self.Q1[state] = {}
        if action not in self.Q1[state]:
            self.Q1[state][action] = 0
        
        if not done:
            # Q1é€‰æ‹©åŠ¨ä½œ
            if next_state not in self.Q1 or len(self.Q1[next_state]) == 0:
                best_action = self.env.action_space.sample()
            else:
                best_action = max(self.Q1[next_state], key=self.Q1[next_state].get)
            
            # Q2è¯„ä¼°
            if next_state not in self.Q2 or best_action not in self.Q2[next_state]:
                next_q = 0
            else:
                next_q = self.Q2[next_state][best_action]
            
            td_target = reward + self.gamma * next_q
        else:
            td_target = reward
        
        self.Q1[state][action] += self.alpha * (td_target - self.Q1[state][action])
    
    def update_q2(self, state, action, reward, next_state, done):
        """ä½¿ç”¨Q1è¯„ä¼°Q2é€‰æ‹©çš„åŠ¨ä½œ"""
        if state not in self.Q2:
            self.Q2[state] = {}
        if action not in self.Q2[state]:
            self.Q2[state][action] = 0
        
        if not done:
            # Q2é€‰æ‹©åŠ¨ä½œ
            if next_state not in self.Q2 or len(self.Q2[next_state]) == 0:
                best_action = self.env.action_space.sample()
            else:
                best_action = max(self.Q2[next_state], key=self.Q2[next_state].get)
            
            # Q1è¯„ä¼°
            if next_state not in self.Q1 or best_action not in self.Q1[next_state]:
                next_q = 0
            else:
                next_q = self.Q1[next_state][best_action]
            
            td_target = reward + self.gamma * next_q
        else:
            td_target = reward
        
        self.Q2[state][action] += self.alpha * (td_target - self.Q2[state][action])
    
    def select_action(self, state):
        """ä½¿ç”¨Q1+Q2çš„å¹³å‡å€¼è¿›è¡ŒÎµ-è´ªå¿ƒ"""
        if np.random.random() < self.epsilon:
            return self.env.action_space.sample()
        
        # åˆå¹¶Q1å’ŒQ2
        q_avg = {}
        all_actions = set()
        
        if state in self.Q1:
            all_actions.update(self.Q1[state].keys())
        if state in self.Q2:
            all_actions.update(self.Q2[state].keys())
        
        if not all_actions:
            return self.env.action_space.sample()
        
        for a in all_actions:
            q1_val = self.Q1.get(state, {}).get(a, 0)
            q2_val = self.Q2.get(state, {}).get(a, 0)
            q_avg[a] = (q1_val + q2_val) / 2
        
        return max(q_avg, key=q_avg.get)
```

---

## å…­ã€å€¼å‡½æ•°é€¼è¿‘

### 6.1 çº¿æ€§å‡½æ•°é€¼è¿‘
```python
import numpy as np

class LinearFunctionApproximation:
    """
    çº¿æ€§å‡½æ•°é€¼è¿‘
    V(s) â‰ˆ Ï†(s)áµ€w
    Q(s,a) â‰ˆ Ï†(s,a)áµ€w
    
    å…¶ä¸­Ï†æ˜¯ç‰¹å¾å‘é‡ï¼Œwæ˜¯æƒé‡
    """
    def __init__(self, feature_dim, alpha=0.01):
        self.w = np.zeros(feature_dim)
        self.alpha = alpha
    
    def predict_value(self, features):
        """é¢„æµ‹çŠ¶æ€ä»·å€¼"""
        return np.dot(features, self.w)
    
    def update(self, features, target):
        """
        æ¢¯åº¦ä¸‹é™æ›´æ–°
        w â† w + Î±[target - V(s)]âˆ‡V(s)
        w â† w + Î±[target - V(s)]Ï†(s)
        """
        prediction = self.predict_value(features)
        error = target - prediction
        self.w += self.alpha * error * features
    
    def semi_gradient_td(self, env, feature_extractor, num_episodes=1000, gamma=0.99):
        """
        åŠæ¢¯åº¦TD(0)
        åªå¯¹ä»·å€¼å‡½æ•°çš„æ¢¯åº¦è¿›è¡Œæ›´æ–°ï¼Œä¸å¯¹ç›®æ ‡çš„æ¢¯åº¦æ›´æ–°
        """
        for episode in range(num_episodes):
            state = env.reset()
            done = False
            
            while not done:
                # æå–ç‰¹å¾
                features = feature_extractor(state)
                
                # é€‰æ‹©åŠ¨ä½œï¼ˆè¿™é‡Œå‡è®¾æœ‰ç­–ç•¥ï¼‰
                action = env.action_space.sample()
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, _ = env.step(action)
                
                # TDæ›´æ–°
                if not done:
                    next_features = feature_extractor(next_state)
                    td_target = reward + gamma * self.predict_value(next_features)
                else:
                    td_target = reward
                
                self.update(features, td_target)
                state = next_state
        
        return self.w

# ç‰¹å¾å·¥ç¨‹ç¤ºä¾‹
class FeatureExtractor:
    """ç‰¹å¾æå–å™¨"""
    
    @staticmethod
    def polynomial_features(state, degree=2):
        """å¤šé¡¹å¼ç‰¹å¾"""
        state = np.array(state).flatten()
        features = [1]  # åç½®é¡¹
        
        # ä¸€é˜¶ç‰¹å¾
        features.extend(state)
        
        # é«˜é˜¶ç‰¹å¾
        if degree >= 2:
            for i in range(len(state)):
                for j in range(i, len(state)):
                    features.append(state[i] * state[j])
        
        return np.array(features)
    
    @staticmethod
    def tile_coding(state, num_tilings=8, num_tiles=8):
        """
        Tile Codingï¼šå°†è¿ç»­çŠ¶æ€ç©ºé—´åˆ†æˆå¤šä¸ªé‡å çš„ç½‘æ ¼
        æ¯ä¸ªç½‘æ ¼ç§°ä¸ºä¸€ä¸ªtiling
        """
        features = []
        state = np.array(state).flatten()
        
        for tiling in range(num_tilings):
            # ä¸ºæ¯ä¸ªtilingæ·»åŠ åç§»
            offset = tiling / num_tilings
            
            for dim in range(len(state)):
                # è®¡ç®—è¯¥ç»´åº¦çš„tileç´¢å¼•
                tile_idx = int((state[dim] + offset) * num_tiles)
                tile_idx = max(0, min(num_tiles - 1, tile_idx))
                
                # åˆ›å»ºone-hotç¼–ç 
                one_hot = np.zeros(num_tiles)
                one_hot[tile_idx] = 1
                features.extend(one_hot)
        
        return np.array(features)
    
    @staticmethod
    def rbf_features(state, centers, sigma=1.0):
        """
        å¾„å‘åŸºå‡½æ•°ï¼ˆRBFï¼‰ç‰¹å¾
        Ï†_i(s) = exp(-||s - c_i||Â² / (2ÏƒÂ²))
        """
        state = np.array(state).flatten()
        features = []
        
        for center in centers:
            center = np.array(center).flatten()
            distance = np.linalg.norm(state - center)
            feature = np.exp(-distance**2 / (2 * sigma**2))
            features.append(feature)
        
        return np.array(features)

# çº¿æ€§Qå‡½æ•°é€¼è¿‘
class LinearQApproximation:
    """
    çº¿æ€§Qå‡½æ•°é€¼è¿‘
    Q(s,a) = Ï†(s,a)áµ€w
    """
    def __init__(self, num_actions, feature_dim, alpha=0.01):
        self.num_actions = num_actions
        # ä¸ºæ¯ä¸ªåŠ¨ä½œç»´æŠ¤ä¸€ä¸ªæƒé‡å‘é‡
        self.w = np.zeros((num_actions, feature_dim))
        self.alpha = alpha
    
    def predict_q(self, features, action):
        """é¢„æµ‹Q(s,a)"""
        return np.dot(features, self.w[action])
    
    def predict_all_q(self, features):
        """é¢„æµ‹æ‰€æœ‰åŠ¨ä½œçš„Qå€¼"""
        return features @ self.w.T
    
    def update_q_learning(self, features, action, reward, next_features, done, gamma=0.99):
        """
        Q-Learningæ›´æ–°
        w â† w + Î±[r + Î³ max_a' Q(s',a') - Q(s,a)]Ï†(s,a)
        """
        current_q = self.predict_q(features, action)
        
        if not done:
            max_next_q = np.max(self.predict_all_q(next_features))
            td_target = reward + gamma * max_next_q
        else:
            td_target = reward
        
        td_error = td_target - current_q
        self.w[action] += self.alpha * td_error * features
    
    def update_sarsa(self, features, action, reward, next_features, next_action, done, gamma=0.99):
        """
        SARSAæ›´æ–°
        w â† w + Î±[r + Î³Q(s',a') - Q(s,a)]Ï†(s,a)
        """
        current_q = self.predict_q(features, action)
        
        if not done:
            next_q = self.predict_q(next_features, next_action)
            td_target = reward + gamma * next_q
        else:
            td_target = reward
        
        td_error = td_target - current_q
        self.w[action] += self.alpha * td_error * features
```

### 6.2 ç¥ç»ç½‘ç»œå‡½æ•°é€¼è¿‘
```python
import torch
import torch.nn as nn
import torch.optim as optim

class ValueNetwork(nn.Module):
    """
    ä½¿ç”¨ç¥ç»ç½‘ç»œé€¼è¿‘ä»·å€¼å‡½æ•°
    """
    def __init__(self, state_dim, hidden_dims=[64, 64]):
        super(ValueNetwork, self).__init__()
        
        layers = []
        input_dim = state_dim
        
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(input_dim, hidden_dim))
            layers.append(nn.ReLU())
            input_dim = hidden_dim
        
        layers.append(nn.Linear(input_dim, 1))  # è¾“å‡ºå•ä¸ªå€¼
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, state):
        return self.network(state)

class QNetwork(nn.Module):
    """
    ä½¿ç”¨ç¥ç»ç½‘ç»œé€¼è¿‘Qå‡½æ•°
    """
    def __init__(self, state_dim, action_dim, hidden_dims=[64, 64]):
        super(QNetwork, self).__init__()
        
        layers = []
        input_dim = state_dim
        
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(input_dim, hidden_dim))
            layers.append(nn.ReLU())
            input_dim = hidden_dim
        
        layers.append(nn.Linear(input_dim, action_dim))  # è¾“å‡ºæ‰€æœ‰åŠ¨ä½œçš„Qå€¼
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, state):
        return self.network(state)

# ä½¿ç”¨ç¥ç»ç½‘ç»œçš„TDå­¦ä¹ 
class NeuralTD:
    """ä½¿ç”¨ç¥ç»ç½‘ç»œçš„TDå­¦ä¹ """
    def __init__(self, state_dim, lr=0.001, gamma=0.99):
        self.value_net = ValueNetwork(state_dim)
        self.optimizer = optim.Adam(self.value_net.parameters(), lr=lr)
        self.gamma = gamma
    
    def train_step(self, state, reward, next_state, done):
        """å•æ­¥è®­ç»ƒ"""
        state_tensor = torch.FloatTensor(state)
        next_state_tensor = torch.FloatTensor(next_state)
        
        # å‰å‘ä¼ æ’­
        current_value = self.value_net(state_tensor)
        
        # è®¡ç®—TDç›®æ ‡
        with torch.no_grad():
            if not done:
                next_value = self.value_net(next_state_tensor)
                td_target = reward + self.gamma * next_value
            else:
                td_target = torch.FloatTensor([reward])
        
        # è®¡ç®—æŸå¤±
        loss = nn.MSELoss()(current_value, td_target)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return loss.item()

# Dueling Network
class DuelingQNetwork(nn.Module):
    """
    Duelingç½‘ç»œæ¶æ„
    Q(s,a) = V(s) + A(s,a) - mean(A(s,Â·))
    
    åˆ†ç¦»ä»·å€¼å’Œä¼˜åŠ¿ï¼Œå­¦ä¹ æ›´ç¨³å®š
    """
    def __init__(self, state_dim, action_dim, hidden_dim=64):
        super(DuelingQNetwork, self).__init__()
        
        # å…±äº«ç‰¹å¾æå–å±‚
        self.feature = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU()
        )
        
        # ä»·å€¼æµ
        self.value_stream = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        # ä¼˜åŠ¿æµ
        self.advantage_stream = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
    
    def forward(self, state):
        features = self.feature(state)
        value = self.value_stream(features)
        advantages = self.advantage_stream(features)
        
        # ç»“åˆä»·å€¼å’Œä¼˜åŠ¿
        # Q(s,a) = V(s) + (A(s,a) - mean_a A(s,a))
        q_values = value + (advantages - advantages.mean(dim=-1, keepdim=True))
        
        return q_values
```

---

## ä¸ƒã€ç­–ç•¥æ¢¯åº¦æ–¹æ³•

### 7.1 REINFORCEç®—æ³•
```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical

class PolicyNetwork(nn.Module):
    """ç­–ç•¥ç½‘ç»œï¼šè¾“å‡ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ"""
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)
    
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        action_probs = F.softmax(self.fc3(x), dim=-1)
        return action_probs

class REINFORCE:
    """
    REINFORCEç®—æ³•ï¼ˆè’™ç‰¹å¡æ´›ç­–ç•¥æ¢¯åº¦ï¼‰
    
    âˆ‡J(Î¸) = ğ”¼_Ï€[âˆ‡log Ï€(a|s,Î¸) * G_t]
    
    åŸºæœ¬æ€æƒ³ï¼š
    - å¥½çš„åŠ¨ä½œï¼ˆé«˜å›æŠ¥ï¼‰â†’å¢åŠ æ¦‚ç‡
    - åçš„åŠ¨ä½œï¼ˆä½å›æŠ¥ï¼‰â†’é™ä½æ¦‚ç‡
    """
    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99):
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        self.gamma = gamma
    
    def select_action(self, state):
        """æ ¹æ®ç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        state = torch.FloatTensor(state).unsqueeze(0)
        probs = self.policy(state)
        dist = Categorical(probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        return action.item(), log_prob
    
    def train(self, env, num_episodes=1000):
        """è®­ç»ƒREINFORCE"""
        episode_rewards = []
        
        for episode in range(num_episodes):
            # æ”¶é›†ä¸€ä¸ªå®Œæ•´è½¨è¿¹
            states, actions, rewards, log_probs = [], [], [], []
            
            state = env.reset()
            done = False
            
            while not done:
                action, log_prob = self.select_action(state)
                next_state, reward, done, _ = env.step(action)
                
                states.append(state)
                actions.append(action)
                rewards.append(reward)
                log_probs.append(log_prob)
                
                state = next_state
            
            # è®¡ç®—å›æŠ¥
            returns = self.compute_returns(rewards)
            
            # ç­–ç•¥æ¢¯åº¦æ›´æ–°
            policy_loss = []
            for log_prob, G in zip(log_probs, returns):
                policy_loss.append(-log_prob * G)
            
            policy_loss = torch.stack(policy_loss).sum()
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            policy_loss.backward()
            self.optimizer.step()
            
            episode_rewards.append(sum(rewards))
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(episode_rewards[-100:])
                print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}")
        
        return episode_rewards
    
    def compute_returns(self, rewards):
        """è®¡ç®—æŠ˜æ‰£å›æŠ¥"""
        returns = []
        G = 0
        for reward in reversed(rewards):
            G = reward + self.gamma * G
            returns.insert(0, G)
        
        # æ ‡å‡†åŒ–å›æŠ¥ï¼ˆå‡å°‘æ–¹å·®ï¼‰
        returns = torch.FloatTensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)
        
        return returns

# å¸¦åŸºçº¿çš„REINFORCE
class REINFORCEWithBaseline:
    """
    å¸¦åŸºçº¿çš„REINFORCE
    ä½¿ç”¨ä»·å€¼å‡½æ•°ä½œä¸ºåŸºçº¿å‡å°‘æ–¹å·®
    
    âˆ‡J(Î¸) = ğ”¼[âˆ‡log Ï€(a|s,Î¸) * (G_t - b(s))]
    å…¶ä¸­b(s)æ˜¯åŸºçº¿ï¼Œé€šå¸¸ç”¨V(s)
    """
    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99):
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.value_net = ValueNetwork(state_dim)
        
        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr)
        
        self.gamma = gamma
    
    def select_action(self, state):
        state = torch.FloatTensor(state).unsqueeze(0)
        probs = self.policy(state)
        dist = Categorical(probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        return action.item(), log_prob
    
    def train(self, env, num_episodes=1000):
        episode_rewards = []
        
        for episode in range(num_episodes):
            states, actions, rewards, log_probs, values = [], [], [], [], []
            
            state = env.reset()
            done = False
            
            while not done:
                action, log_prob = self.select_action(state)
                
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                value = self.value_net(state_tensor)
                
                next_state, reward, done, _ = env.step(action)
                
                states.append(state)
                actions.append(action)
                rewards.append(reward)
                log_probs.append(log_prob)
                values.append(value)
                
                state = next_state
            
            # è®¡ç®—å›æŠ¥å’Œä¼˜åŠ¿
            returns = self.compute_returns(rewards)
            values = torch.cat(values).squeeze()
            advantages = returns - values.detach()
            
            # æ›´æ–°ç­–ç•¥ç½‘ç»œ
            policy_loss = []
            for log_prob, advantage in zip(log_probs, advantages):
                policy_loss.append(-log_prob * advantage)
            policy_loss = torch.stack(policy_loss).sum()
            
            self.policy_optimizer.zero_grad()
            policy_loss.backward()
            self.policy_optimizer.step()
            
            # æ›´æ–°ä»·å€¼ç½‘ç»œ
            value_loss = F.mse_loss(values, returns)
            
            self.value_optimizer.zero_grad()
            value_loss.backward()
            self.value_optimizer.step()
            
            episode_rewards.append(sum(rewards))
        
        return episode_rewards
    
    def compute_returns(self, rewards):
        returns = []
        G = 0
        for reward in reversed(rewards):
            G = reward + self.gamma * G
            returns.insert(0, G)
        return torch.FloatTensor(returns)
```

### 7.2 Actor-CriticåŸºç¡€
```python
class ActorCriticNetwork(nn.Module):
    """
    Actor-Criticç½‘ç»œ
    Actorï¼šè¾“å‡ºç­–ç•¥Ï€(a|s)
    Criticï¼šè¾“å‡ºä»·å€¼V(s)
    """
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(ActorCriticNetwork, self).__init__()
        
        # å…±äº«å±‚
        self.shared = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU()
        )
        
        # Actorå¤´
        self.actor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
        
        # Criticå¤´
        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, state):
        features = self.shared(state)
        action_probs = F.softmax(self.actor(features), dim=-1)
        value = self.critic(features)
        return action_probs, value

class ActorCritic:
    """
    Actor-Criticç®—æ³•ï¼ˆå•æ­¥ï¼‰
    ä¼˜åŠ¿ï¼šåœ¨çº¿å­¦ä¹ ï¼Œä½æ–¹å·®
    """
    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99):
        self.ac_net = ActorCriticNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.ac_net.parameters(), lr=lr)
        self.gamma = gamma
    
    def select_action(self, state):
        state = torch.FloatTensor(state).unsqueeze(0)
        action_probs, value = self.ac_net(state)
        dist = Categorical(action_probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        return action.item(), log_prob, value
    
    def train(self, env, num_episodes=1000):
        episode_rewards = []
        
        for episode in range(num_episodes):
            state = env.reset()
            done = False
            total_reward = 0
            
            while not done:
                # é€‰æ‹©åŠ¨ä½œ
                action, log_prob, value = self.select_action(state)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, _ = env.step(action)
                total_reward += reward
                
                # è®¡ç®—TDè¯¯å·®ï¼ˆä¼˜åŠ¿ï¼‰
                next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)
                _, next_value = self.ac_net(next_state_tensor)
                
                if done:
                    td_target = reward
                else:
                    td_target = reward + self.gamma * next_value.item()
                
                advantage = td_target - value.item()
                
                # è®¡ç®—æŸå¤±
                actor_loss = -log_prob * advantage
                critic_loss = F.mse_loss(value, torch.FloatTensor([td_target]))
                
                loss = actor_loss + critic_loss
                
                # æ›´æ–°ç½‘ç»œ
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                
                state = next_state
            
            episode_rewards.append(total_reward)
        
        return episode_rewards
```

---

## å…«ã€Actor-Criticæ–¹æ³•

### 8.1 A2C (Advantage Actor-Critic)
```python
class A2C:
    """
    Advantage Actor-Critic (A2C)
    ä½¿ç”¨ä¼˜åŠ¿å‡½æ•°ï¼šA(s,a) = Q(s,a) - V(s)
    """
    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99, 
                 value_coef=0.5, entropy_coef=0.01):
        self.ac_net = ActorCriticNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.ac_net.parameters(), lr=lr)
        
        self.gamma = gamma
        self.value_coef = value_coef
        self.entropy_coef = entropy_coef
    
    def train_step(self, states, actions, rewards, next_states, dones):
        """
        æ‰¹é‡æ›´æ–°
        """
        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        rewards = torch.FloatTensor(rewards)
        next_states = torch.FloatTensor(next_states)
        dones = torch.FloatTensor(dones)
        
        # å‰å‘ä¼ æ’­
        action_probs, values = self.ac_net(states)
        _, next_values = self.ac_net(next_states)
        
        # è®¡ç®—ä¼˜åŠ¿
        td_targets = rewards + self.gamma * next_values.squeeze() * (1 - dones)
        advantages = td_targets - values.squeeze()
        
        # ActoræŸå¤±ï¼ˆç­–ç•¥æ¢¯åº¦ï¼‰
        dist = Categorical(action_probs)
        log_probs = dist.log_prob(actions)
        actor_loss = -(log_probs * advantages.detach()).mean()
        
        # CriticæŸå¤±ï¼ˆTDè¯¯å·®ï¼‰
        critic_loss = advantages.pow(2).mean()
        
        # ç†µæ­£åˆ™åŒ–ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
        entropy = dist.entropy().mean()
        
        # æ€»æŸå¤±
        loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy
        
        # æ›´æ–°
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.ac_net.parameters(), 0.5)
        self.optimizer.step()
        
        return loss.item(), actor_loss.item(), critic_loss.item(), entropy.item()

### 8.2 A3C (Asynchronous Advantage Actor-Critic)
```python
import torch.multiprocessing as mp

class A3C:
    """
    A3Cï¼šå¼‚æ­¥ä¼˜åŠ¿Actor-Critic
    ä½¿ç”¨å¤šä¸ªå¹¶è¡Œworkeræ”¶é›†ç»éªŒ
    """
    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99):
        # å…¨å±€ç½‘ç»œï¼ˆå…±äº«å‚æ•°ï¼‰
        self.global_net = ActorCriticNetwork(state_dim, action_dim)
        self.global_net.share_memory()
        
        self.optimizer = optim.Adam(self.global_net.parameters(), lr=lr)
        self.gamma = gamma
    
    def worker(self, worker_id, env_fn, num_episodes):
        """
        Workerè¿›ç¨‹ï¼šç‹¬ç«‹é‡‡æ ·å’Œè®¡ç®—æ¢¯åº¦
        """
        # åˆ›å»ºæœ¬åœ°ç½‘ç»œ
        local_net = ActorCriticNetwork(state_dim, action_dim)
        env = env_fn()
        
        for episode in range(num_episodes):
            # åŒæ­¥å…¨å±€å‚æ•°åˆ°æœ¬åœ°
            local_net.load_state_dict(self.global_net.state_dict())
            
            # æ”¶é›†è½¨è¿¹
            states, actions, rewards = [], [], []
            state = env.reset()
            done = False
            
            while not done:
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                action_probs, _ = local_net(state_tensor)
                dist = Categorical(action_probs)
                action = dist.sample()
                
                next_state, reward, done, _ = env.step(action.item())
                
                states.append(state)
                actions.append(action.item())
                rewards.append(reward)
                
                state = next_state
            
            # è®¡ç®—æŸå¤±å¹¶æ›´æ–°å…¨å±€ç½‘ç»œ
            self.update_global(local_net, states, actions, rewards)
    
    def update_global(self, local_net, states, actions, rewards):
        """æ›´æ–°å…¨å±€ç½‘ç»œ"""
        # è®¡ç®—å›æŠ¥
        returns = []
        G = 0
        for reward in reversed(rewards):
            G = reward + self.gamma * G
            returns.insert(0, G)
        returns = torch.FloatTensor(returns)
        
        # è®¡ç®—æŸå¤±
        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        
        action_probs, values = local_net(states)
        dist = Categorical(action_probs)
        log_probs = dist.log_prob(actions)
        
        advantages = returns - values.squeeze().detach()
        
        actor_loss = -(log_probs * advantages).mean()
        critic_loss = advantages.pow(2).mean()
        entropy = dist.entropy().mean()
        
        loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy
        
        # è®¡ç®—æ¢¯åº¦
        self.optimizer.zero_grad()
        loss.backward()
        
        # å°†æœ¬åœ°æ¢¯åº¦åº”ç”¨åˆ°å…¨0
                for next_state in self.mdp.states:
                    trans_prob = self.mdp.get_transition_prob(state, action, next_state)
                    reward = self.mdp.get_reward(state, action, next_state)
                    q += trans_prob * (reward + self.mdp.gamma * V[next_state])
                q_values[action] = q
            
            # é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
            best_action = max(q_values, key=q_values.get)
            new_policy[state] = best_action
            
            # æ£€æŸ¥ç­–ç•¥æ˜¯å¦æ”¹å˜
            if state in new_policy and new_policy[state] != best_action:
                policy_stable = False
        
        return new_policy, policy_stable
```

### 3.3 ç­–ç•¥è¿­ä»£
```python
class PolicyIteration:
    """
    ç­–ç•¥è¿­ä»£ï¼šäº¤æ›¿è¿›è¡Œç­–ç•¥è¯„ä¼°å’Œç­–ç•¥æ”¹è¿›
    1. ç­–ç•¥è¯„ä¼°ï¼šè®¡ç®—V^Ï€
    2. ç­–ç•¥æ”¹è¿›ï¼šÏ€' = greedy(V^Ï€)
    3. é‡å¤ç›´åˆ°ç­–ç•¥æ”¶æ•›
    """
    def __init__(self, mdp, theta=1e-6):
        self.mdp = mdp
        self.theta = theta
    
    def iterate(self, max_iterations=100):
        """ç­–ç•¥è¿­ä»£ä¸»å¾ªç¯"""
        # åˆå§‹åŒ–éšæœºç­–ç•¥
        policy = {s: np.random.choice(self.mdp.actions) 
                 for s in self.mdp.states}
        V = {s: 0 for s in self.mdp.states}
        
        for iteration in range(max_iterations):
            # 1. ç­–ç•¥è¯„ä¼°
            V = self.policy_evaluation(policy, V)
            
            # 2. ç­–ç•¥æ”¹è¿›
            new_policy, policy_stable = self.policy_improvement(V)
            
            if policy_stable:
                print(f"ç­–ç•¥è¿­ä»£æ”¶æ•›äºç¬¬ {iteration+1} æ¬¡è¿­ä»£")
                break
            
            policy = new_policy
        
        return policy, V
    
    def policy_evaluation(self, policy, V):
        """ç­–ç•¥è¯„ä¼°å­è¿‡ç¨‹"""
        while True:
            delta = 0
            new_V = V.copy()
            
            for state in self.mdp.states:
                if self.mdp.is_terminal(state):
                    continue
                
                action = policy[state]
                new_value = 0
                
                for next_state in self.mdp.states:
                    trans_prob = self.mdp.get_transition_prob(state, action, next_state)
                    reward = self.mdp.get_reward(state, action, next_state)
                    new_value += trans_prob * (reward + self.mdp.gamma * V[next_state])
                
                new_V[state] = new_value
                delta = max(delta, abs(V[state] - new_value))
            
            V = new_V
            
            if delta < self.theta:
                break
        
        return V
    
    def policy_improvement(self, V):
        """ç­–ç•¥æ”¹è¿›å­è¿‡ç¨‹"""
        new_policy = {}
        policy_stable = True
        
        for state in self.mdp.states:
            if self.mdp.is_terminal(state):
                continue
            
            # è®¡ç®—Qå€¼
            q_values = {}
            for action in self.mdp.actions:
                q = 0
                for next_state in self.mdp.states:
                    trans_prob = self.mdp.get_transition_prob(state, action, next_state)
                    reward = self.mdp.get_reward(state, action, next_state)
                    q += trans_prob * (reward + self.mdp.gamma * V[next_state])
                q_values[action] = q
            
            best_action = max(q_values, key=q_values.get)
            new_policy[state] = best_action
        
        return new_policy, policy_stable
```

### 3.4 ä»·å€¼è¿­ä»£
```python
class ValueIteration:
    """
    ä»·å€¼è¿­ä»£ï¼šç›´æ¥è¿­ä»£æœ€ä¼˜è´å°”æ›¼æ–¹ç¨‹
    V_{k+1}(s) = max_a Î£_{s'} P(s'|s,a)[R(s,a,s') + Î³V_k(s')]
    
    ç›¸æ¯”ç­–ç•¥è¿­ä»£ï¼Œä»·å€¼è¿­ä»£æ›´ç®€å•é«˜æ•ˆ
    """
    def __init__(self, mdp, theta=1e-6):
        self.mdp = mdp
        self.theta = theta
    
    def iterate(self, max_iterations=1000):
        """ä»·å€¼è¿­ä»£ä¸»å¾ªç¯"""
        # åˆå§‹åŒ–ä»·å€¼å‡½æ•°
        V = {s: 0 for s in self.mdp.states}
        
        for iteration in range(max_iterations):
            delta = 0
            new_V = V.copy()
            
            for state in self.mdp.states:
                if self.mdp.is_terminal(state):
                    continue
                
                # è´å°”æ›¼æœ€ä¼˜æ›´æ–°
                max_value = float('-inf')
                
                for action in self.mdp.actions:
                    value = 0
                    for next_state in self.mdp.states:
                        trans_prob = self.mdp.get_transition_prob(state, action, next_state)
                        reward = self.mdp.get_reward(state, action, next_state)
                        value += trans_prob * (reward + self.mdp.gamma * V[next_state])
                    
                    max_value = max(max_value, value)
                
                new_V[state] = max_value
                delta = max(delta, abs(V[state] - new_value))
            
            V = new_V
            
            if delta < self.theta:
                print(f"ä»·å€¼è¿­ä»£æ”¶æ•›äºç¬¬ {iteration+1} æ¬¡è¿­ä»£")
                break
        
        # æå–æœ€ä¼˜ç­–ç•¥
        policy = self.extract_policy(V)
        
        return policy, V
    
    def extract_policy(self, V):
        """ä»ä»·å€¼å‡½æ•°æå–ç­–ç•¥"""
        policy = {}
        
        for state in self.mdp.states:
            if self.mdp.is_terminal(state):
                continue
            
            q_values = {}
            for action in self.mdp.actions:
                q = 0
                for next_state in self.mdp.states:
                    trans_prob = self.mdp.get_transition_prob(state, action, next_state)
                    reward = self.mdp.get_reward(state, action, next_state)
                    q += trans_prob * (reward + self.mdp.gamma * V[next_state])
                q_values[action] = q
            
            policy[state] = max(q_values, key=q_values.get)
        
        return policy
```

---

## ä¹ã€æ·±åº¦å¼ºåŒ–å­¦ä¹ 

### 9.1 DQN (Deep Q-Network)

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from collections import deque, namedtuple
import random

# ç»éªŒå›æ”¾ç¼“å†²åŒº
Transition = namedtuple('Transition', 
                       ('state', 'action', 'reward', 'next_state', 'done'))

class ReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        """æ·»åŠ ç»éªŒ"""
        self.buffer.append(Transition(state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        """éšæœºé‡‡æ ·æ‰¹é‡ç»éªŒ"""
        transitions = random.sample(self.buffer, batch_size)
        batch = Transition(*zip(*transitions))
        
        states = torch.FloatTensor(batch.state)
        actions = torch.LongTensor(batch.action)
        rewards = torch.FloatTensor(batch.reward)
        next_states = torch.FloatTensor(batch.next_state)
        dones = torch.FloatTensor(batch.done)
        
        return states, actions, rewards, next_states, dones
    
    def __len__(self):
        return len(self.buffer)

class DQNNetwork(nn.Module):
    """DQNç½‘ç»œæ¶æ„"""
    def __init__(self, state_dim, action_dim, hidden_dims=[128, 128]):
        super(DQNNetwork, self).__init__()
        
        layers = []
        input_dim = state_dim
        
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(input_dim, hidden_dim))
            layers.append(nn.ReLU())
            input_dim = hidden_dim
        
        layers.append(nn.Linear(input_dim, action_dim))
        self.network = nn.Sequential(*layers)
    
    def forward(self, state):
        return self.network(state)

class DQN:
    """
    Deep Q-Network
    
    å…³é”®æŠ€æœ¯ï¼š
    1. ç»éªŒå›æ”¾ (Experience Replay)
    2. ç›®æ ‡ç½‘ç»œ (Target Network)
    3. Îµ-è´ªå¿ƒæ¢ç´¢
    """
    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99,
                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,
                 buffer_size=10000, batch_size=64, target_update=10):
        
        # Qç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.q_net = DQNNetwork(state_dim, action_dim)
        self.target_net = DQNNetwork(state_dim, action_dim)
        self.target_net.load_state_dict(self.q_net.state_dict())
        
        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)
        
        # ç»éªŒå›æ”¾
        self.replay_buffer = ReplayBuffer(buffer_size)
        
        # è¶…å‚æ•°
        self.gamma = gamma
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        self.batch_size = batch_size
        self.target_update = target_update
        
        self.action_dim = action_dim
        self.update_counter = 0
    
    def select_action(self, state, training=True):
        """Îµ-è´ªå¿ƒåŠ¨ä½œé€‰æ‹©"""
        if training and random.random() < self.epsilon:
            return random.randrange(self.action_dim)
        
        with torch.no_grad():
            state = torch.FloatTensor(state).unsqueeze(0)
            q_values = self.q_net(state)
            return q_values.argmax().item()
    
    def train_step(self):
        """SACè®­ç»ƒæ­¥éª¤"""
        if len(self.replay_buffer) < self.batch_size:
            return None
        
        # é‡‡æ ·
        states, actions, rewards, next_states, dones = \
            self.replay_buffer.sample(self.batch_size)
        
        # æ›´æ–°Critic
        with torch.no_grad():
            next_actions, next_log_probs, _ = self.actor.sample(next_states)
            target_q1 = self.critic1_target(next_states, next_actions)
            target_q2 = self.critic2_target(next_states, next_actions)
            target_q = torch.min(target_q1, target_q2) - self.alpha * next_log_probs
            target_q = rewards.unsqueeze(1) + self.gamma * target_q * (1 - dones.unsqueeze(1))
        
        current_q1 = self.critic1(states, actions)
        current_q2 = self.critic2(states, actions)
        
        critic1_loss = F.mse_loss(current_q1, target_q)
        critic2_loss = F.mse_loss(current_q2, target_q)
        
        self.critic1_optimizer.zero_grad()
        critic1_loss.backward()
        self.critic1_optimizer.step()
        
        self.critic2_optimizer.zero_grad()
        critic2_loss.backward()
        self.critic2_optimizer.step()
        
        # æ›´æ–°Actor
        new_actions, log_probs, _ = self.actor.sample(states)
        q1_new = self.critic1(states, new_actions)
        q2_new = self.critic2(states, new_actions)
        q_new = torch.min(q1_new, q2_new)
        
        actor_loss = (self.alpha * log_probs - q_new).mean()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        # æ›´æ–°æ¸©åº¦å‚æ•°
        if self.auto_entropy:
            alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()
            
            self.alpha_optimizer.zero_grad()
            alpha_loss.backward()
            self.alpha_optimizer.step()
            
            self.alpha = self.log_alpha.exp().item()
        
        # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.soft_update(self.critic1, self.critic1_target)
        self.soft_update(self.critic2, self.critic2_target)
        
        return actor_loss.item(), critic1_loss.item(), critic2_loss.item()
    
    def soft_update(self, source, target):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        for param, target_param in zip(source.parameters(), target.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)

class GaussianPolicy(nn.Module):
    """é«˜æ–¯ç­–ç•¥ç½‘ç»œ"""
    def __init__(self, state_dim, action_dim, max_action, hidden_dim=256):
        super(GaussianPolicy, self).__init__()
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        self.mean = nn.Linear(hidden_dim, action_dim)
        self.log_std = nn.Linear(hidden_dim, action_dim)
        
        self.max_action = max_action
    
    def forward(self, state):
        x = self.network(state)
        mean = self.mean(x)
        log_std = self.log_std(x)
        log_std = torch.clamp(log_std, -20, 2)
        return mean, log_std
    
    def sample(self, state):
        """é‡‡æ ·åŠ¨ä½œ"""
        mean, log_std = self.forward(state)
        std = log_std.exp()
        
        # é‡å‚æ•°åŒ–æŠ€å·§
        normal = torch.distributions.Normal(mean, std)
        x_t = normal.rsample()
        action = torch.tanh(x_t)
        
        # è®¡ç®—logæ¦‚ç‡
        log_prob = normal.log_prob(x_t)
        log_prob -= torch.log(1 - action.pow(2) + 1e-6)
        log_prob = log_prob.sum(dim=-1, keepdim=True)
        
        action = action * self.max_action
        mean = torch.tanh(mean) * self.max_action
        
        return action, log_prob, mean
```

### 10.4 TD3 (Twin Delayed DDPG)

```python
class TD3:
    """
    TD3ï¼šåŒå»¶è¿ŸDDPG
    
    æ”¹è¿›ï¼š
    1. åŒCriticç½‘ç»œï¼ˆå‡å°‘è¿‡ä¼°è®¡ï¼‰
    2. å»¶è¿Ÿç­–ç•¥æ›´æ–°
    3. ç›®æ ‡ç­–ç•¥å¹³æ»‘
    """
    def __init__(self, state_dim, action_dim, max_action,
                 actor_lr=1e-3, critic_lr=1e-3, gamma=0.99, tau=0.005,
                 policy_noise=0.2, noise_clip=0.5, policy_delay=2):
        
        # Actor
        self.actor = DDPGActor(state_dim, action_dim, max_action)
        self.actor_target = DDPGActor(state_dim, action_dim, max_action)
        self.actor_target.load_state_dict(self.actor.state_dict())
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)
        
        # åŒCritic
        self.critic1 = DDPGCritic(state_dim, action_dim)
        self.critic2 = DDPGCritic(state_dim, action_dim)
        self.critic1_target = DDPGCritic(state_dim, action_dim)
        self.critic2_target = DDPGCritic(state_dim, action_dim)
        
        self.critic1_target.load_state_dict(self.critic1.state_dict())
        self.critic2_target.load_state_dict(self.critic2.state_dict())
        
        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=critic_lr)
        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=critic_lr)
        
        self.max_action = max_action
        self.gamma = gamma
        self.tau = tau
        self.policy_noise = policy_noise
        self.noise_clip = noise_clip
        self.policy_delay = policy_delay
        
        self.replay_buffer = ReplayBuffer(100000)
        self.batch_size = 256
        self.total_iterations = 0
    
    def select_action(self, state, noise=0.1):
        """é€‰æ‹©åŠ¨ä½œ"""
        state = torch.FloatTensor(state).unsqueeze(0)
        action = self.actor(state).detach().numpy()[0]
        
        if noise > 0:
            action += np.random.normal(0, noise, size=action.shape)
            action = np.clip(action, -self.max_action, self.max_action)
        
        return action
    
    def train_step(self):
        """TD3è®­ç»ƒæ­¥éª¤"""
        if len(self.replay_buffer) < self.batch_size:
            return None
        
        self.total_iterations += 1
        
        # é‡‡æ ·
        states, actions, rewards, next_states, dones = \
            self.replay_buffer.sample(self.batch_size)
        
        # æ›´æ–°Critic
        with torch.no_grad():
            # ç›®æ ‡ç­–ç•¥å¹³æ»‘
            noise = (torch.randn_like(actions) * self.policy_noise).clamp(
                -self.noise_clip, self.noise_clip
            )
            next_actions = (self.actor_target(next_states) + noise).clamp(
                -self.max_action, self.max_action
            )
            
            # è®¡ç®—ç›®æ ‡Qå€¼ï¼ˆå–æœ€å°å€¼ï¼‰
            target_q1 = self.critic1_target(next_states, next_actions)
            target_q2 = self.critic2_target(next_states, next_actions)
            target_q = torch.min(target_q1, target_q2)
            target_q = rewards.unsqueeze(1) + self.gamma * target_q * (1 - dones.unsqueeze(1))
        
        # æ›´æ–°ä¸¤ä¸ªCritic
        current_q1 = self.critic1(states, actions)
        current_q2 = self.critic2(states, actions)
        
        critic1_loss = F.mse_loss(current_q1, target_q)
        critic2_loss = F.mse_loss(current_q2, target_q)
        
        self.critic1_optimizer.zero_grad()
        critic1_loss.backward()
        self.critic1_optimizer.step()
        
        self.critic2_optimizer.zero_grad()
        critic2_loss.backward()
        self.critic2_optimizer.step()
        
        # å»¶è¿Ÿç­–ç•¥æ›´æ–°
        if self.total_iterations % self.policy_delay == 0:
            # æ›´æ–°Actor
            actor_loss = -self.critic1(states, self.actor(states)).mean()
            
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            self.actor_optimizer.step()
            
            # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
            self.soft_update(self.actor, self.actor_target)
            self.soft_update(self.critic1, self.critic1_target)
            self.soft_update(self.critic2, self.critic2_target)
            
            return actor_loss.item(), critic1_loss.item()
        
        return None, critic1_loss.item()
    
    def soft_update(self, source, target):
        """è½¯æ›´æ–°"""
        for param, target_param in zip(source.parameters(), target.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
```

---

## åä¸€ã€å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ 

### 11.1 ç‹¬ç«‹å­¦ä¹ 

```python
class IndependentQLearning:
    """
    ç‹¬ç«‹Qå­¦ä¹ ï¼šæ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹å­¦ä¹ 
    å°†å…¶ä»–æ™ºèƒ½ä½“è§†ä¸ºç¯å¢ƒçš„ä¸€éƒ¨åˆ†
    """
    def __init__(self, num_agents, state_dim, action_dim, lr=0.1, gamma=0.99, epsilon=0.1):
        self.num_agents = num_agents
        self.agents = [
            QLearning(None, gamma, lr, epsilon) 
            for _ in range(num_agents)
        ]
    
    def select_actions(self, states):
        """æ‰€æœ‰æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ"""
        return [agent.select_action(state) for agent, state in zip(self.agents, states)]
    
    def update(self, states, actions, rewards, next_states, dones):
        """æ›´æ–°æ‰€æœ‰æ™ºèƒ½ä½“"""
        for i, agent in enumerate(self.agents):
            agent.update_q(states[i], actions[i], rewards[i], next_states[i], dones[i])
```

### 11.2 MADDPG (Multi-Agent DDPG)

```python
class MADDPG:
    """
    MADDPGï¼šå¤šæ™ºèƒ½ä½“DDPG
    
    å…³é”®æ€æƒ³ï¼š
    - é›†ä¸­å¼è®­ç»ƒï¼ˆCriticçœ‹åˆ°å…¨å±€ä¿¡æ¯ï¼‰
    - åˆ†å¸ƒå¼æ‰§è¡Œï¼ˆActoråªç”¨å±€éƒ¨ä¿¡æ¯ï¼‰
    """
    def __init__(self, num_agents, state_dims, action_dims, 
                 actor_lr=1e-4, critic_lr=1e-3, gamma=0.99, tau=0.01):
        
        self.num_agents = num_agents
        self.agents = []
        
        # å…¨å±€çŠ¶æ€å’ŒåŠ¨ä½œç»´åº¦
        total_state_dim = sum(state_dims)
        total_action_dim = sum(action_dims)
        
        for i in range(num_agents):
            agent = {
                'actor': DDPGActor(state_dims[i], action_dims[i], 1.0),
                'actor_target': DDPGActor(state_dims[i], action_dims[i], 1.0),
                'critic': DDPGCritic(total_state_dim, total_action_dim),
                'critic_target': DDPGCritic(total_state_dim, total_action_dim),
                'actor_optimizer': None,
                'critic_optimizer': None
            }
            
            agent['actor_target'].load_state_dict(agent['actor'].state_dict())
            agent['critic_target'].load_state_dict(agent['critic'].state_dict())
            
            agent['actor_optimizer'] = optim.Adam(agent['actor'].parameters(), lr=actor_lr)
            agent['critic_optimizer'] = optim.Adam(agent['critic'].parameters(), lr=critic_lr)
            
            self.agents.append(agent)
        
        self.gamma = gamma
        self.tau = tau
        self.replay_buffer = ReplayBuffer(100000)
        self.batch_size = 1024
    
    def select_actions(self, states, noise=0.0):
        """æ‰€æœ‰æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ"""
        actions = []
        for i, state in enumerate(states):
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            action = self.agents[i]['actor'](state_tensor).detach().numpy()[0]
            
            if noise > 0:
                action += np.random.normal(0, noise, size=action.shape)
            
            actions.append(action)
        
        return actions
    
    def train_step(self):
        """MADDPGè®­ç»ƒæ­¥éª¤"""
        if len(self.replay_buffer) < self.batch_size:
            return None
        
        # é‡‡æ ·ï¼ˆå‡è®¾bufferå­˜å‚¨äº†å…¨å±€ä¿¡æ¯ï¼‰
        states, actions, rewards, next_states, dones = \
            self.replay_buffer.sample(self.batch_size)
        
        # states: [batch, num_agents, state_dim]
        # actions: [batch, num_agents, action_dim]
        
        # å±•å¹³å…¨å±€çŠ¶æ€å’ŒåŠ¨ä½œ
        batch_size = states.shape[0]
        global_states = states.reshape(batch_size, -1)
        global_actions = actions.reshape(batch_size, -1)
        global_next_states = next_states.reshape(batch_size, -1)
        
        for agent_id in range(self.num_agents):
            agent = self.agents[agent_id]
            
            # æ›´æ–°Criticï¼ˆä½¿ç”¨å…¨å±€ä¿¡æ¯ï¼‰
            with torch.no_grad():
                # è·å–æ‰€æœ‰æ™ºèƒ½ä½“çš„ä¸‹ä¸€ä¸ªåŠ¨ä½œ
                next_actions_list = []
                for i in range(self.num_agents):
                    next_action = self.agents[i]['actor_target'](next_states[:, i])
                    next_actions_list.append(next_action)
                
                global_next_actions = torch.cat(next_actions_list, dim=-1)
                
                target_q = agent['critic_target'](global_next_states, global_next_actions)
                target_q = rewards[:, agent_id].unsqueeze(1) + \
                          self.gamma * target_q * (1 - dones[:, agent_id].unsqueeze(1))
            
            current_q = agent['critic'](global_states, global_actions)
            critic_loss = F.mse_loss(current_q, target_q)
            
            agent['critic_optimizer'].zero_grad()
            critic_loss.backward()
            agent['critic_optimizer'].step()
            
            # æ›´æ–°Actorï¼ˆåªç”¨å±€éƒ¨çŠ¶æ€ï¼‰
            # æ„é€ å½“å‰æ™ºèƒ½ä½“çš„åŠ¨ä½œï¼Œå…¶ä»–æ™ºèƒ½ä½“åŠ¨ä½œæ¥è‡ªå½“å‰ç­–ç•¥
            actions_list = []
            for i in range(self.num_agents):
                if i == agent_id:
                    action = agent['actor'](states[:, i])
                else:
                    action = self.agents[i]['actor'](states[:, i]).detach()
                actions_list.append(action)
            
            global_actions_for_actor = torch.cat(actions_list, dim=-1)
            
            actor_loss = -agent['critic'](global_states, global_actions_for_actor).mean()
            
            agent['actor_optimizer'].zero_grad()
            actor_loss.backward()
            agent['actor_optimizer'].step()
            
            # è½¯æ›´æ–°
            self.soft_update(agent['actor'], agent['actor_target'])
            self.soft_update(agent['critic'], agent['critic_target'])
    
    def soft_update(self, source, target):
        """è½¯æ›´æ–°"""
        for param, target_param in zip(source.parameters(), target.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
```

---

## åäºŒã€æ¨¡å‹å­¦ä¹ 

### 12.1 Dyna-Q

```python
class DynaQ:
    """
    Dyna-Qï¼šç»“åˆæ¨¡å‹å­¦ä¹ å’Œæ— æ¨¡å‹å­¦ä¹ 
    
    æµç¨‹ï¼š
    1. ä¸ç¯å¢ƒäº¤äº’ï¼ˆçœŸå®ç»éªŒï¼‰
    2. æ›´æ–°Qå€¼
    3. æ›´æ–°ç¯å¢ƒæ¨¡å‹
    4. ç”¨æ¨¡å‹ç”Ÿæˆæ¨¡æ‹Ÿç»éªŒ
    5. ç”¨æ¨¡æ‹Ÿç»éªŒæ›´æ–°Qå€¼
    """
    def __init__(self, state_dim, action_dim, lr=0.1, gamma=0.99, 
                 epsilon=0.1, planning_steps=5):
        
        self.Q = {}
        self.model = {}  # model[s][a] = (r, s')
        
        self.lr = lr
        self.gamma = gamma
        self.epsilon = epsilon
        self.planning_steps = planning_steps
        
        self.action_dim = action_dim
        self.visited_states = set()
    
    def select_action(self, state):
        """Îµ-è´ªå¿ƒé€‰æ‹©åŠ¨ä½œ"""
        if random.random() < self.epsilon:
            return random.randrange(self.action_dim)
        
        if state not in self.Q or len(self.Q[state]) == 0:
            return random.randrange(self.action_dim)
        
        return max(self.Q[state], key=self.Q[state].get)
    
    def update(self, state, action, reward, next_state, done):
        """Dyna-Qæ›´æ–°"""
        # 1. ç›´æ¥RLæ›´æ–°ï¼ˆçœŸå®ç»éªŒï¼‰
        if state not in self.Q:
            self.Q[state] = {}
        if action not in self.Q[state]:
            self.Q[state][action] = 0
        
        if not done:
            if next_state not in self.Q or len(self.Q[next_state]) == 0:
                max_next_q = 0
            else:
                max_next_q = max(self.Q[next_state].values())
            td_target = reward + self.gamma * max_next_q
        else:
            td_target = reward
        
        self.Q[state][action] += self.lr * (td_target - self.Q[state][action])
        
        # 2. æ›´æ–°æ¨¡å‹
        if state not in self.model:
            self.model[state] = {}
        self.model[state][action] = (reward, next_state, done)
        self.visited_states.add(state)
        
        # 3. è§„åˆ’ï¼ˆä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ¨¡æ‹Ÿç»éªŒï¼‰
        for _ in range(self.planning_steps):
            # éšæœºé€‰æ‹©è®¿é—®è¿‡çš„çŠ¶æ€
            s = random.choice(list(self.visited_states))
            
            # éšæœºé€‰æ‹©è¯¥çŠ¶æ€ä¸‹æ‰§è¡Œè¿‡çš„åŠ¨ä½œ
            if s not in self.model or len(self.model[s]) == 0:
                continue
            
            a = random.choice(list(self.model[s].keys()))
            
            # ä»æ¨¡å‹è·å–è½¬ç§»
            r, s_next, d = self.model[s][a]
            
            # æ›´æ–°Qå€¼ï¼ˆæ¨¡æ‹Ÿç»éªŒï¼‰
            if s not in self.Q:
                self.Q[s] = {}
            if a not in self.Q[s]:
                self.Q[s][a] = 0
            
            if not d:
                if s_next not in self.Q or len(self.Q[s_next]) == 0:
                    max_next_q = 0
                else:
                    max_next_q = max(self.Q[s_next].values())
                td_target = r + self.gamma * max_next_q
            else:
                td_target = r
            
            self.Q[s][a] += self.lr * (td_target - self.Q[s][a])
```

### 12.2 ä¸–ç•Œæ¨¡å‹

```python
class WorldModel(nn.Module):
    """
    ä¸–ç•Œæ¨¡å‹ï¼šå­¦ä¹ ç¯å¢ƒåŠ¨æ€
    é¢„æµ‹ä¸‹ä¸€ä¸ªçŠ¶æ€å’Œå¥–åŠ±
    """
    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super(WorldModel, self).__init__()
        
        self.network = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # é¢„æµ‹ä¸‹ä¸€ä¸ªçŠ¶æ€
        self.next_state_head = nn.Linear(hidden_dim, state_dim)
        
        # é¢„æµ‹å¥–åŠ±
        self.reward_head = nn.Linear(hidden_dim, 1)
        
        # é¢„æµ‹ç»ˆæ­¢
        self.done_head = nn.Linear(hidden_dim, 1)
    
    def forward(self, state, action):
        """
        é¢„æµ‹ï¼š(next_state, reward, done)
        """
        x = torch.cat([state, action], dim=-1)
        features = self.network(x)
        
        next_state = self.next_state_head(features)
        reward = self.reward_head(features)
        done = torch.sigmoid(self.done_head(features))
        
        return next_state, reward, done

class MBPO:
    """
    Model-Based Policy Optimization
    ç»“åˆæ¨¡å‹å­¦ä¹ å’Œç­–ç•¥ä¼˜åŒ–
    """
    def __init__(self, state_dim, action_dim, max_action):
        # ä¸–ç•Œæ¨¡å‹
        self.world_model = WorldModel(state_dim, action_dim)
        self.model_optimizer = optim.Adam(self.world_model.parameters(), lr=1e-3)
        
        # ç­–ç•¥ï¼ˆä½¿ç”¨SACï¼‰
        self.policy = SAC(state_dim, action_dim, max_action)
        
        # çœŸå®å’Œæ¨¡æ‹Ÿç»éªŒç¼“å†²åŒº
        self.real_buffer = ReplayBuffer(100000)
        self.model_buffer = ReplayBuffer(100000)
    
    def train_world_model(self, num_epochs=5):
        """è®­ç»ƒä¸–ç•Œæ¨¡å‹"""
        for _ in range(num_epochs):
            if len(self.real_buffer) < 256:
                continue
            
            states, actions, rewards, next_states, dones = \
                self.real_buffer.sample(256)
            
            # é¢„æµ‹
            pred_next_states, pred_rewards, pred_dones = \
                self.world_model(states, actions)
            
            # è®¡ç®—æŸå¤±
            state_loss = F.mse_loss(pred_next_states, next_states)
            reward_loss = F.mse_loss(pred_rewards.squeeze(), rewards)
            done_loss = F.binary_cross_entropy(pred_dones.squeeze(), dones)
            
            loss = state_loss + reward_loss + done_loss
            
            # æ›´æ–°
            self.model_optimizer.zero_grad()
            loss.backward()
            self.model_optimizer.step()
    
    def generate_model_data(self, num_samples=10000):
        """ä½¿ç”¨ä¸–ç•Œæ¨¡å‹ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®"""
        if len(self.real_buffer) < 256:
            return
        
        # ä»çœŸå®æ•°æ®é‡‡æ ·åˆå§‹çŠ¶æ€
        states, _, _, _, _ = self.real_buffer.sample(num_samples)
        
        for state in states:
            # ä½¿ç”¨å½“å‰ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
            action = self.policy.select_action(state.numpy())
            
            # ä½¿ç”¨ä¸–ç•Œæ¨¡å‹é¢„æµ‹è½¬ç§»
            with torch.no_grad():
                state_tensor = state.unsqueeze(0)
                action_tensor = torch.FloatTensor(action).unsqueeze(0)
                
                next_state, reward, done = self.world_model(state_tensor, action_tensor)
                
                next_state = next_state.squeeze().numpy()
                reward = reward.item()
                done = (done.item() > 0.5)
            
            # å­˜å‚¨åˆ°æ¨¡å‹ç¼“å†²åŒº
            self.model_buffer.push(state.numpy(), action, reward, next_state, done)
```

---

## åä¸‰ã€é€†å¼ºåŒ–å­¦ä¹ 

### 13.1 æœ€å¤§ç†µIRL

```python
class MaxEntIRL:
    """
    æœ€å¤§ç†µé€†å¼ºåŒ–å­¦ä¹ 
    ä»ä¸“å®¶æ¼”ç¤ºä¸­å­¦ä¹ å¥–åŠ±å‡½æ•°
    """
    def __init__(self, state_dim, action_dim, lr=0.01):
        # å¥–åŠ±å‡½æ•°å‚æ•°åŒ–
        self.reward_net = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        self.optimizer = optim.Adam(self.reward_net.parameters(), lr=lr)
    
    def compute_reward(self, state):
        """è®¡ç®—å¥–åŠ±"""
        state = torch.FloatTensor(state)
        return self.reward_net(state).item()
    
    def train(self, expert_trajectories, policy, num_iterations=100):
        """
        è®­ç»ƒIRL
        
        ç›®æ ‡ï¼šæœ€å¤§åŒ–ä¸“å®¶è½¨è¿¹çš„ä¼¼ç„¶
        åŒæ—¶æœ€å°åŒ–ç­–ç•¥è½¨è¿¹ä¸ä¸“å®¶è½¨è¿¹çš„å·®å¼‚
        """
        for iteration in range(num_iterations):
            # 1. è®¡ç®—ä¸“å®¶è½¨è¿¹çš„ç‰¹å¾æœŸæœ›
            expert_features = self.compute_feature_expectations(expert_trajectories)
            
            # 2. ç”¨å½“å‰å¥–åŠ±è®­ç»ƒç­–ç•¥
            policy.train_with_reward(self.reward_net, num_episodes=10)
            
            # 3. ç”Ÿæˆç­–ç•¥è½¨è¿¹
            policy_trajectories = policy.generate_trajectories(num_episodes=10)
            
            # 4. è®¡ç®—ç­–ç•¥è½¨è¿¹çš„ç‰¹å¾æœŸæœ›
            policy_features = self.compute_feature_expectations(policy_trajectories)
            
            # 5. æ›´æ–°å¥–åŠ±å‡½æ•°
            loss = -torch.sum(expert_features * torch.log(policy_features + 1e-8))
            
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
    
    def compute_feature_expectations(self, trajectories):
        """è®¡ç®—ç‰¹å¾æœŸæœ›"""
        features = []
        for traj in trajectories:
            for state, action, _ in traj:
                state_tensor = torch.FloatTensor(state)
                feature = self.reward_net(state_tensor)
                features.append(feature)
        
        return torch.stack(features).mean(dim=0)
```

---

## åå››ã€å®ç”¨æŠ€å·§ä¸è°ƒè¯•

### 14.1 è¶…å‚æ•°è°ƒä¼˜

```python
class HyperparameterTuning:
    """è¶…å‚æ•°è°ƒä¼˜å»ºè®®"""
    
    @staticmethod
    def learning_rate_schedule():
        """å­¦ä¹ ç‡è°ƒåº¦"""
        # 1. çº¿æ€§è¡°å‡
        def linear_decay(initial_lr, final_lr, total_steps):
            def schedule(step):
                fraction = min(step / total_steps, 1.0)
                return initial_lr - (initial_lr - final_lr) * fraction
            return schedule
        
        # 2. ä½™å¼¦é€€ç«
        def cosine_annealing(initial_lr, final_lr, total_steps):
            def schedule(step):
                fraction = step / total_steps
                return final_lr + 0.5 * (initial_lr - final_lr) * \
                       (1 + np.cos(np.pi * fraction))
            return schedule
        
        return linear_decay, cosine_annealing
    
    @staticmethod
    def exploration_schedule():
        """æ¢ç´¢ç­–ç•¥è°ƒåº¦"""
        # Îµ-è´ªå¿ƒè¡°å‡
        def epsilon_decay(start=1.0, end=0.01, decay_steps=10000):
            def schedule(step):
                return max(end, start - (start - end) * min(step / decay_steps, 1.0))
            return schedule
        
        return epsilon_decay
    
    @staticmethod
    def common_ranges():
        """å¸¸ç”¨è¶…å‚æ•°èŒƒå›´"""
        return {
            'learning_rate': [1e-5, 1e-4, 3e-4, 1e-3, 3e-3],
            'gamma': [0.95, 0.99, 0.995, 0.999],
            'batch_size': [32, 64, 128, 256, 512],
            'hidden_dims': [[64, 64], [128, 128], [256, 256]],
            'buffer_size': [10000, 50000, 100000, 1000000],
            'tau': [0.001, 0.005, 0.01, 0.05]
        }
```

### 14.2 è°ƒè¯•æŠ€å·§

```python
class DebuggingTools:
    """å¼ºåŒ–å­¦ä¹ è°ƒè¯•å·¥å…·"""
    
    @staticmethod
    def plot_learning_curve(rewards, window=100):
        """ç»˜åˆ¶å­¦ä¹ æ›²çº¿"""
        import matplotlib.pyplot as plt
        
        # å¹³æ»‘å¥–åŠ±
        smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')
        
        plt.figure(figsize=(10, 6))
        plt.plot(rewards, alpha=0.3, label='Raw')
        plt.plot(smoothed, label=f'{window}-episode Moving Average')
        plt.xlabel('Episode')
        plt.ylabel('Total Reward')
        plt.legend()
        plt.title('Learning Curve')
        plt.grid(True)
        plt.show()
    
    @staticmethod
    def analyze_q_values(Q, states):
        """åˆ†æQå€¼åˆ†å¸ƒ"""
        import matplotlib.pyplot as plt
        
        q_values = []
        for state in states:
            if state in Q:
                q_values.extend(Q[state].values())
        
        plt.figure(figsize=(10, 6))
        plt.hist(q_values, bins=50)
        plt.xlabel('Q-value')
        plt.ylabel('Frequency')
        plt.title('Q-value Distribution')
        plt.grid(True)
        plt.show()
        
        print(f"Mean Q-value: {np.mean(q_values):.3f}")
        print(f"Std Q-value: {np.std(q_values):.3f}")
        print(f"Min Q-value: {np.min(q_values):.3f}")
        print(f"Max Q-value: {np.max(q_values):.3f}")
    
    @staticmethod
    def check_gradient_flow(model):
        """æ£€æŸ¥æ¢¯åº¦æµ"""
        total_norm = 0
        for p in model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        total_norm = total_norm ** 0.5
        
        print(f"Total gradient norm: {total_norm:.6f}")
        
        if total_norm < 1e-6:
            print("WARNING: Gradients are vanishing!")
        elif total_norm > 100:
            print("WARNING: Gradients are exploding!")
    
    @staticmethod
    def visualize_policy(env, policy, num_episodes=5):
        """å¯è§†åŒ–ç­–ç•¥"""
        for episode in range(num_episodes):
            state = env.reset()
            done = False
            total_reward = 0
            
            while not done:
                env.render()
                action = policy.select_action(state, deterministic=True)
                state, reward, done, _ = env.step(action)
                total_reward += reward
            
            print(f"Episode {episode + 1} Reward: {total_reward}")
        
        env.close()
    
    @staticmethod
    def log_training_stats(episode, metrics):
        """è®°å½•è®­ç»ƒç»Ÿè®¡"""
        print(f"\n=== Episode {episode} ===")
        for key, value in metrics.items():
            if isinstance(value, float):
                print(f"{key}: {value:.4f}")
            else:
                print(f"{key}: {value}")
```

---

## åäº”ã€é«˜çº§ä¸»é¢˜

### 15.1 å±‚æ¬¡å¼ºåŒ–å­¦ä¹  (HRL)

```python
class HierarchicalPolicy:
    """
    å±‚æ¬¡ç­–ç•¥ï¼šé«˜å±‚ç­–ç•¥é€‰æ‹©å­ç›®æ ‡ï¼Œä½å±‚ç­–ç•¥æ‰§è¡ŒåŠ¨ä½œ
    """
    def __init__(self, state_dim, action_dim, goal_dim):
        # é«˜å±‚ç­–ç•¥ï¼ˆå…ƒæ§åˆ¶å™¨ï¼‰
        self.high_level = PolicyNetwork(state_dim, goal_dim)
        
        # ä½å±‚ç­–ç•¥ï¼ˆæ§åˆ¶å™¨ï¼‰
        self.low_level = PolicyNetwork(state_dim + goal_dim, action_dim)
        
        self.high_optimizer = optim.Adam(self.high_level.parameters(), lr=1e-4)
        self.low_optimizer = optim.Adam(self.low_level.parameters(), lr=3e-4)
    
    def select_goal(self, state):
        """é«˜å±‚ç­–ç•¥é€‰æ‹©å­ç›®æ ‡"""
        state = torch.FloatTensor(state).unsqueeze(0)
        goal_probs = self.high_level(state)
        dist = Categorical(goal_probs)
        goal = dist.sample()
        return goal.item(), dist.log_prob(goal)
    
    def select_action(self, state, goal):
        """ä½å±‚ç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        state = torch.FloatTensor(state).unsqueeze(0)
        goal_tensor = torch.FloatTensor([goal]).unsqueeze(0)
        combined = torch.cat([state, goal_tensor], dim=-1)
        
        action_probs = self.low_level(combined)
        dist = Categorical(action_probs)
        action = dist.sample()
        return action.item(), dist.log_prob(action)
    
    def train_step(self, trajectory):
        """
        è®­ç»ƒå±‚æ¬¡ç­–ç•¥
        trajectory: [(state, goal, action, reward, next_state, done)]
        """
        # è®­ç»ƒä½å±‚ç­–ç•¥
        low_loss = 0
        for state, goal, action, reward, next_state, done in trajectory:
            _, log_prob = self.select_action(state, goal)
            low_loss += -log_prob * reward  # ç®€åŒ–ç‰ˆï¼Œå®é™…åº”ä½¿ç”¨ä¼˜åŠ¿å‡½æ•°
        
        self.low_optimizer.zero_grad()
        low_loss.backward()
        self.low_optimizer.step()
        
        # è®­ç»ƒé«˜å±‚ç­–ç•¥ï¼ˆåŸºäºå­ç›®æ ‡å®Œæˆæƒ…å†µï¼‰
        # å®ç°çœç•¥ï¼Œå–å†³äºå…·ä½“çš„å­ç›®æ ‡å®šä¹‰

class OptionCritic:
    """
    Option-Criticï¼šå­¦ä¹ é€‰é¡¹ï¼ˆoptionï¼‰çš„æ¡†æ¶
    é€‰é¡¹ = (åˆå§‹é›†åˆ, ç­–ç•¥, ç»ˆæ­¢æ¡ä»¶)
    """
    def __init__(self, state_dim, num_options, action_dim):
        self.num_options = num_options
        
        # é€‰é¡¹å†…ç­–ç•¥ Ï€(a|s,o)
        self.intra_option_policy = nn.ModuleList([
            PolicyNetwork(state_dim, action_dim) 
            for _ in range(num_options)
        ])
        
        # é€‰é¡¹ç»ˆæ­¢å‡½æ•° Î²(s,o)
        self.termination = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, num_options),
            nn.Sigmoid()
        )
        
        # é€‰é¡¹é€‰æ‹©ç­–ç•¥ Ï€_Î©(o|s)
        self.option_policy = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, num_options)
        )
        
        self.optimizer = optim.Adam(
            list(self.intra_option_policy.parameters()) + 
            list(self.termination.parameters()) + 
            list(self.option_policy.parameters()),
            lr=1e-3
        )
```

### 15.2 å…ƒå¼ºåŒ–å­¦ä¹  (Meta-RL)

```python
class MAML:
    """
    Model-Agnostic Meta-Learning for RL
    å­¦ä¹ ä¸€ä¸ªå¥½çš„åˆå§‹åŒ–ï¼Œä½¿å…¶èƒ½å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡
    """
    def __init__(self, state_dim, action_dim, alpha=0.01, beta=0.001):
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.alpha = alpha  # å†…å¾ªç¯å­¦ä¹ ç‡
        self.beta = beta    # å¤–å¾ªç¯å­¦ä¹ ç‡
        
        self.meta_optimizer = optim.Adam(self.policy.parameters(), lr=beta)
    
    def inner_loop_update(self, task_data, num_steps=1):
        """
        å†…å¾ªç¯ï¼šåœ¨å•ä¸ªä»»åŠ¡ä¸Šå¿«é€Ÿé€‚åº”
        """
        # å¤åˆ¶å½“å‰å‚æ•°
        adapted_params = [p.clone() for p in self.policy.parameters()]
        
        for _ in range(num_steps):
            # åœ¨ä»»åŠ¡æ•°æ®ä¸Šè®¡ç®—æŸå¤±
            loss = self.compute_task_loss(task_data, adapted_params)
            
            # è®¡ç®—æ¢¯åº¦
            grads = torch.autograd.grad(loss, adapted_params, create_graph=True)
            
            # æ›´æ–°å‚æ•°
            adapted_params = [p - self.alpha * g for p, g in zip(adapted_params, grads)]
        
        return adapted_params
    
    def meta_update(self, task_batch):
        """
        å¤–å¾ªç¯ï¼šå…ƒæ›´æ–°
        """
        meta_loss = 0
        
        for task in task_batch:
            # å†…å¾ªç¯é€‚åº”
            train_data, test_data = task
            adapted_params = self.inner_loop_update(train_data)
            
            # åœ¨æµ‹è¯•æ•°æ®ä¸Šè¯„ä¼°
            task_loss = self.compute_task_loss(test_data, adapted_params)
            meta_loss += task_loss
        
        meta_loss /= len(task_batch)
        
        # å…ƒæ¢¯åº¦ä¸‹é™
        self.meta_optimizer.zero_grad()
        meta_loss.backward()
        self.meta_optimizer.step()
        
        return meta_loss.item()
    
    def compute_task_loss(self, data, params):
        """è®¡ç®—ä»»åŠ¡æŸå¤±"""
        # å®ç°çœç•¥ï¼Œå–å†³äºå…·ä½“ä»»åŠ¡
        pass

class RL2:
    """
    RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning
    ä½¿ç”¨RNNä½œä¸ºå…ƒå­¦ä¹ å™¨
    """
    def __init__(self, state_dim, action_dim, hidden_dim=256):
        self.rnn = nn.LSTM(
            state_dim + action_dim + 1,  # state + prev_action + reward
            hidden_dim,
            batch_first=True
        )
        
        self.policy_head = nn.Linear(hidden_dim, action_dim)
        self.value_head = nn.Linear(hidden_dim, 1)
        
        self.optimizer = optim.Adam(
            list(self.rnn.parameters()) + 
            list(self.policy_head.parameters()) + 
            list(self.value_head.parameters()),
            lr=1e-3
        )
    
    def forward(self, history, hidden=None):
        """
        å‰å‘ä¼ æ’­
        history: [batch, seq_len, state_dim + action_dim + 1]
        """
        rnn_out, hidden = self.rnn(history, hidden)
        
        action_probs = F.softmax(self.policy_head(rnn_out), dim=-1)
        values = self.value_head(rnn_out)
        
        return action_probs, values, hidden
    
    def train_on_task_distribution(self, task_sampler, num_iterations=1000):
        """
        åœ¨ä»»åŠ¡åˆ†å¸ƒä¸Šè®­ç»ƒ
        """
        for iteration in range(num_iterations):
            # é‡‡æ ·ä¸€æ‰¹ä»»åŠ¡
            tasks = task_sampler.sample(batch_size=16)
            
            total_loss = 0
            for task in tasks:
                # æ”¶é›†ä»»åŠ¡è½¨è¿¹
                trajectory = self.collect_trajectory(task)
                
                # è®¡ç®—æŸå¤±
                loss = self.compute_loss(trajectory)
                total_loss += loss
            
            # æ›´æ–°
            self.optimizer.zero_grad()
            total_loss.backward()
            self.optimizer.step()
```

### 15.3 ç¦»çº¿å¼ºåŒ–å­¦ä¹ 

```python
class ConservativeQLearning:
    """
    CQL (Conservative Q-Learning)
    ç”¨äºç¦»çº¿RLï¼Œé¿å…å¤–æ¨è¯¯å·®
    """
    def __init__(self, state_dim, action_dim, alpha=1.0):
        self.q_net = QNetwork(state_dim, action_dim)
        self.target_net = QNetwork(state_dim, action_dim)
        self.target_net.load_state_dict(self.q_net.state_dict())
        
        self.optimizer = optim.Adam(self.q_net.parameters(), lr=3e-4)
        self.alpha = alpha  # CQLæ­£åˆ™åŒ–ç³»æ•°
    
    def train_step(self, offline_batch):
        """
        CQLè®­ç»ƒæ­¥éª¤
        
        æœ€å°åŒ–ï¼š
        CQL_loss = Î± * (log_sum_exp Q(s,a) - Q(s,a_data)) + TD_loss
        """
        states, actions, rewards, next_states, dones = offline_batch
        
        # æ ‡å‡†Q-learningç›®æ ‡
        with torch.no_grad():
            next_q = self.target_net(next_states).max(1)[0]
            target_q = rewards + 0.99 * next_q * (1 - dones)
        
        current_q = self.q_net(states).gather(1, actions.unsqueeze(1)).squeeze()
        td_loss = F.mse_loss(current_q, target_q)
        
        # CQLæƒ©ç½šé¡¹
        all_q = self.q_net(states)
        logsumexp_q = torch.logsumexp(all_q, dim=1)
        data_q = all_q.gather(1, actions.unsqueeze(1)).squeeze()
        
        cql_loss = (logsumexp_q - data_q).mean()
        
        # æ€»æŸå¤±
        loss = td_loss + self.alpha * cql_loss
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return loss.item(), td_loss.item(), cql_loss.item()

class BehaviorCloning:
    """
    è¡Œä¸ºå…‹éš†ï¼šæ¨¡ä»¿å­¦ä¹ çš„åŸºç¡€æ–¹æ³•
    ç›´æ¥ä»ä¸“å®¶æ¼”ç¤ºå­¦ä¹ ç­–ç•¥
    """
    def __init__(self, state_dim, action_dim):
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-3)
    
    def train(self, expert_data, num_epochs=100, batch_size=64):
        """
        è®­ç»ƒè¡Œä¸ºå…‹éš†
        expert_data: [(state, action)]
        """
        dataset = torch.utils.data.TensorDataset(
            torch.FloatTensor([s for s, a in expert_data]),
            torch.LongTensor([a for s, a in expert_data])
        )
        dataloader = torch.utils.data.DataLoader(
            dataset, batch_size=batch_size, shuffle=True
        )
        
        for epoch in range(num_epochs):
            total_loss = 0
            
            for states, actions in dataloader:
                # é¢„æµ‹åŠ¨ä½œ
                action_probs = self.policy(states)
                
                # äº¤å‰ç†µæŸå¤±
                loss = F.cross_entropy(action_probs, actions)
                
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                
                total_loss += loss.item()
            
            avg_loss = total_loss / len(dataloader)
            
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}")
```

---

## åå…­ã€å®é™…åº”ç”¨æ¡ˆä¾‹

### 16.1 Atariæ¸¸æˆ

```python
class AtariDQN:
    """
    åœ¨Atariæ¸¸æˆä¸Šåº”ç”¨DQN
    """
    def __init__(self, num_actions):
        self.q_net = self.build_cnn(num_actions)
        self.target_net = self.build_cnn(num_actions)
        self.target_net.load_state_dict(self.q_net.state_dict())
        
        self.optimizer = optim.Adam(self.q_net.parameters(), lr=1e-4)
        self.replay_buffer = ReplayBuffer(100000)
    
    def build_cnn(self, num_actions):
        """æ„å»ºCNNç½‘ç»œå¤„ç†å›¾åƒ"""
        return nn.Sequential(
            # è¾“å…¥: [batch, 4, 84, 84] (4å¸§ç°åº¦å›¾)
            nn.Conv2d(4, 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(7 * 7 * 64, 512),
            nn.ReLU(),
            nn.Linear(512, num_actions)
        )
    
    def preprocess_frame(self, frame):
        """
        é¢„å¤„ç†å¸§ï¼š
        1. è½¬ç°åº¦
        2. è£å‰ª
        3. ç¼©æ”¾åˆ°84x84
        """
        # å®ç°çœç•¥
        pass
    
    def train(self, env, num_frames=10000000):
        """è®­ç»ƒ"""
        frame_stack = []  # ä¿æŒæœ€è¿‘4å¸§
        
        state = env.reset()
        frame = self.preprocess_frame(state)
        
        for _ in range(4):
            frame_stack.append(frame)
        
        for frame_idx in range(num_frames):
            # é€‰æ‹©åŠ¨ä½œ
            state_tensor = torch.FloatTensor(np.array(frame_stack)).unsqueeze(0)
            action = self.select_action(state_tensor)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, _ = env.step(action)
            
            # å¤„ç†å¸§
            next_frame = self.preprocess_frame(next_state)
            next_frame_stack = frame_stack[1:] + [next_frame]
            
            # å­˜å‚¨ç»éªŒ
            self.replay_buffer.push(
                np.array(frame_stack),
                action,
                reward,
                np.array(next_frame_stack),
                done
            )
            
            # è®­ç»ƒ
            if frame_idx > 10000:
                self.train_step()
            
            frame_stack = next_frame_stack
            
            if done:
                state = env.reset()
                frame = self.preprocess_frame(state)
                frame_stack = [frame] * 4
```

### 16.2 æœºå™¨äººæ§åˆ¶

```python
class RobotController:
    """
    ä½¿ç”¨RLæ§åˆ¶æœºå™¨äºº
    """
    def __init__(self, state_dim, action_dim):
        # ä½¿ç”¨SACï¼ˆé€‚åˆè¿ç»­æ§åˆ¶ï¼‰
        self.agent = SAC(state_dim, action_dim, max_action=1.0)
    
    def train_on_simulation(self, sim_env, num_episodes=1000):
        """åœ¨ä»¿çœŸç¯å¢ƒä¸­è®­ç»ƒ"""
        for episode in range(num_episodes):
            state = sim_env.reset()
            done = False
            
            while not done:
                action = self.agent.select_action(state)
                next_state, reward, done, _ = sim_env.step(action)
                
                self.agent.replay_buffer.push(
                    state, action, reward, next_state, done
                )
                
                self.agent.train_step()
                state = next_state
    
    def sim_to_real_transfer(self, real_env, num_episodes=10):
        """
        ä»ä»¿çœŸåˆ°çœŸå®çš„è¿ç§»
        ä½¿ç”¨åŸŸéšæœºåŒ–å’Œå¾®è°ƒ
        """
        for episode in range(num_episodes):
            state = real_env.reset()
            done = False
            
            while not done:
                # ä½¿ç”¨è®­ç»ƒå¥½çš„ç­–ç•¥
                action = self.agent.select_action(state, deterministic=True)
                next_state, reward, done, _ = real_env.step(action)
                
                # åœ¨çœŸå®æ•°æ®ä¸Šå¾®è°ƒ
                self.agent.replay_buffer.push(
                    state, action, reward, next_state, done
                )
                
                if len(self.agent.replay_buffer) > 256:
                    self.agent.train_step()
                
                state = next_state
```

### 16.3 æ¨èç³»ç»Ÿ

```python
class RecommenderAgent:
    """
    åŸºäºRLçš„æ¨èç³»ç»Ÿ
    """
    def __init__(self, num_items, embedding_dim=64):
        self.num_items = num_items
        
        # ç‰©å“åµŒå…¥
        self.item_embeddings = nn.Embedding(num_items, embedding_dim)
        
        # ç­–ç•¥ç½‘ç»œ
        self.policy = nn.Sequential(
            nn.Linear(embedding_dim * 2, 128),  # ç”¨æˆ·çŠ¶æ€ + å€™é€‰ç‰©å“
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )
        
        self.optimizer = optim.Adam(
            list(self.item_embeddings.parameters()) + 
            list(self.policy.parameters()),
            lr=1e-3
        )
    
    def get_user_state(self, user_history):
        """ä»ç”¨æˆ·å†å²è·å–çŠ¶æ€è¡¨ç¤º"""
        # å¹³å‡å†å²ç‰©å“çš„åµŒå…¥
        history_items = torch.LongTensor(user_history)
        embeddings = self.item_embeddings(history_items)
        return embeddings.mean(dim=0)
    
    def recommend(self, user_state, candidate_items, top_k=10):
        """æ¨ètop-kç‰©å“"""
        scores = []
        
        for item in candidate_items:
            item_emb = self.item_embeddings(torch.LongTensor([item]))
            combined = torch.cat([user_state, item_emb.squeeze()], dim=0)
            score = self.policy(combined.unsqueeze(0))
            scores.append(score.item())
        
        # é€‰æ‹©top-k
        top_indices = np.argsort(scores)[-top_k:][::-1]
        return [candidate_items[i] for i in top_indices]
    
    def train_step(self, user_history, item, reward):
        """è®­ç»ƒæ­¥éª¤"""
        user_state = self.get_user_state(user_history)
        item_emb = self.item_embeddings(torch.LongTensor([item]))
        
        combined = torch.cat([user_state, item_emb.squeeze()], dim=0)
        predicted_score = self.policy(combined.unsqueeze(0))
        
        # ä½¿ç”¨å®é™…åé¦ˆä½œä¸ºç›®æ ‡
        loss = F.binary_cross_entropy(predicted_score, torch.FloatTensor([[reward]]))
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
```

---

## åä¸ƒã€æ€»ç»“ä¸æœªæ¥æ–¹å‘

### 17.1 æ ¸å¿ƒæ¦‚å¿µæ€»ç»“

```python
class RLSummary:
    """
    å¼ºåŒ–å­¦ä¹ æ ¸å¿ƒæ¦‚å¿µæ€»ç»“
    """
    
    @staticmethod
    def key_components():
        """å…³é”®ç»„æˆéƒ¨åˆ†"""
        return {
            'æ™ºèƒ½ä½“': 'åšå‡ºå†³ç­–çš„å®ä½“',
            'ç¯å¢ƒ': 'æ™ºèƒ½ä½“äº¤äº’çš„ä¸–ç•Œ',
            'çŠ¶æ€': 'ç¯å¢ƒçš„æè¿°',
            'åŠ¨ä½œ': 'æ™ºèƒ½ä½“çš„é€‰æ‹©',
            'å¥–åŠ±': 'åé¦ˆä¿¡å·',
            'ç­–ç•¥': 'çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„',
            'ä»·å€¼å‡½æ•°': 'é•¿æœŸå›æŠ¥çš„é¢„æœŸ'
        }
    
    @staticmethod
    def algorithm_categories():
        """ç®—æ³•åˆ†ç±»"""
        return {
            'åŸºäºå€¼': ['Q-Learning', 'DQN', 'Double DQN', 'Dueling DQN'],
            'åŸºäºç­–ç•¥': ['REINFORCE', 'PPO', 'TRPO'],
            'Actor-Critic': ['A2C', 'A3C', 'SAC', 'TD3', 'DDPG'],
            'åŸºäºæ¨¡å‹': ['Dyna-Q', 'MBPO', 'World Models'],
            'å…¶ä»–': ['é€†å¼ºåŒ–å­¦ä¹ ', 'å…ƒå¼ºåŒ–å­¦ä¹ ', 'å¤šæ™ºèƒ½ä½“RL']
        }
    
    @staticmethod
    def when_to_use_what():
        """ä½•æ—¶ä½¿ç”¨å“ªç§ç®—æ³•"""
        return {
            'ç¦»æ•£åŠ¨ä½œç©ºé—´': 'DQN, PPO, A2C',
            'è¿ç»­åŠ¨ä½œç©ºé—´': 'SAC, TD3, DDPG, PPO',
            'éœ€è¦æ ·æœ¬æ•ˆç‡': 'SAC, TD3, åŸºäºæ¨¡å‹çš„æ–¹æ³•',
            'å¤§è§„æ¨¡å¹¶è¡Œ': 'A3C, IMPALA',
            'ç¦»çº¿æ•°æ®': 'CQL, BCQ, è¡Œä¸ºå…‹éš†',
            'ç¨€ç–å¥–åŠ±': 'å±‚æ¬¡RL, å¥½å¥‡å¿ƒé©±åŠ¨, HER',
            'å¤šæ™ºèƒ½ä½“': 'MADDPG, QMIX, MAPPO'
        }
    
    @staticmethod
    def common_pitfalls():
        """å¸¸è§é™·é˜±"""
        return [
            '1. è¿‡æ‹Ÿåˆåˆ°è®­ç»ƒç¯å¢ƒ',
            '2. å¥–åŠ±è®¾è®¡ä¸å½“å¯¼è‡´æ„å¤–è¡Œä¸º',
            '3. æ¢ç´¢ä¸è¶³é™·å…¥å±€éƒ¨æœ€ä¼˜',
            '4. è¶…å‚æ•°æ•æ„Ÿæ€§é«˜',
            '5. è®­ç»ƒä¸ç¨³å®š',
            '6. ä»¿çœŸåˆ°çœŸå®çš„å·®è·',
            '7. éå¹³ç¨³æ€§é—®é¢˜',
            '8. æ ·æœ¬æ•ˆç‡ä½'
        ]
    
    @staticmethod
    def best_practices():
        """æœ€ä½³å®è·µ"""
        return [
            '1. ä»ç®€å•ç®—æ³•å¼€å§‹ï¼ˆå¦‚DQN, PPOï¼‰',
            '2. ä»”ç»†è®¾è®¡å¥–åŠ±å‡½æ•°',
            '3. ä½¿ç”¨ç»éªŒå›æ”¾å’Œç›®æ ‡ç½‘ç»œ',
            '4. è¿›è¡Œå……åˆ†çš„æ¢ç´¢',
            '5. ç›‘æ§å­¦ä¹ æ›²çº¿å’Œå…³é”®æŒ‡æ ‡',
            '6. ä½¿ç”¨æ ‡å‡†åŒ–å’Œå½’ä¸€åŒ–',
            '7. è°ƒæ•´è¶…å‚æ•°',
            '8. è¿›è¡Œæ¶ˆèå®éªŒ',
            '9. åœ¨å¤šä¸ªéšæœºç§å­ä¸Šæµ‹è¯•',
            '10. ä»ä¸“å®¶æ¼”ç¤ºå­¦ä¹ ï¼ˆå¦‚æœå¯ç”¨ï¼‰'
        ]
```

### 17.2 æœªæ¥ç ”ç©¶æ–¹å‘

```markdown
## å¼ºåŒ–å­¦ä¹ çš„æœªæ¥æ–¹å‘

### 1. æ ·æœ¬æ•ˆç‡
- æ›´é«˜æ•ˆçš„æ¢ç´¢ç­–ç•¥
- æ›´å¥½çš„æ¨¡å‹å­¦ä¹ 
- ç¦»çº¿RLçš„è¿›å±•
- è¿ç§»å­¦ä¹ å’Œå…ƒå­¦ä¹ 

### 2. æ³›åŒ–èƒ½åŠ›
- è·¨ä»»åŠ¡æ³›åŒ–
- é›¶æ ·æœ¬å­¦ä¹ 
- å°‘æ ·æœ¬å­¦ä¹ 
- åŸŸé€‚åº”

### 3. å®‰å…¨æ€§å’Œå¯é æ€§
- å®‰å…¨æ¢ç´¢
- çº¦æŸå¼ºåŒ–å­¦ä¹ 
- é²æ£’æ€§
- å¯è§£é‡Šæ€§

### 4. çœŸå®ä¸–ç•Œåº”ç”¨
- ä»¿çœŸåˆ°çœŸå®çš„è¿ç§»
- äººæœºäº¤äº’
- ç°å®ä¸–ç•Œçš„çº¦æŸ
- é•¿æœŸè§„åˆ’

### 5. ç†è®ºåŸºç¡€
- æ”¶æ•›æ€§ä¿è¯
- æ ·æœ¬å¤æ‚åº¦åˆ†æ
- æ¢ç´¢-åˆ©ç”¨æƒè¡¡
- å‡½æ•°é€¼è¿‘çš„ç†è®º

### 6. å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
- åä½œä¸ç«äº‰
- é€šä¿¡å­¦ä¹ 
- ç¤¾ä¼šå­¦ä¹ 
- æ¶Œç°è¡Œä¸º

### 7. ä¸å…¶ä»–AIé¢†åŸŸç»“åˆ
- RL + å¤§è¯­è¨€æ¨¡å‹
- RL + è®¡ç®—æœºè§†è§‰
- RL + å› æœæ¨ç†
- RL + çŸ¥è¯†å›¾è°±
```

---

## é™„å½•ï¼šå¸¸ç”¨åº“å’Œèµ„æº

```python
class RLResources:
    """å¼ºåŒ–å­¦ä¹ èµ„æº"""
    
    @staticmethod
    def popular_libraries():
        """æµè¡Œçš„RLåº“"""
        return {
            'OpenAI Gym': 'æ ‡å‡†RLç¯å¢ƒæ¥å£',
            'Stable-Baselines3': 'PyTorchå®ç°çš„æ ‡å‡†ç®—æ³•',
            'RLlib': 'Rayç”Ÿæ€çš„å¯æ‰©å±•RLåº“',
            'TF-Agents': 'TensorFlowçš„RLåº“',
            'Tianshou': 'æ¨¡å—åŒ–çš„PyTorch RLæ¡†æ¶',
            'CleanRL': 'ç®€æ´çš„RLå®ç°',
            'Dopamine': 'Googleçš„RLç ”ç©¶æ¡†æ¶'
        }
    
    @staticmethod
    def simulation_environments():
        """ä»¿çœŸç¯å¢ƒ"""
        return {
            'OpenAI Gym': 'ç»å…¸æ§åˆ¶ã€Atariç­‰',
            'MuJoCo': 'ç‰©ç†ä»¿çœŸ',
            'PyBullet': 'å¼€æºæœºå™¨äººä»¿çœŸ',
            'Unity ML-Agents': 'Unityæ¸¸æˆå¼•æ“',
            'DeepMind Control Suite': 'è¿ç»­æ§åˆ¶ä»»åŠ¡',
            'ProcGen': 'ç¨‹åºç”Ÿæˆçš„æ¸¸æˆç¯å¢ƒ',
            'MinAtar': 'ç®€åŒ–ç‰ˆAtari',
            'Roboschool': 'æœºå™¨äººå­¦ä¹ ç¯å¢ƒ'
        }
    
    @staticmethod
    def learning_resources():
        """å­¦ä¹ èµ„æº"""
        return {
            'ä¹¦ç±': [
                'Reinforcement Learning: An Introduction (Sutton & Barto)',
                'Deep Reinforcement Learning Hands-On (Lapan)',
                'Algorithms for Reinforcement Learning (SzepesvÃ¡ri)'
            ],
            'è¯¾ç¨‹': [
                'David Silver RL Course',
                'CS285 Deep RL (UC Berkeley)',
                'DeepMind x UCL RL Course'
            ],
            'è®ºæ–‡': [
                'DQN (Mnih et al., 2015)',
                'PPO (Schulman et al., 2017)',
                'SAC (Haarnoja et al., 2018)',
                'AlphaGo (Silver et al., 2016)'
            ]
        }

# å®Œæ•´ç¬”è®°ç»“æŸ
```

---

## ç»“è¯­

æœ¬ç¬”è®°æ¶µç›–äº†å¼ºåŒ–å­¦ä¹ ä»åŸºç¡€åˆ°é«˜çº§çš„æ ¸å¿ƒå†…å®¹ï¼ŒåŒ…æ‹¬ï¼š

âœ… **åŸºç¡€ç†è®º**ï¼šMDPã€ä»·å€¼å‡½æ•°ã€ç­–ç•¥
âœ… **ç»å…¸ç®—æ³•**ï¼šåŠ¨æ€è§„åˆ’ã€è’™ç‰¹å¡æ´›ã€æ—¶åºå·®åˆ†
âœ… **æ·±åº¦å¼ºåŒ–å­¦ä¹ **ï¼šDQNã€PPOã€SACã€TD3ç­‰
âœ… **é«˜çº§ä¸»é¢˜**ï¼šå¤šæ™ºèƒ½ä½“ã€å…ƒå­¦ä¹ ã€ç¦»çº¿RL
âœ… **å®é™…åº”ç”¨**ï¼šæ¸¸æˆã€æœºå™¨äººã€æ¨èç³»ç»Ÿ

**å»ºè®®å­¦ä¹ è·¯å¾„**ï¼š
1. æŒæ¡åŸºç¡€æ¦‚å¿µå’Œæ•°å­¦åŸç†
2. å®ç°ç®€å•çš„è¡¨æ ¼æ–¹æ³•ï¼ˆQ-Learningï¼‰
3. å­¦ä¹ æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆDQN, PPOï¼‰
4. åœ¨æ ‡å‡†ç¯å¢ƒä¸Šå®éªŒï¼ˆGymï¼‰
5. æ¢ç´¢é«˜çº§ä¸»é¢˜å’Œå®é™…åº”ç”¨

ç¥å­¦ä¹ é¡ºåˆ©ï¼ğŸš€
