# å¼ºåŒ–å­¦ä¹ å®Œæ•´ç¬”è®°

## ç›®å½•
1. [åŸºç¡€æ¦‚å¿µ](#ä¸€åŸºç¡€æ¦‚å¿µ)
2. [é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹](#äºŒé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹mdp)
3. [åŠ¨æ€è§„åˆ’](#ä¸‰åŠ¨æ€è§„åˆ’)
4. [è’™ç‰¹å¡æ´›æ–¹æ³•](#å››è’™ç‰¹å¡æ´›æ–¹æ³•)
5. [æ—¶åºå·®åˆ†å­¦ä¹ ](#äº”æ—¶åºå·®åˆ†å­¦ä¹ )
6. [å€¼å‡½æ•°é€¼è¿‘](#å…­å€¼å‡½æ•°é€¼è¿‘)
7. [ç­–ç•¥æ¢¯åº¦æ–¹æ³•](#ä¸ƒç­–ç•¥æ¢¯åº¦æ–¹æ³•)
8. [Actor-Critic](#å…«actor-criticæ–¹æ³•)
9. [æ·±åº¦å¼ºåŒ–å­¦ä¹ ](#ä¹æ·±åº¦å¼ºåŒ–å­¦ä¹ )
10. [é«˜çº§ç®—æ³•](#åé«˜çº§ç®—æ³•)

---

## ä¸€ã€åŸºç¡€æ¦‚å¿µ

### 1.1 å¼ºåŒ–å­¦ä¹ ç®€ä»‹
```
å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning, RLï¼‰æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œ
ç ”ç©¶æ™ºèƒ½ä½“ï¼ˆAgentï¼‰å¦‚ä½•åœ¨ç¯å¢ƒï¼ˆEnvironmentï¼‰ä¸­é€šè¿‡è¯•é”™å­¦ä¹ ï¼Œ
ä»¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚

æ ¸å¿ƒç‰¹ç‚¹ï¼š
- å»¶è¿Ÿå¥–åŠ±ï¼šè¡ŒåŠ¨çš„ç»“æœå¯èƒ½åœ¨å¾ˆä¹…ä¹‹åæ‰æ˜¾ç°
- æ¢ç´¢ä¸åˆ©ç”¨ï¼šéœ€è¦å¹³è¡¡å°è¯•æ–°ç­–ç•¥å’Œåˆ©ç”¨å·²çŸ¥å¥½ç­–ç•¥
- åºè´¯å†³ç­–ï¼šå½“å‰å†³ç­–å½±å“æœªæ¥çŠ¶æ€
```

### 1.2 åŸºæœ¬è¦ç´ 
```python
# å¼ºåŒ–å­¦ä¹ çš„äº”å…ƒç»„
class RLSystem:
    def __init__(self):
        # çŠ¶æ€ç©ºé—´ (State Space)
        self.states = S
        
        # åŠ¨ä½œç©ºé—´ (Action Space)
        self.actions = A
        
        # å¥–åŠ±å‡½æ•° (Reward Function)
        # R: S Ã— A â†’ â„
        self.reward_function = R
        
        # çŠ¶æ€è½¬ç§»æ¦‚ç‡ (Transition Probability)
        # P: S Ã— A Ã— S â†’ [0,1]
        self.transition_prob = P
        
        # æŠ˜æ‰£å› å­ (Discount Factor)
        # Î³ âˆˆ [0,1]
        self.gamma = 0.99

# æ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’
def agent_environment_interaction():
    """
    æ™ºèƒ½ä½“-ç¯å¢ƒäº¤äº’å¾ªç¯ï¼š
    1. æ™ºèƒ½ä½“è§‚å¯ŸçŠ¶æ€ s_t
    2. æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ a_t
    3. ç¯å¢ƒè¿”å›å¥–åŠ± r_t å’Œæ–°çŠ¶æ€ s_{t+1}
    4. é‡å¤
    """
    state = env.reset()
    
    for t in range(max_steps):
        # æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ
        action = agent.select_action(state)
        
        # ç¯å¢ƒå“åº”
        next_state, reward, done, info = env.step(action)
        
        # æ™ºèƒ½ä½“å­¦ä¹ 
        agent.learn(state, action, reward, next_state, done)
        
        state = next_state
        
        if done:
            break
```

### 1.3 å›æŠ¥ä¸ä»·å€¼å‡½æ•°
```python
import numpy as np

# 1. å›æŠ¥ (Return)
def compute_return(rewards, gamma=0.99):
    """
    è®¡ç®—ç´¯ç§¯æŠ˜æ‰£å›æŠ¥
    G_t = r_t + Î³r_{t+1} + Î³Â²r_{t+2} + ... = Î£ Î³^k * r_{t+k}
    """
    G = 0
    for t in range(len(rewards)-1, -1, -1):
        G = rewards[t] + gamma * G
    return G

# ç¤ºä¾‹
rewards = [1, 0, 0, 1, 1]
print(f"å›æŠ¥: {compute_return(rewards)}")  # 2.9701

# 2. çŠ¶æ€ä»·å€¼å‡½æ•° (State Value Function)
def state_value_function():
    """
    V^Ï€(s) = ğ”¼_Ï€[G_t | s_t = s]
           = ğ”¼_Ï€[r_t + Î³V^Ï€(s_{t+1}) | s_t = s]
    
    è¡¨ç¤ºåœ¨çŠ¶æ€sä¸‹ï¼Œéµå¾ªç­–ç•¥Ï€èƒ½è·å¾—çš„æœŸæœ›å›æŠ¥
    """
    pass

# 3. åŠ¨ä½œä»·å€¼å‡½æ•° (Action Value Function)
def action_value_function():
    """
    Q^Ï€(s,a) = ğ”¼_Ï€[G_t | s_t = s, a_t = a]
             = ğ”¼[r_t + Î³Q^Ï€(s_{t+1}, a_{t+1}) | s_t = s, a_t = a]
    
    è¡¨ç¤ºåœ¨çŠ¶æ€sä¸‹é‡‡å–åŠ¨ä½œaï¼Œç„¶åéµå¾ªç­–ç•¥Ï€çš„æœŸæœ›å›æŠ¥
    """
    pass

# 4. ä¼˜åŠ¿å‡½æ•° (Advantage Function)
def advantage_function(Q, V, state, action):
    """
    A^Ï€(s,a) = Q^Ï€(s,a) - V^Ï€(s)
    
    è¡¨ç¤ºåœ¨çŠ¶æ€sä¸‹é‡‡å–åŠ¨ä½œaç›¸æ¯”å¹³å‡æ°´å¹³çš„ä¼˜åŠ¿
    """
    return Q[state, action] - V[state]
```

### 1.4 ç­–ç•¥
```python
# 1. ç¡®å®šæ€§ç­–ç•¥ (Deterministic Policy)
class DeterministicPolicy:
    """
    Ï€: S â†’ A
    æ¯ä¸ªçŠ¶æ€æ˜ å°„åˆ°å”¯ä¸€çš„åŠ¨ä½œ
    """
    def __init__(self, policy_dict):
        self.policy = policy_dict
    
    def select_action(self, state):
        return self.policy[state]

# 2. éšæœºç­–ç•¥ (Stochastic Policy)
class StochasticPolicy:
    """
    Ï€: S Ã— A â†’ [0,1]
    Ï€(a|s) è¡¨ç¤ºåœ¨çŠ¶æ€sä¸‹é€‰æ‹©åŠ¨ä½œaçš„æ¦‚ç‡
    """
    def __init__(self, policy_probs):
        self.policy = policy_probs
    
    def select_action(self, state):
        actions = list(self.policy[state].keys())
        probs = list(self.policy[state].values())
        return np.random.choice(actions, p=probs)

# 3. Îµ-è´ªå¿ƒç­–ç•¥ (Îµ-greedy Policy)
class EpsilonGreedyPolicy:
    """
    ä»¥æ¦‚ç‡Îµéšæœºæ¢ç´¢ï¼Œä»¥æ¦‚ç‡1-Îµé€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
    """
    def __init__(self, Q, epsilon=0.1):
        self.Q = Q
        self.epsilon = epsilon
    
    def select_action(self, state, actions):
        if np.random.random() < self.epsilon:
            return np.random.choice(actions)  # æ¢ç´¢
        else:
            return np.argmax(self.Q[state])   # åˆ©ç”¨

# 4. Softmaxç­–ç•¥ (Boltzmannç­–ç•¥)
class SoftmaxPolicy:
    """
    Ï€(a|s) = exp(Q(s,a)/Ï„) / Î£_a' exp(Q(s,a')/Ï„)
    Ï„æ˜¯æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶æ¢ç´¢ç¨‹åº¦
    """
    def __init__(self, Q, temperature=1.0):
        self.Q = Q
        self.tau = temperature
    
    def select_action(self, state, actions):
        q_values = self.Q[state]
        probs = np.exp(q_values / self.tau)
        probs = probs / np.sum(probs)
        return np.random.choice(actions, p=probs)
```

### 1.5 æ¢ç´¢ç­–ç•¥
```python
# 1. Îµè¡°å‡
class EpsilonDecay:
    """éšç€è®­ç»ƒè¿›è¡Œï¼Œå‡å°‘æ¢ç´¢"""
    def __init__(self, epsilon_start=1.0, epsilon_end=0.01, decay_steps=10000):
        self.epsilon_start = epsilon_start
        self.epsilon_end = epsilon_end
        self.decay_steps = decay_steps
    
    def get_epsilon(self, step):
        epsilon = self.epsilon_start - (self.epsilon_start - self.epsilon_end) * \
                  min(step / self.decay_steps, 1.0)
        return epsilon

# 2. ä¸Šç½®ä¿¡ç•Œ (UCB - Upper Confidence Bound)
class UCBExploration:
    """
    é€‰æ‹© a = argmax_a [Q(s,a) + c * sqrt(ln(t) / N(s,a))]
    å¹³è¡¡åˆ©ç”¨ï¼ˆQå€¼ï¼‰å’Œæ¢ç´¢ï¼ˆè®¿é—®æ¬¡æ•°å°‘çš„åŠ¨ä½œï¼‰
    """
    def __init__(self, c=2.0):
        self.c = c
        self.N = {}  # è®¿é—®è®¡æ•°
        self.t = 0   # æ€»æ­¥æ•°
    
    def select_action(self, state, Q, actions):
        self.t += 1
        if state not in self.N:
            self.N[state] = {a: 0 for a in actions}
        
        ucb_values = []
        for a in actions:
            if self.N[state][a] == 0:
                return a  # ä¼˜å…ˆé€‰æ‹©æœªè®¿é—®çš„åŠ¨ä½œ
            
            ucb = Q[state][a] + self.c * np.sqrt(np.log(self.t) / self.N[state][a])
            ucb_values.append(ucb)
        
        action = actions[np.argmax(ucb_values)]
        self.N[state][action] += 1
        return action

# 3. Thompsoné‡‡æ ·
class ThompsonSampling:
    """
    åŸºäºè´å¶æ–¯æ¨æ–­çš„æ¢ç´¢ç­–ç•¥
    ä¸ºæ¯ä¸ªåŠ¨ä½œç»´æŠ¤ä¸€ä¸ªåˆ†å¸ƒï¼Œä»åˆ†å¸ƒä¸­é‡‡æ ·
    """
    def __init__(self):
        self.alpha = {}  # æˆåŠŸè®¡æ•°
        self.beta = {}   # å¤±è´¥è®¡æ•°
    
    def select_action(self, state, actions):
        if state not in self.alpha:
            self.alpha[state] = {a: 1 for a in actions}
            self.beta[state] = {a: 1 for a in actions}
        
        samples = {}
        for a in actions:
            # ä»Betaåˆ†å¸ƒé‡‡æ ·
            samples[a] = np.random.beta(self.alpha[state][a], self.beta[state][a])
        
        return max(samples, key=samples.get)
    
    def update(self, state, action, reward):
        if reward > 0:
            self.alpha[state][action] += 1
        else:
            self.beta[state][action] += 1
```

---

## äºŒã€é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)

### 2.1 MDPå®šä¹‰
```python
class MDP:
    """
    é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹äº”å…ƒç»„: (S, A, P, R, Î³)
    
    - S: çŠ¶æ€ç©ºé—´
    - A: åŠ¨ä½œç©ºé—´
    - P: çŠ¶æ€è½¬ç§»æ¦‚ç‡ P(s'|s,a)
    - R: å¥–åŠ±å‡½æ•° R(s,a,s')
    - Î³: æŠ˜æ‰£å› å­
    """
    def __init__(self, states, actions, transitions, rewards, gamma=0.99):
        self.states = states
        self.actions = actions
        self.P = transitions  # P[s][a][s'] = æ¦‚ç‡
        self.R = rewards      # R[s][a][s'] = å¥–åŠ±
        self.gamma = gamma
    
    def get_transition_prob(self, state, action, next_state):
        """è·å–çŠ¶æ€è½¬ç§»æ¦‚ç‡"""
        return self.P[state][action].get(next_state, 0)
    
    def get_reward(self, state, action, next_state):
        """è·å–å¥–åŠ±"""
        return self.R[state][action].get(next_state, 0)

# ç¤ºä¾‹ï¼šç½‘æ ¼ä¸–ç•ŒMDP
class GridWorldMDP:
    """
    ç®€å•çš„4Ã—4ç½‘æ ¼ä¸–ç•Œ
    ç›®æ ‡ï¼šä»èµ·ç‚¹(0,0)åˆ°è¾¾ç»ˆç‚¹(3,3)
    """
    def __init__(self):
        self.grid_size = 4
        self.actions = ['up', 'down', 'left', 'right']
        self.gamma = 0.9
        
    def get_next_state(self, state, action):
        """ç¡®å®šæ€§çŠ¶æ€è½¬ç§»"""
        x, y = state
        
        if action == 'up':
            x = max(0, x - 1)
        elif action == 'down':
            x = min(self.grid_size - 1, x + 1)
        elif action == 'left':
            y = max(0, y - 1)
        elif action == 'right':
            y = min(self.grid_size - 1, y + 1)
        
        return (x, y)
    
    def get_reward(self, state, action, next_state):
        """å¥–åŠ±å‡½æ•°"""
        if next_state == (3, 3):  # ç»ˆç‚¹
            return 1.0
        return -0.01  # æ¯æ­¥å°æƒ©ç½šï¼Œé¼“åŠ±å¿«é€Ÿåˆ°è¾¾ç»ˆç‚¹
    
    def is_terminal(self, state):
        """åˆ¤æ–­æ˜¯å¦ä¸ºç»ˆæ­¢çŠ¶æ€"""
        return state == (3, 3)
```

### 2.2 è´å°”æ›¼æ–¹ç¨‹
```python
# 1. è´å°”æ›¼æœŸæœ›æ–¹ç¨‹ (Bellman Expectation Equation)
def bellman_expectation_v(V, mdp, policy, state):
    """
    çŠ¶æ€ä»·å€¼å‡½æ•°çš„è´å°”æ›¼æœŸæœ›æ–¹ç¨‹
    V^Ï€(s) = Î£_a Ï€(a|s) Î£_{s'} P(s'|s,a)[R(s,a,s') + Î³V^Ï€(s')]
    """
    value = 0
    for action in mdp.actions:
        action_prob = policy.get_action_prob(state, action)
        
        for next_state in mdp.states:
            trans_prob = mdp.get_transition_prob(state, action, next_state)
            reward = mdp.get_reward(state, action, next_state)
            
            value += action_prob * trans_prob * (reward + mdp.gamma * V[next_state])
    
    return value

def bellman_expectation_q(Q, mdp, policy, state, action):
    """
    åŠ¨ä½œä»·å€¼å‡½æ•°çš„è´å°”æ›¼æœŸæœ›æ–¹ç¨‹
    Q^Ï€(s,a) = Î£_{s'} P(s'|s,a)[R(s,a,s') + Î³ Î£_{a'} Ï€(a'|s')Q^Ï€(s',a')]
    """
    value = 0
    for next_state in mdp.states:
        trans_prob = mdp.get_transition_prob(state, action, next_state)
        reward = mdp.get_reward(state, action, next_state)
        
        next_value = 0
        for next_action in mdp.actions:
            action_prob = policy.get_action_prob(next_state, next_action)
            next_value += action_prob * Q[next_state, next_action]
        
        value += trans_prob * (reward + mdp.gamma * next_value)
    
    return value

# 2. è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ (Bellman Optimality Equation)
def bellman_optimality_v(V, mdp, state):
    """
    æœ€ä¼˜çŠ¶æ€ä»·å€¼å‡½æ•°
    V*(s) = max_a Î£_{s'} P(s'|s,a)[R(s,a,s') + Î³V*(s')]
    """
    max_value = float('-inf')
    
    for action in mdp.actions:
        value = 0
        for next_state in mdp.states:
            trans_prob = mdp.get_transition_prob(state, action, next_state)
            reward = mdp.get_reward(state, action, next_state)
            value += trans_prob * (reward + mdp.gamma * V[next_state])
        
        max_value = max(max_value, value)
    
    return max_value

def bellman_optimality_q(Q, mdp, state, action):
    """
    æœ€ä¼˜åŠ¨ä½œä»·å€¼å‡½æ•°
    Q*(s,a) = Î£_{s'} P(s'|s,a)[R(s,a,s') + Î³ max_{a'} Q*(s',a')]
    """
    value = 0
    
    for next_state in mdp.states:
        trans_prob = mdp.get_transition_prob(state, action, next_state)
        reward = mdp.get_reward(state, action, next_state)
        max_next_q = max(Q[next_state, a] for a in mdp.actions)
        value += trans_prob * (reward + mdp.gamma * max_next_q)
    
    return value
```

### 2.3 é©¬å°”å¯å¤«æ€§è´¨
```python
class MarkovProperty:
    """
    é©¬å°”å¯å¤«æ€§è´¨ï¼šæœªæ¥åªä¾èµ–äºå½“å‰çŠ¶æ€ï¼Œä¸å†å²æ— å…³
    P(s_{t+1} | s_t, s_{t-1}, ..., s_0) = P(s_{t+1} | s_t)
    """
    
    @staticmethod
    def check_markov_property(trajectory, transition_counts):
        """
        æ£€æŸ¥è½¨è¿¹æ˜¯å¦æ»¡è¶³é©¬å°”å¯å¤«æ€§è´¨
        é€šè¿‡æ¯”è¾ƒ P(s'|s) å’Œ P(s'|s,history)
        """
        # è®¡ç®—å•æ­¥è½¬ç§»æ¦‚ç‡
        single_step = {}
        for i in range(len(trajectory) - 1):
            s, s_next = trajectory[i], trajectory[i+1]
            if s not in single_step:
                single_step[s] = {}
            single_step[s][s_next] = single_step[s].get(s_next, 0) + 1
        
        # å½’ä¸€åŒ–
        for s in single_step:
            total = sum(single_step[s].values())
            for s_next in single_step[s]:
                single_step[s][s_next] /= total
        
        # è®¡ç®—å¸¦å†å²çš„è½¬ç§»æ¦‚ç‡
        history_step = {}
        for i in range(2, len(trajectory)):
            history = tuple(trajectory[:i])
            s_next = trajectory[i]
            if history not in history_step:
                history_step[history] = {}
            history_step[history][s_next] = history_step[history].get(s_next, 0) + 1
        
        # æ¯”è¾ƒå·®å¼‚
        # ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦æ›´ä¸¥æ ¼çš„ç»Ÿè®¡æµ‹è¯•ï¼‰
        return single_step, history_step

# éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (POMDP)
class POMDP:
    """
    å½“æ™ºèƒ½ä½“æ— æ³•å®Œå…¨è§‚æµ‹çŠ¶æ€æ—¶ï¼Œä½¿ç”¨POMDP
    
    ä¸ƒå…ƒç»„: (S, A, P, R, Î©, O, Î³)
    - Î©: è§‚æµ‹ç©ºé—´
    - O: è§‚æµ‹æ¦‚ç‡ O(o|s,a)
    """
    def __init__(self, states, actions, observations, 
                 transitions, rewards, obs_probs, gamma=0.99):
        self.states = states
        self.actions = actions
        self.observations = observations
        self.P = transitions
        self.R = rewards
        self.O = obs_probs  # O[s][a][o] = æ¦‚ç‡
        self.gamma = gamma
        
        # ä¿¡å¿µçŠ¶æ€ (Belief State)
        self.belief = self.initialize_belief()
    
    def initialize_belief(self):
        """åˆå§‹åŒ–å‡åŒ€ä¿¡å¿µ"""
        n = len(self.states)
        return {s: 1.0/n for s in self.states}
    
    def update_belief(self, belief, action, observation):
        """
        è´å¶æ–¯ä¿¡å¿µæ›´æ–°
        b'(s') âˆ O(o|s',a) Î£_s P(s'|s,a)b(s)
        """
        new_belief = {}
        
        for s_next in self.states:
            prob = 0
            for s in self.states:
                prob += self.P[s][action].get(s_next, 0) * belief[s]
            prob *= self.O[s_next][action].get(observation, 0)
            new_belief[s_next] = prob
        
        # å½’ä¸€åŒ–
        total = sum(new_belief.values())
        if total > 0:
            new_belief = {s: p/total for s, p in new_belief.items()}
        
        return new_belief
```

---

## ä¸‰ã€åŠ¨æ€è§„åˆ’

### 3.1 ç­–ç•¥è¯„ä¼°
```python
import numpy as np

class PolicyEvaluation:
    """
    ç­–ç•¥è¯„ä¼°ï¼šè®¡ç®—ç»™å®šç­–ç•¥Ï€çš„ä»·å€¼å‡½æ•°V^Ï€
    ä½¿ç”¨è¿­ä»£æ–¹æ³•æ±‚è§£è´å°”æ›¼æœŸæœ›æ–¹ç¨‹
    """
    def __init__(self, mdp, policy, theta=1e-6):
        self.mdp = mdp
        self.policy = policy
        self.theta = theta  # æ”¶æ•›é˜ˆå€¼
    
    def evaluate(self, max_iterations=1000):
        """
        è¿­ä»£ç­–ç•¥è¯„ä¼°
        V_{k+1}(s) = Î£_a Ï€(a|s) Î£_{s'} P(s'|s,a)[R(s,a,s') + Î³V_k(s')]
        """
        # åˆå§‹åŒ–ä»·å€¼å‡½æ•°
        V = {s: 0 for s in self.mdp.states}
        
        for iteration in range(max_iterations):
            delta = 0
            new_V = V.copy()
            
            for state in self.mdp.states:
                if self.mdp.is_terminal(state):
                    continue
                
                v = V[state]
                
                # è´å°”æ›¼æœŸæœ›æ›´æ–°
                new_value = 0
                for action in self.mdp.actions:
                    action_prob = self.policy.get_action_prob(state, action)
                    
                    for next_state in self.mdp.states:
                        trans_prob = self.mdp.get_transition_prob(state, action, next_state)
                        reward = self.mdp.get_reward(state, action, next_state)
                        new_value += action_prob * trans_prob * \
                                   (reward + self.mdp.gamma * V[next_state])
                
                new_V[state] = new_value
                delta = max(delta, abs(v - new_value))
            
            V = new_V
            
            # æ£€æŸ¥æ”¶æ•›
            if delta < self.theta:
                print(f"ç­–ç•¥è¯„ä¼°æ”¶æ•›äºç¬¬ {iteration+1} æ¬¡è¿­ä»£")
                break
        
        return V

# ç¤ºä¾‹ï¼šç½‘æ ¼ä¸–ç•Œç­–ç•¥è¯„ä¼°
def example_policy_evaluation():
    # åˆ›å»º4Ã—4ç½‘æ ¼ä¸–ç•Œ
    mdp = GridWorldMDP()
    
    # å®šä¹‰éšæœºç­–ç•¥ï¼ˆæ¯ä¸ªæ–¹å‘æ¦‚ç‡ç›¸ç­‰ï¼‰
    policy = UniformPolicy(mdp.actions)
    
    # è¯„ä¼°ç­–ç•¥
    evaluator = PolicyEvaluation(mdp, policy)
    V = evaluator.evaluate()
    
    # æ‰“å°ä»·å€¼å‡½æ•°
    print("çŠ¶æ€ä»·å€¼å‡½æ•°:")
    for i in range(4):
        for j in range(4):
            print(f"{V[(i,j)]:.2f}", end="  ")
        print()
```

### 3.2 ç­–ç•¥æ”¹è¿›
```python
class PolicyImprovement:
    """
    ç­–ç•¥æ”¹è¿›ï¼šæ ¹æ®ä»·å€¼å‡½æ•°æ”¹è¿›ç­–ç•¥
    Ï€'(s) = argmax_a Q^Ï€(s,a)
    """
    def __init__(self, mdp):
        self.mdp = mdp
    
    def improve(self, V):
        """
        è´ªå¿ƒç­–ç•¥æ”¹è¿›
        """
        new_policy = {}
        policy_stable = True
        
        for state in self.mdp.states:
            if self.mdp.is_terminal(state):
                continue
            
            # è®¡ç®—æ¯ä¸ªåŠ¨ä½œçš„Qå€¼
            q_values = {}
            for action in self.mdp.actions:
                q = 0
                for next_state in self.mdp.states:
                    trans_prob = self.mdp.get_transition_prob(state, action, next_state)
                    reward = self.mdp.get_reward(state, action, next_state)
                    q += trans_prob * (reward + self.mdp.gamma * V[next_state])
                q_values[action] = q
            
            policy[state] = max(q_values, key=q_values.get)
        
        return policy

# å¼‚æ­¥åŠ¨æ€è§„åˆ’
class AsynchronousDP:
    """
    å¼‚æ­¥åŠ¨æ€è§„åˆ’ï¼šä¸éœ€è¦å®Œæ•´æ‰«ææ‰€æœ‰çŠ¶æ€
    - åŸåœ°æ›´æ–° (In-place)
    - ä¼˜å…ˆçº§æ‰«æ
    - å®æ—¶åŠ¨æ€è§„åˆ’
    """
    def __init__(self, mdp):
        self.mdp = mdp
    
    def prioritized_sweeping(self, V, theta=1e-6, max_iterations=1000):
        """
        ä¼˜å…ˆçº§æ‰«æï¼šä¼˜å…ˆæ›´æ–°Bellmanè¯¯å·®å¤§çš„çŠ¶æ€
        """
        import heapq
        
        # åˆå§‹åŒ–ä¼˜å…ˆé˜Ÿåˆ—ï¼ˆæœ€å¤§å †ï¼Œç”¨è´Ÿå€¼å®ç°ï¼‰
        priority_queue = []
        
        # è®¡ç®—åˆå§‹ä¼˜å…ˆçº§
        for state in self.mdp.states:
            if not self.mdp.is_terminal(state):
                bellman_error = self.compute_bellman_error(state, V)
                if bellman_error > theta:
                    heapq.heappush(priority_queue, (-bellman_error, state))
        
        for iteration in range(max_iterations):
            if not priority_queue:
                break
            
            # å–å‡ºè¯¯å·®æœ€å¤§çš„çŠ¶æ€
            _, state = heapq.heappop(priority_queue)
            
            # æ›´æ–°è¯¥çŠ¶æ€
            old_value = V[state]
            V[state] = self.bellman_backup(state, V)
            
            # æ›´æ–°å‰é©±çŠ¶æ€çš„ä¼˜å…ˆçº§
            for prev_state in self.get_predecessors(state):
                if not self.mdp.is_terminal(prev_state):
                    bellman_error = self.compute_bellman_error(prev_state, V)
                    if bellman_error > theta:
                        heapq.heappush(priority_queue, (-bellman_error, prev_state))
        
        return V
    
    def bellman_backup(self, state, V):
        """æ‰§è¡ŒBellmanæ›´æ–°"""
        max_value = float('-inf')
        
        for action in self.mdp.actions:
            value = 0
            for next_state in self.mdp.states:
                trans_prob = self.mdp.get_transition_prob(state, action, next_state)
                reward = self.mdp.get_reward(state, action, next_state)
                value += trans_prob * (reward + self.mdp.gamma * V[next_state])
            max_value = max(max_value, value)
        
        return max_value
    
    def compute_bellman_error(self, state, V):
        """è®¡ç®—Bellmanè¯¯å·®"""
        new_value = self.bellman_backup(state, V)
        return abs(V[state] - new_value)
    
    def get_predecessors(self, state):
        """è·å–å¯ä»¥è½¬ç§»åˆ°è¯¥çŠ¶æ€çš„å‰é©±çŠ¶æ€"""
        predecessors = []
        for s in self.mdp.states:
            for a in self.mdp.actions:
                if self.mdp.get_transition_prob(s, a, state) > 0:
                    predecessors.append(s)
        return predecessors
```

---

## å››ã€è’™ç‰¹å¡æ´›æ–¹æ³•

### 4.1 è’™ç‰¹å¡æ´›é¢„æµ‹
```python
class MonteCarloPredictor:
    """
    è’™ç‰¹å¡æ´›ç­–ç•¥è¯„ä¼°
    é€šè¿‡é‡‡æ ·å®Œæ•´è½¨è¿¹ä¼°è®¡ä»·å€¼å‡½æ•°
    ä¸éœ€è¦ç¯å¢ƒæ¨¡å‹ï¼ˆmodel-freeï¼‰
    """
    def __init__(self, gamma=0.99):
        self.gamma = gamma
        self.returns = {}  # è®°å½•æ¯ä¸ªçŠ¶æ€çš„å›æŠ¥
        self.V = {}        # çŠ¶æ€ä»·å€¼å‡½æ•°
    
    def first_visit_mc(self, episodes):
        """
        é¦–æ¬¡è®¿é—®MCï¼šåªç»Ÿè®¡çŠ¶æ€ç¬¬ä¸€æ¬¡å‡ºç°æ—¶çš„å›æŠ¥
        """
        for episode in episodes:
            # episode = [(s0,a0,r0), (s1,a1,r1), ..., (sT,aT,rT)]
            states_visited = set()
            G = 0
            
            # ä»åå‘å‰è®¡ç®—å›æŠ¥
            for t in range(len(episode)-1, -1, -1):
                state, action, reward = episode[t]
                G = reward + self.gamma * G
                
                # é¦–æ¬¡è®¿é—®
                if state not in states_visited:
                    states_visited.add(state)
                    
                    if state not in self.returns:
                        self.returns[state] = []
                    self.returns[state].append(G)
                    
                    # æ›´æ–°ä»·å€¼å‡½æ•°ï¼ˆå¹³å‡ï¼‰
                    self.V[state] = np.mean(self.returns[state])
        
        return self.V
    
    def every_visit_mc(self, episodes):
        """
        æ¯æ¬¡è®¿é—®MCï¼šç»Ÿè®¡çŠ¶æ€æ‰€æœ‰å‡ºç°æ—¶çš„å›æŠ¥
        """
        for episode in episodes:
            G = 0
            
            for t in range(len(episode)-1, -1, -1):
                state, action, reward = episode[t]
                G = reward + self.gamma * G
                
                if state not in self.returns:
                    self.returns[state] = []
                self.returns[state].append(G)
                self.V[state] = np.mean(self.returns[state])
        
        return self.V
    
    def incremental_mc(self, episodes, alpha=None):
        """
        å¢é‡å¼MCï¼šä½¿ç”¨å¢é‡å¹³å‡æ›´æ–°
        V(s) â† V(s) + Î±[G - V(s)]
        """
        for state in self.V:
            if state not in self.returns:
                self.returns[state] = []
        
        for episode in episodes:
            G = 0
            states_visited = set()
            
            for t in range(len(episode)-1, -1, -1):
                state, action, reward = episode[t]
                G = reward + self.gamma * G
                
                if state not in states_visited:
                    states_visited.add(state)
                    
                    if state not in self.V:
                        self.V[state] = 0
                    
                    # å¢é‡æ›´æ–°
                    if alpha is None:
                        # è‡ªé€‚åº”å­¦ä¹ ç‡
                        n = len(self.returns.get(state, [])) + 1
                        step_size = 1.0 / n
                    else:
                        step_size = alpha
                    
                    self.V[state] += step_size * (G - self.V[state])
        
        return self.V

# é‡è¦æ€§é‡‡æ ·
class ImportanceSamplingMC:
    """
    é‡è¦æ€§é‡‡æ ·ï¼šä½¿ç”¨è¡Œä¸ºç­–ç•¥é‡‡æ ·ï¼Œè¯„ä¼°ç›®æ ‡ç­–ç•¥
    é€‚ç”¨äºç¦»ç­–ç•¥ï¼ˆoff-policyï¼‰å­¦ä¹ 
    """
    def __init__(self, gamma=0.99):
        self.gamma = gamma
        self.V = {}
        self.C = {}  # ç´¯ç§¯æƒé‡
    
    def weighted_importance_sampling(self, episodes, behavior_policy, target_policy):
        """
        åŠ æƒé‡è¦æ€§é‡‡æ ·
        Ï_t = Ï€(a_t|s_t) / b(a_t|s_t)
        """
        for episode in episodes:
            G = 0
            W = 1.0  # é‡è¦æ€§é‡‡æ ·æ¯”ç‡
            
            for t in range(len(episode)-1, -1, -1):
                state, action, reward = episode[t]
                G = reward + self.gamma * G
                
                if state not in self.C:
                    self.C[state] = 0
                    self.V[state] = 0
                
                self.C[state] += W
                
                # åŠ æƒæ›´æ–°
                self.V[state] += (W / self.C[state]) * (G - self.V[state])
                
                # æ›´æ–°é‡è¦æ€§é‡‡æ ·æ¯”ç‡
                pi_prob = target_policy.get_action_prob(state, action)
                b_prob = behavior_policy.get_action_prob(state, action)
                
                if b_prob == 0:
                    break
                
                W *= pi_prob / b_prob
                
                if W == 0:
                    break
        
        return self.V
```

### 4.2 è’™ç‰¹å¡æ´›æ§åˆ¶
```python
class MonteCarloControl:
    """
    è’™ç‰¹å¡æ´›æ§åˆ¶ï¼šé€šè¿‡é‡‡æ ·å­¦ä¹ æœ€ä¼˜ç­–ç•¥
    """
    def __init__(self, env, gamma=0.99, epsilon=0.1):
        self.env = env
        self.gamma = gamma
        self.epsilon = epsilon
        self.Q = {}  # Q(s,a)
        self.returns = {}
        self.policy = {}
    
    def on_policy_mc_control(self, num_episodes=1000):
        """
        åŒç­–ç•¥MCæ§åˆ¶ï¼ˆÎµ-è´ªå¿ƒç­–ç•¥æ”¹è¿›ï¼‰
        1. ä½¿ç”¨Îµ-è´ªå¿ƒç­–ç•¥ç”Ÿæˆè½¨è¿¹
        2. è¯„ä¼°Qå‡½æ•°
        3. æ”¹è¿›ç­–ç•¥
        """
        for episode_num in range(num_episodes):
            # ç”Ÿæˆè½¨è¿¹
            episode = self.generate_episode()
            
            # æ›´æ–°Qå€¼ï¼ˆé¦–æ¬¡è®¿é—®ï¼‰
            states_actions_visited = set()
            G = 0
            
            for t in range(len(episode)-1, -1, -1):
                state, action, reward = episode[t]
                G = reward + self.gamma * G
                
                sa_pair = (state, action)
                if sa_pair not in states_actions_visited:
                    states_actions_visited.add(sa_pair)
                    
                    if sa_pair not in self.returns:
                        self.returns[sa_pair] = []
                    self.returns[sa_pair].append(G)
                    
                    # æ›´æ–°Qå€¼
                    if state not in self.Q:
                        self.Q[state] = {}
                    self.Q[state][action] = np.mean(self.returns[sa_pair])
                    
                    # ç­–ç•¥æ”¹è¿›ï¼ˆÎµ-è´ªå¿ƒï¼‰
                    self.update_epsilon_greedy_policy(state)
        
        return self.Q, self.policy
    
    def off_policy_mc_control(self, num_episodes=1000):
        """
        ç¦»ç­–ç•¥MCæ§åˆ¶
        è¡Œä¸ºç­–ç•¥ï¼šÎµ-è´ªå¿ƒï¼ˆç”¨äºæ¢ç´¢ï¼‰
        ç›®æ ‡ç­–ç•¥ï¼šè´ªå¿ƒï¼ˆè¦å­¦ä¹ çš„ç­–ç•¥ï¼‰
        """
        # åˆå§‹åŒ–ç›®æ ‡ç­–ç•¥ä¸ºè´ªå¿ƒ
        target_policy = {}
        
        # ç´¯ç§¯æƒé‡
        C = {}
        
        for episode_num in range(num_episodes):
            # ä½¿ç”¨è¡Œä¸ºç­–ç•¥ç”Ÿæˆè½¨è¿¹
            episode = self.generate_episode()
            
            G = 0
            W = 1.0
            
            for t in range(len(episode)-1, -1, -1):
                state, action, reward = episode[t]
                G = reward + self.gamma * G
                
                sa_pair = (state, action)
                
                if sa_pair not in C:
                    C[sa_pair] = 0
                    if state not in self.Q:
                        self.Q[state] = {}
                    if action not in self.Q[state]:
                        self.Q[state][action] = 0
                
                C[sa_pair] += W
                
                # åŠ æƒæ›´æ–°Qå€¼
                self.Q[state][action] += (W / C[sa_pair]) * \
                                        (G - self.Q[state][action])
                
                # æ›´æ–°ç›®æ ‡ç­–ç•¥ï¼ˆè´ªå¿ƒï¼‰
                if state in self.Q and len(self.Q[state]) > 0:
                    target_policy[state] = max(self.Q[state], 
                                              key=self.Q[state].get)
                
                # å¦‚æœåŠ¨ä½œä¸æ˜¯è´ªå¿ƒåŠ¨ä½œï¼Œç»ˆæ­¢
                if state not in target_policy or action != target_policy[state]:
                    break
                
                # æ›´æ–°é‡è¦æ€§é‡‡æ ·æ¯”ç‡
                # è¡Œä¸ºç­–ç•¥æ˜¯Îµ-è´ªå¿ƒï¼Œç›®æ ‡ç­–ç•¥æ˜¯è´ªå¿ƒ
                num_actions = len(self.env.action_space)
                b_prob = self.epsilon / num_actions + \
                        (1 - self.epsilon) * (action == target_policy.get(state))
                pi_prob = 1.0  # è´ªå¿ƒç­–ç•¥
                
                W *= pi_prob / b_prob
        
        return self.Q, target_policy
    
    def generate_episode(self):
        """ä½¿ç”¨å½“å‰ç­–ç•¥ç”Ÿæˆä¸€ä¸ªè½¨è¿¹"""
        episode = []
        state = self.env.reset()
        done = False
        
        while not done:
            # Îµ-è´ªå¿ƒé€‰æ‹©åŠ¨ä½œ
            action = self.select_epsilon_greedy_action(state)
            next_state, reward, done, _ = self.env.step(action)
            episode.append((state, action, reward))
            state = next_state
        
        return episode
    
    def select_epsilon_greedy_action(self, state):
        """Îµ-è´ªå¿ƒåŠ¨ä½œé€‰æ‹©"""
        if np.random.random() < self.epsilon:
            return self.env.action_space.sample()
        else:
            if state not in self.Q or len(self.Q[state]) == 0:
                return self.env.action_space.sample()
            return max(self.Q[state], key=self.Q[state].get)
    
    def update_epsilon_greedy_policy(self, state):
        """æ›´æ–°Îµ-è´ªå¿ƒç­–ç•¥"""
        if state in self.Q and len(self.Q[state]) > 0:
            self.policy[state] = max(self.Q[state], key=self.Q[state].get)
```

---

## äº”ã€æ—¶åºå·®åˆ†å­¦ä¹ 

### 5.1 TDé¢„æµ‹
```python
class TDPredictor:
    """
    æ—¶åºå·®åˆ†ï¼ˆTDï¼‰é¢„æµ‹
    ç»“åˆäº†MCå’ŒDPçš„ä¼˜ç‚¹ï¼š
    - åƒMCä¸€æ ·æ˜¯model-freeçš„
    - åƒDPä¸€æ ·å¯ä»¥bootstrapï¼ˆç”¨ä¼°è®¡æ›´æ–°ä¼°è®¡ï¼‰
    """
    def __init__(self, gamma=0.99, alpha=0.1):
        self.gamma = gamma
        self.alpha = alpha
        self.V = {}
    
    def td_0(self, env, policy, num_episodes=1000):
        """
        TD(0)ï¼šå•æ­¥æ—¶åºå·®åˆ†
        V(s_t) â† V(s_t) + Î±[r_t + Î³V(s_{t+1}) - V(s_t)]
        
        TDç›®æ ‡ï¼šr_t + Î³V(s_{t+1})
        TDè¯¯å·®ï¼šÎ´_t = r_t + Î³V(s_{t+1}) - V(s_t)
        """
        for episode in range(num_episodes):
            state = env.reset()
            done = False
            
            while not done:
                # æ ¹æ®ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
                action = policy.select_action(state)
                next_state, reward, done, _ = env.step(action)
                
                # åˆå§‹åŒ–Vå€¼
                if state not in self.V:
                    self.V[state] = 0
                if next_state not in self.V:
                    self.V[next_state] = 0
                
                # TDæ›´æ–°
                td_target = reward + self.gamma * self.V[next_state] * (not done)
                td_error = td_target - self.V[state]
                self.V[state] += self.alpha * td_error
                
                state = next_state
        
        return self.V
    
    def td_lambda(self, env, policy, lambda_=0.9, num_episodes=1000):
        """
        TD(Î»)ï¼šä½¿ç”¨èµ„æ ¼è¿¹
        ç»“åˆäº†å¤šæ­¥TDçš„ä¼˜ç‚¹
        
        Î» = 0: TD(0)
        Î» = 1: MC
        """
        for episode in range(num_episodes):
            state = env.reset()
            done = False
            
            # èµ„æ ¼è¿¹
            eligibility_trace = {}
            
            while not done:
                action = policy.select_action(state)
                next_state, reward, done, _ = env.step(action)
                
                if state not in self.V:
                    self.V[state] = 0
                if next_state not in self.V:
                    self.V[next_state] = 0
                
                # TDè¯¯å·®
                td_error = reward + self.gamma * self.V[next_state] * (not done) - self.V[state]
                
                # æ›´æ–°èµ„æ ¼è¿¹
                if state not in eligibility_trace:
                    eligibility_trace[state] = 0
                eligibility_trace[state] += 1
                
                # æ›´æ–°æ‰€æœ‰çŠ¶æ€çš„ä»·å€¼ï¼ˆæ ¹æ®èµ„æ ¼è¿¹ï¼‰
                for s in eligibility_trace:
                    self.V[s] += self.alpha * td_error * eligibility_trace[s]
                    eligibility_trace[s] *= self.gamma * lambda_
                
                state = next_state
        
        return self.V
    
    def n_step_td(self, env, policy, n=5, num_episodes=1000):
        """
        næ­¥TD
        G_t^{(n)} = r_t + Î³r_{t+1} + ... + Î³^{n-1}r_{t+n-1} + Î³^nV(s_{t+n})
        """
        for episode in range(num_episodes):
            # å­˜å‚¨è½¨è¿¹
            states = [env.reset()]
            actions = []
            rewards = [0]  # å¡«å……ï¼Œä½¿ç´¢å¼•å¯¹é½
            
            T = float('inf')
            t = 0
            
            while True:
                if t < T:
                    action = policy.select_action(states[t])
                    next_state, reward, done, _ = env.step(action)
                    
                    states.append(next_state)
                    actions.append(action)
                    rewards.append(reward)
                    
                    if done:
                        T = t + 1
                
                # æ›´æ–°æ—¶åˆ»
                tau = t - n + 1
                
                if tau >= 0:
                    # è®¡ç®—næ­¥å›æŠ¥
                    G = sum([self.gamma**(i-tau-1) * rewards[i] 
                            for i in range(tau+1, min(tau+n, T)+1)])
                    
                    if tau + n < T:
                        state_tau_n = states[tau + n]
                        if state_tau_n not in self.V:
                            self.V[state_tau_n] = 0
                        G += self.gamma**n * self.V[state_tau_n]
                    
                    # æ›´æ–°
                    state_tau = states[tau]
                    if state_tau not in self.V:
                        self.V[state_tau] = 0
                    self.V[state_tau] += self.alpha * (G - self.V[state_tau])
                
                if tau == T - 1:
                    break
                
                t += 1
        
        return self.V
```

### 5.2 SARSA
```python
class SARSA:
    """
    SARSAï¼šåŒç­–ç•¥TDæ§åˆ¶ç®—æ³•
    State-Action-Reward-State-Action
    
    Q(s,a) â† Q(s,a) + Î±[r + Î³Q(s',a') - Q(s,a)]
    """
    def __init__(self, env, gamma=0.99, alpha=0.1, epsilon=0.1):
        self.env = env
        self.gamma = gamma
        self.alpha = alpha
        self.epsilon = epsilon
        self.Q = {}
    
    def train(self, num_episodes=1000):
        """SARSAè®­ç»ƒ"""
        episode_rewards = []
        
        for episode in range(num_episodes):
            state = self.env.reset()
            action = self.select_action(state)
            
            total_reward = 0
            done = False
            
            while not done:
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, _ = self.env.step(action)
                total_reward += reward
                
                # é€‰æ‹©ä¸‹ä¸€ä¸ªåŠ¨ä½œï¼ˆåŒç­–ç•¥ï¼‰
                next_action = self.select_action(next_state)
                
                # SARSAæ›´æ–°
                self.update_q(state, action, reward, next_state, next_action, done)
                
                state = next_state
                action = next_action
            
            episode_rewards.append(total_reward)
            
            # Îµè¡°å‡
            self.epsilon = max(0.01, self.epsilon * 0.995)
        
        return self.Q, episode_rewards
    
    def update_q(self, state, action, reward, next_state, next_action, done):
        """SARSA Qå€¼æ›´æ–°"""
        if state not in self.Q:
            self.Q[state] = {}
        if action not in self.Q[state]:
            self.Q[state][action] = 0
        
        if not done:
            if next_state not in self.Q:
                self.Q[next_state] = {}
            if next_action not in self.Q[next_state]:
                self.Q[next_state][next_action] = 0
            
            td_target = reward + self.gamma * self.Q[next_state][next_action]
        else:
            td_target = reward
        
        td_error = td_target - self.Q[state][action]
        self.Q[state][action] += self.alpha * td_error
    
    def select_action(self, state):
        """Îµ-è´ªå¿ƒåŠ¨ä½œé€‰æ‹©"""
        if np.random.random() < self.epsilon:
            return self.env.action_space.sample()
        else:
            if state not in self.Q or len(self.Q[state]) == 0:
                return self.env.action_space.sample()
            return max(self.Q[state], key=self.Q[state].get)

# SARSA(Î»)ï¼šå¸¦èµ„æ ¼è¿¹çš„SARSA
class SARSALambda(SARSA):
    """
    SARSA(Î»)ï¼šç»“åˆèµ„æ ¼è¿¹çš„SARSA
    å¯ä»¥æ›´å¿«åœ°ä¼ æ’­å¥–åŠ±ä¿¡å·
    """
    def __init__(self, env, gamma=0.99, alpha=0.1, epsilon=0.1, lambda_=0.9):
        super().__init__(env, gamma, alpha, epsilon)
        self.lambda_ = lambda_
    
    def train(self, num_episodes=1000):
        """SARSA(Î»)è®­ç»ƒ"""
        episode_rewards = []
        
        for episode in range(num_episodes):
            state = self.env.reset()
            action = self.select_action(state)
            
            # èµ„æ ¼è¿¹
            eligibility_trace = {}
            
            total_reward = 0
            done = False
            
            while not done:
                next_state, reward, done, _ = self.env.step(action)
                total_reward += reward
                
                next_action = self.select_action(next_state)
                
                # åˆå§‹åŒ–Qå€¼
                if state not in self.Q:
                    self.Q[state] = {}
                if action not in self.Q[state]:
                    self.Q[state][action] = 0
                
                # TDè¯¯å·®
                if not done:
                    if next_state not in self.Q:
                        self.Q[next_state] = {}
                    if next_action not in self.Q[next_state]:
                        self.Q[next_state][next_action] = 0
                    td_error = reward + self.gamma * self.Q[next_state][next_action] - \
                              self.Q[state][action]
                else:
                    td_error = reward - self.Q[state][action]
                
                # æ›´æ–°èµ„æ ¼è¿¹
                if state not in eligibility_trace:
                    eligibility_trace[state] = {}
                if action not in eligibility_trace[state]:
                    eligibility_trace[state][action] = 0
                eligibility_trace[state][action] += 1
                
                # æ›´æ–°æ‰€æœ‰çŠ¶æ€-åŠ¨ä½œå¯¹
                for s in list(eligibility_trace.keys()):
                    for a in list(eligibility_trace[s].keys()):
                        if s not in self.Q:
                            self.Q[s] = {}
                        if a not in self.Q[s]:
                            self.Q[s][a] = 0
                        
                        self.Q[s][a] += self.alpha * td_error * eligibility_trace[s][a]
                        eligibility_trace[s][a] *= self.gamma * self.lambda_
                        
                        # æ¸…é™¤å¾ˆå°çš„èµ„æ ¼è¿¹
                        if eligibility_trace[s][a] < 1e-5:
                            del eligibility_trace[s][a]
                
                state = next_state
                action = next_action
            
            episode_rewards.append(total_reward)
            self.epsilon = max(0.01, self.epsilon * 0.995)
        
        return self.Q, episode_rewards
```

### 5.3 Q-Learning
```python
class QLearning:
    """
    Q-Learningï¼šç¦»ç­–ç•¥TDæ§åˆ¶ç®—æ³•
    
    Q(s,a) â† Q(s,a) + Î±[r + Î³ max_a' Q(s',a') - Q(s,a)]
    
    å…³é”®ç‰¹ç‚¹ï¼š
    - è¡Œä¸ºç­–ç•¥ï¼šÎµ-è´ªå¿ƒï¼ˆç”¨äºæ¢ç´¢ï¼‰
    - ç›®æ ‡ç­–ç•¥ï¼šè´ªå¿ƒï¼ˆmax_a'ï¼‰
    """
    def __init__(self, env, gamma=0.99, alpha=0.1, epsilon=0.1):
        self.env = env
        self.gamma = gamma
        self.alpha = alpha
        self.epsilon = epsilon
        self.Q = {}
    
    def train(self, num_episodes=1000):
        """Q-Learningè®­ç»ƒ"""
        episode_rewards = []
        
        for episode in range(num_episodes):
            state = self.env.reset()
            total_reward = 0
            done = False
            
            while not done:
                # ä½¿ç”¨Îµ-è´ªå¿ƒé€‰æ‹©åŠ¨ä½œï¼ˆè¡Œä¸ºç­–ç•¥ï¼‰
                action = self.select_action(state)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, _ = self.env.step(action)
                total_reward += reward
                
                # Q-Learningæ›´æ–°
                self.update_q(state, action, reward, next_state, done)
                
                state = next_state
            
            episode_rewards.append(total_reward)
            
            # Îµè¡°å‡
            self.epsilon = max(0.01, self.epsilon * 0.995)
        
        return self.Q, episode_rewards
    
    def update_q(self, state, action, reward, next_state, done):
        """Q-Learningæ›´æ–°è§„åˆ™"""
        if state not in self.Q:
            self.Q[state] = {}
        if action not in self.Q[state]:
            self.Q[state][action] = 0
        
        if not done:
            # ä½¿ç”¨maxï¼ˆè´ªå¿ƒï¼Œç›®æ ‡ç­–ç•¥ï¼‰
            if next_state not in self.Q or len(self.Q[next_state]) == 0:
                max_next_q = 0
            else:
                max_next_q = max(self.Q[next_state].values())
            
            td_target = reward + self.gamma * max_next_q
        else:
            td_target = reward
        
        td_error = td_target - self.Q[state][action]
        self.Q[state][action] += self.alpha * td_error
    
    def select_action(self, state):
        """Îµ-è´ªå¿ƒåŠ¨ä½œé€‰æ‹©"""
        if np.random.random() < self.epsilon:
            return self.env.action_space.sample()
        else:
            if state not in self.Q or len(self.Q[state]) == 0:
                return self.env.action_space.sample()
            return max(self.Q[state], key=self.Q[state].get)
    
    def get_greedy_action(self, state):
        """è·å–è´ªå¿ƒåŠ¨ä½œï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        if state not in self.Q or len(self.Q[state]) == 0:
            return self.env.action_space.sample()
        return max(self.Q[state], key=self.Q[state].get)

# Double Q-Learning
class DoubleQLearning:
    """
    Double Q-Learningï¼šè§£å†³Q-Learningçš„è¿‡ä¼°è®¡é—®é¢˜
    ç»´æŠ¤ä¸¤ä¸ªQå‡½æ•°ï¼šQ1å’ŒQ2
    
    æ›´æ–°Q1æ—¶ï¼Œç”¨Q1é€‰æ‹©åŠ¨ä½œï¼Œç”¨Q2è¯„ä¼°
    æ›´æ–°Q2æ—¶ï¼Œç”¨Q2é€‰æ‹©åŠ¨ä½œï¼Œç”¨Q1è¯„ä¼°
    """
    def __init__(self, env, gamma=0.99, alpha=0.1, epsilon=0.1):
        self.env = env
        self.gamma = gamma
        self.alpha = alpha
        self.epsilon = epsilon
        self.Q1 = {}
        self.Q2 = {}
    
    def train(self, num_episodes=1000):
        """Double Q-Learningè®­ç»ƒ"""
        episode_rewards = []
        
        for episode in range(num_episodes):
            state = self.env.reset()
            total_reward = 0
            done = False
            
            while not done:
                action = self.select_action(state)
                next_state, reward, done, _ = self.env.step(action)
                total_reward += reward
                
                # éšæœºé€‰æ‹©æ›´æ–°Q1æˆ–Q2
                if np.random.random() < 0.5:
                    self.update_q1(state, action, reward, next_state, done)
                else:
                    self.update_q2(state, action, reward, next_state, done)
                
                state = next_state
            
            episode_rewards.append(total_reward)
            self.epsilon = max(0.01, self.epsilon * 0.995)
        
        return self.Q1, self.Q2, episode_rewards
    
    def update_q1(self, state, action, reward, next_state, done):
        """ä½¿ç”¨Q2è¯„ä¼°Q1é€‰æ‹©çš„åŠ¨ä½œ"""
        if state not in self.Q1:
            self.Q1[state] = {}
        if action not in self.Q1[state]:
            self.Q1[state][action] = 0
        
        if not done:
            # Q1é€‰æ‹©åŠ¨ä½œ
            if next_state not in self.Q1 or len(self.Q1[next_state]) == 0:
                best_action = self.env.action_space.sample()
            else:
                best_action = max(self.Q1[next_state], key=self.Q1[next_state].get)
            
            # Q2è¯„ä¼°
            if next_state not in self.Q2 or best_action not in self.Q2[next_state]:
                next_q = 0
            else:
                next_q = self.Q2[next_state][best_action]
            
            td_target = reward + self.gamma * next_q
        else:
            td_target = reward
        
        self.Q1[state][action] += self.alpha * (td_target - self.Q1[state][action])
    
    def update_q2(self, state, action, reward, next_state, done):
        """ä½¿ç”¨Q1è¯„ä¼°Q2é€‰æ‹©çš„åŠ¨ä½œ"""
        if state not in self.Q2:
            self.Q2[state] = {}
        if action not in self.Q2[state]:
            self.Q2[state][action] = 0
        
        if not done:
            # Q2é€‰æ‹©åŠ¨ä½œ
            if next_state not in self.Q2 or len(self.Q2[next_state]) == 0:
                best_action = self.env.action_space.sample()
            else:
                best_action = max(self.Q2[next_state], key=self.Q2[next_state].get)
            
            # Q1è¯„ä¼°
            if next_state not in self.Q1 or best_action not in self.Q1[next_state]:
                next_q = 0
            else:
                next_q = self.Q1[next_state][best_action]
            
            td_target = reward + self.gamma * next_q
        else:
            td_target = reward
        
        self.Q2[state][action] += self.alpha * (td_target - self.Q2[state][action])
    
    def select_action(self, state):
        """ä½¿ç”¨Q1+Q2çš„å¹³å‡å€¼è¿›è¡ŒÎµ-è´ªå¿ƒ"""
        if np.random.random() < self.epsilon:
            return self.env.action_space.sample()
        
        # åˆå¹¶Q1å’ŒQ2
        q_avg = {}
        all_actions = set()
        
        if state in self.Q1:
            all_actions.update(self.Q1[state].keys())
        if state in self.Q2:
            all_actions.update(self.Q2[state].keys())
        
        if not all_actions:
            return self.env.action_space.sample()
        
        for a in all_actions:
            q1_val = self.Q1.get(state, {}).get(a, 0)
            q2_val = self.Q2.get(state, {}).get(a, 0)
            q_avg[a] = (q1_val + q2_val) / 2
        
        return max(q_avg, key=q_avg.get)
```

---

## å…­ã€å€¼å‡½æ•°é€¼è¿‘

### 6.1 çº¿æ€§å‡½æ•°é€¼è¿‘
```python
import numpy as np

class LinearFunctionApproximation:
    """
    çº¿æ€§å‡½æ•°é€¼è¿‘
    V(s) â‰ˆ Ï†(s)áµ€w
    Q(s,a) â‰ˆ Ï†(s,a)áµ€w
    
    å…¶ä¸­Ï†æ˜¯ç‰¹å¾å‘é‡ï¼Œwæ˜¯æƒé‡
    """
    def __init__(self, feature_dim, alpha=0.01):
        self.w = np.zeros(feature_dim)
        self.alpha = alpha
    
    def predict_value(self, features):
        """é¢„æµ‹çŠ¶æ€ä»·å€¼"""
        return np.dot(features, self.w)
    
    def update(self, features, target):
        """
        æ¢¯åº¦ä¸‹é™æ›´æ–°
        w â† w + Î±[target - V(s)]âˆ‡V(s)
        w â† w + Î±[target - V(s)]Ï†(s)
        """
        prediction = self.predict_value(features)
        error = target - prediction
        self.w += self.alpha * error * features
    
    def semi_gradient_td(self, env, feature_extractor, num_episodes=1000, gamma=0.99):
        """
        åŠæ¢¯åº¦TD(0)
        åªå¯¹ä»·å€¼å‡½æ•°çš„æ¢¯åº¦è¿›è¡Œæ›´æ–°ï¼Œä¸å¯¹ç›®æ ‡çš„æ¢¯åº¦æ›´æ–°
        """
        for episode in range(num_episodes):
            state = env.reset()
            done = False
            
            while not done:
                # æå–ç‰¹å¾
                features = feature_extractor(state)
                
                # é€‰æ‹©åŠ¨ä½œï¼ˆè¿™é‡Œå‡è®¾æœ‰ç­–ç•¥ï¼‰
                action = env.action_space.sample()
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, _ = env.step(action)
                
                # TDæ›´æ–°
                if not done:
                    next_features = feature_extractor(next_state)
                    td_target = reward + gamma * self.predict_value(next_features)
                else:
                    td_target = reward
                
                self.update(features, td_target)
                state = next_state
        
        return self.w

# ç‰¹å¾å·¥ç¨‹ç¤ºä¾‹
class FeatureExtractor:
    """ç‰¹å¾æå–å™¨"""
    
    @staticmethod
    def polynomial_features(state, degree=2):
        """å¤šé¡¹å¼ç‰¹å¾"""
        state = np.array(state).flatten()
        features = [1]  # åç½®é¡¹
        
        # ä¸€é˜¶ç‰¹å¾
        features.extend(state)
        
        # é«˜é˜¶ç‰¹å¾
        if degree >= 2:
            for i in range(len(state)):
                for j in range(i, len(state)):
                    features.append(state[i] * state[j])
        
        return np.array(features)
    
    @staticmethod
    def tile_coding(state, num_tilings=8, num_tiles=8):
        """
        Tile Codingï¼šå°†è¿ç»­çŠ¶æ€ç©ºé—´åˆ†æˆå¤šä¸ªé‡å çš„ç½‘æ ¼
        æ¯ä¸ªç½‘æ ¼ç§°ä¸ºä¸€ä¸ªtiling
        """
        features = []
        state = np.array(state).flatten()
        
        for tiling in range(num_tilings):
            # ä¸ºæ¯ä¸ªtilingæ·»åŠ åç§»
            offset = tiling / num_tilings
            
            for dim in range(len(state)):
                # è®¡ç®—è¯¥ç»´åº¦çš„tileç´¢å¼•
                tile_idx = int((state[dim] + offset) * num_tiles)
                tile_idx = max(0, min(num_tiles - 1, tile_idx))
                
                # åˆ›å»ºone-hotç¼–ç 
                one_hot = np.zeros(num_tiles)
                one_hot[tile_idx] = 1
                features.extend(one_hot)
        
        return np.array(features)
    
    @staticmethod
    def rbf_features(state, centers, sigma=1.0):
        """
        å¾„å‘åŸºå‡½æ•°ï¼ˆRBFï¼‰ç‰¹å¾
        Ï†_i(s) = exp(-||s - c_i||Â² / (2ÏƒÂ²))
        """
        state = np.array(state).flatten()
        features = []
        
        for center in centers:
            center = np.array(center).flatten()
            distance = np.linalg.norm(state - center)
            feature = np.exp(-distance**2 / (2 * sigma**2))
            features.append(feature)
        
        return np.array(features)

# çº¿æ€§Qå‡½æ•°é€¼è¿‘
class LinearQApproximation:
    """
    çº¿æ€§Qå‡½æ•°é€¼è¿‘
    Q(s,a) = Ï†(s,a)áµ€w
    """
    def __init__(self, num_actions, feature_dim, alpha=0.01):
        self.num_actions = num_actions
        # ä¸ºæ¯ä¸ªåŠ¨ä½œç»´æŠ¤ä¸€ä¸ªæƒé‡å‘é‡
        self.w = np.zeros((num_actions, feature_dim))
        self.alpha = alpha
    
    def predict_q(self, features, action):
        """é¢„æµ‹Q(s,a)"""
        return np.dot(features, self.w[action])
    
    def predict_all_q(self, features):
        """é¢„æµ‹æ‰€æœ‰åŠ¨ä½œçš„Qå€¼"""
        return features @ self.w.T
    
    def update_q_learning(self, features, action, reward, next_features, done, gamma=0.99):
        """
        Q-Learningæ›´æ–°
        w â† w + Î±[r + Î³ max_a' Q(s',a') - Q(s,a)]Ï†(s,a)
        """
        current_q = self.predict_q(features, action)
        
        if not done:
            max_next_q = np.max(self.predict_all_q(next_features))
            td_target = reward + gamma * max_next_q
        else:
            td_target = reward
        
        td_error = td_target - current_q
        self.w[action] += self.alpha * td_error * features
    
    def update_sarsa(self, features, action, reward, next_features, next_action, done, gamma=0.99):
        """
        SARSAæ›´æ–°
        w â† w + Î±[r + Î³Q(s',a') - Q(s,a)]Ï†(s,a)
        """
        current_q = self.predict_q(features, action)
        
        if not done:
            next_q = self.predict_q(next_features, next_action)
            td_target = reward + gamma * next_q
        else:
            td_target = reward
        
        td_error = td_target - current_q
        self.w[action] += self.alpha * td_error * features
```

### 6.2 ç¥ç»ç½‘ç»œå‡½æ•°é€¼è¿‘
```python
import torch
import torch.nn as nn
import torch.optim as optim

class ValueNetwork(nn.Module):
    """
    ä½¿ç”¨ç¥ç»ç½‘ç»œé€¼è¿‘ä»·å€¼å‡½æ•°
    """
    def __init__(self, state_dim, hidden_dims=[64, 64]):
        super(ValueNetwork, self).__init__()
        
        layers = []
        input_dim = state_dim
        
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(input_dim, hidden_dim))
            layers.append(nn.ReLU())
            input_dim = hidden_dim
        
        layers.append(nn.Linear(input_dim, 1))  # è¾“å‡ºå•ä¸ªå€¼
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, state):
        return self.network(state)

class QNetwork(nn.Module):
    """
    ä½¿ç”¨ç¥ç»ç½‘ç»œé€¼è¿‘Qå‡½æ•°
    """
    def __init__(self, state_dim, action_dim, hidden_dims=[64, 64]):
        super(QNetwork, self).__init__()
        
        layers = []
        input_dim = state_dim
        
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(input_dim, hidden_dim))
            layers.append(nn.ReLU())
            input_dim = hidden_dim
        
        layers.append(nn.Linear(input_dim, action_dim))  # è¾“å‡ºæ‰€æœ‰åŠ¨ä½œçš„Qå€¼
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, state):
        return self.network(state)

# ä½¿ç”¨ç¥ç»ç½‘ç»œçš„TDå­¦ä¹ 
class NeuralTD:
    """ä½¿ç”¨ç¥ç»ç½‘ç»œçš„TDå­¦ä¹ """
    def __init__(self, state_dim, lr=0.001, gamma=0.99):
        self.value_net = ValueNetwork(state_dim)
        self.optimizer = optim.Adam(self.value_net.parameters(), lr=lr)
        self.gamma = gamma
    
    def train_step(self, state, reward, next_state, done):
        """å•æ­¥è®­ç»ƒ"""
        state_tensor = torch.FloatTensor(state)
        next_state_tensor = torch.FloatTensor(next_state)
        
        # å‰å‘ä¼ æ’­
        current_value = self.value_net(state_tensor)
        
        # è®¡ç®—TDç›®æ ‡
        with torch.no_grad():
            if not done:
                next_value = self.value_net(next_state_tensor)
                td_target = reward + self.gamma * next_value
            else:
                td_target = torch.FloatTensor([reward])
        
        # è®¡ç®—æŸå¤±
        loss = nn.MSELoss()(current_value, td_target)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return loss.item()

# Dueling Network
class DuelingQNetwork(nn.Module):
    """
    Duelingç½‘ç»œæ¶æ„
    Q(s,a) = V(s) + A(s,a) - mean(A(s,Â·))
    
    åˆ†ç¦»ä»·å€¼å’Œä¼˜åŠ¿ï¼Œå­¦ä¹ æ›´ç¨³å®š
    """
    def __init__(self, state_dim, action_dim, hidden_dim=64):
        super(DuelingQNetwork, self).__init__()
        
        # å…±äº«ç‰¹å¾æå–å±‚
        self.feature = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU()
        )
        
        # ä»·å€¼æµ
        self.value_stream = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        # ä¼˜åŠ¿æµ
        self.advantage_stream = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
    
    def forward(self, state):
        features = self.feature(state)
        value = self.value_stream(features)
        advantages = self.advantage_stream(features)
        
        # ç»“åˆä»·å€¼å’Œä¼˜åŠ¿
        # Q(s,a) = V(s) + (A(s,a) - mean_a A(s,a))
        q_values = value + (advantages - advantages.mean(dim=-1, keepdim=True))
        
        return q_values
```

---

## ä¸ƒã€ç­–ç•¥æ¢¯åº¦æ–¹æ³•

### 7.1 REINFORCEç®—æ³•
```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical

class PolicyNetwork(nn.Module):
    """ç­–ç•¥ç½‘ç»œï¼šè¾“å‡ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ"""
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)
    
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        action_probs = F.softmax(self.fc3(x), dim=-1)
        return action_probs

class REINFORCE:
    """
    REINFORCEç®—æ³•ï¼ˆè’™ç‰¹å¡æ´›ç­–ç•¥æ¢¯åº¦ï¼‰
    
    âˆ‡J(Î¸) = ğ”¼_Ï€[âˆ‡log Ï€(a|s,Î¸) * G_t]
    
    åŸºæœ¬æ€æƒ³ï¼š
    - å¥½çš„åŠ¨ä½œï¼ˆé«˜å›æŠ¥ï¼‰â†’å¢åŠ æ¦‚ç‡
    - åçš„åŠ¨ä½œï¼ˆä½å›æŠ¥ï¼‰â†’é™ä½æ¦‚ç‡
    """
    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99):
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        self.gamma = gamma
    
    def select_action(self, state):
        """æ ¹æ®ç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        state = torch.FloatTensor(state).unsqueeze(0)
        probs = self.policy(state)
        dist = Categorical(probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        return action.item(), log_prob
    
    def train(self, env, num_episodes=1000):
        """è®­ç»ƒREINFORCE"""
        episode_rewards = []
        
        for episode in range(num_episodes):
            # æ”¶é›†ä¸€ä¸ªå®Œæ•´è½¨è¿¹
            states, actions, rewards, log_probs = [], [], [], []
            
            state = env.reset()
            done = False
            
            while not done:
                action, log_prob = self.select_action(state)
                next_state, reward, done, _ = env.step(action)
                
                states.append(state)
                actions.append(action)
                rewards.append(reward)
                log_probs.append(log_prob)
                
                state = next_state
            
            # è®¡ç®—å›æŠ¥
            returns = self.compute_returns(rewards)
            
            # ç­–ç•¥æ¢¯åº¦æ›´æ–°
            policy_loss = []
            for log_prob, G in zip(log_probs, returns):
                policy_loss.append(-log_prob * G)
            
            policy_loss = torch.stack(policy_loss).sum()
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            policy_loss.backward()
            self.optimizer.step()
            
            episode_rewards.append(sum(rewards))
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(episode_rewards[-100:])
                print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}")
        
        return episode_rewards
    
    def compute_returns(self, rewards):
        """è®¡ç®—æŠ˜æ‰£å›æŠ¥"""
        returns = []
        G = 0
        for reward in reversed(rewards):
            G = reward + self.gamma * G
            returns.insert(0, G)
        
        # æ ‡å‡†åŒ–å›æŠ¥ï¼ˆå‡å°‘æ–¹å·®ï¼‰
        returns = torch.FloatTensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)
        
        return returns

# å¸¦åŸºçº¿çš„REINFORCE
class REINFORCEWithBaseline:
    """
    å¸¦åŸºçº¿çš„REINFORCE
    ä½¿ç”¨ä»·å€¼å‡½æ•°ä½œä¸ºåŸºçº¿å‡å°‘æ–¹å·®
    
    âˆ‡J(Î¸) = ğ”¼[âˆ‡log Ï€(a|s,Î¸) * (G_t - b(s))]
    å…¶ä¸­b(s)æ˜¯åŸºçº¿ï¼Œé€šå¸¸ç”¨V(s)
    """
    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99):
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.value_net = ValueNetwork(state_dim)
        
        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr)
        
        self.gamma = gamma
    
    def select_action(self, state):
        state = torch.FloatTensor(state).unsqueeze(0)
        probs = self.policy(state)
        dist = Categorical(probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        return action.item(), log_prob
    
    def train(self, env, num_episodes=1000):
        episode_rewards = []
        
        for episode in range(num_episodes):
            states, actions, rewards, log_probs, values = [], [], [], [], []
            
            state = env.reset()
            done = False
            
            while not done:
                action, log_prob = self.select_action(state)
                
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                value = self.value_net(state_tensor)
                
                next_state, reward, done, _ = env.step(action)
                
                states.append(state)
                actions.append(action)
                rewards.append(reward)
                log_probs.append(log_prob)
                values.append(value)
                
                state = next_state
            
            # è®¡ç®—å›æŠ¥å’Œä¼˜åŠ¿
            returns = self.compute_returns(rewards)
            values = torch.cat(values).squeeze()
            advantages = returns - values.detach()
            
            # æ›´æ–°ç­–ç•¥ç½‘ç»œ
            policy_loss = []
            for log_prob, advantage in zip(log_probs, advantages):
                policy_loss.append(-log_prob * advantage)
            policy_loss = torch.stack(policy_loss).sum()
            
            self.policy_optimizer.zero_grad()
            policy_loss.backward()
            self.policy_optimizer.step()
            
            # æ›´æ–°ä»·å€¼ç½‘ç»œ
            value_loss = F.mse_loss(values, returns)
            
            self.value_optimizer.zero_grad()
            value_loss.backward()
            self.value_optimizer.step()
            
            episode_rewards.append(sum(rewards))
        
        return episode_rewards
    
    def compute_returns(self, rewards):
        returns = []
        G = 0
        for reward in reversed(rewards):
            G = reward + self.gamma * G
            returns.insert(0, G)
        return torch.FloatTensor(returns)
```

### 7.2 Actor-CriticåŸºç¡€
```python
class ActorCriticNetwork(nn.Module):
    """
    Actor-Criticç½‘ç»œ
    Actorï¼šè¾“å‡ºç­–ç•¥Ï€(a|s)
    Criticï¼šè¾“å‡ºä»·å€¼V(s)
    """
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(ActorCriticNetwork, self).__init__()
        
        # å…±äº«å±‚
        self.shared = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU()
        )
        
        # Actorå¤´
        self.actor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
        
        # Criticå¤´
        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, state):
        features = self.shared(state)
        action_probs = F.softmax(self.actor(features), dim=-1)
        value = self.critic(features)
        return action_probs, value

class ActorCritic:
    """
    Actor-Criticç®—æ³•ï¼ˆå•æ­¥ï¼‰
    ä¼˜åŠ¿ï¼šåœ¨çº¿å­¦ä¹ ï¼Œä½æ–¹å·®
    """
    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99):
        self.ac_net = ActorCriticNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.ac_net.parameters(), lr=lr)
        self.gamma = gamma
    
    def select_action(self, state):
        state = torch.FloatTensor(state).unsqueeze(0)
        action_probs, value = self.ac_net(state)
        dist = Categorical(action_probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        return action.item(), log_prob, value
    
    def train(self, env, num_episodes=1000):
        episode_rewards = []
        
        for episode in range(num_episodes):
            state = env.reset()
            done = False
            total_reward = 0
            
            while not done:
                # é€‰æ‹©åŠ¨ä½œ
                action, log_prob, value = self.select_action(state)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, _ = env.step(action)
                total_reward += reward
                
                # è®¡ç®—TDè¯¯å·®ï¼ˆä¼˜åŠ¿ï¼‰
                next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)
                _, next_value = self.ac_net(next_state_tensor)
                
                if done:
                    td_target = reward
                else:
                    td_target = reward + self.gamma * next_value.item()
                
                advantage = td_target - value.item()
                
                # è®¡ç®—æŸå¤±
                actor_loss = -log_prob * advantage
                critic_loss = F.mse_loss(value, torch.FloatTensor([td_target]))
                
                loss = actor_loss + critic_loss
                
                # æ›´æ–°ç½‘ç»œ
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                
                state = next_state
            
            episode_rewards.append(total_reward)
        
        return episode_rewards
```

---

## å…«ã€Actor-Criticæ–¹æ³•

### 8.1 A2C (Advantage Actor-Critic)
```python
class A2C:
    """
    Advantage Actor-Critic (A2C)
    ä½¿ç”¨ä¼˜åŠ¿å‡½æ•°ï¼šA(s,a) = Q(s,a) - V(s)
    """
    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99, 
                 value_coef=0.5, entropy_coef=0.01):
        self.ac_net = ActorCriticNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.ac_net.parameters(), lr=lr)
        
        self.gamma = gamma
        self.value_coef = value_coef
        self.entropy_coef = entropy_coef
    
    def train_step(self, states, actions, rewards, next_states, dones):
        """
        æ‰¹é‡æ›´æ–°
        """
        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        rewards = torch.FloatTensor(rewards)
        next_states = torch.FloatTensor(next_states)
        dones = torch.FloatTensor(dones)
        
        # å‰å‘ä¼ æ’­
        action_probs, values = self.ac_net(states)
        _, next_values = self.ac_net(next_states)
        
        # è®¡ç®—ä¼˜åŠ¿
        td_targets = rewards + self.gamma * next_values.squeeze() * (1 - dones)
        advantages = td_targets - values.squeeze()
        
        # ActoræŸå¤±ï¼ˆç­–ç•¥æ¢¯åº¦ï¼‰
        dist = Categorical(action_probs)
        log_probs = dist.log_prob(actions)
        actor_loss = -(log_probs * advantages.detach()).mean()
        
        # CriticæŸå¤±ï¼ˆTDè¯¯å·®ï¼‰
        critic_loss = advantages.pow(2).mean()
        
        # ç†µæ­£åˆ™åŒ–ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
        entropy = dist.entropy().mean()
        
        # æ€»æŸå¤±
        loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy
        
        # æ›´æ–°
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.ac_net.parameters(), 0.5)
        self.optimizer.step()
        
        return loss.item(), actor_loss.item(), critic_loss.item(), entropy.item()

### 8.2 A3C (Asynchronous Advantage Actor-Critic)
```python
import torch.multiprocessing as mp

class A3C:
    """
    A3Cï¼šå¼‚æ­¥ä¼˜åŠ¿Actor-Critic
    ä½¿ç”¨å¤šä¸ªå¹¶è¡Œworkeræ”¶é›†ç»éªŒ
    """
    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99):
        # å…¨å±€ç½‘ç»œï¼ˆå…±äº«å‚æ•°ï¼‰
        self.global_net = ActorCriticNetwork(state_dim, action_dim)
        self.global_net.share_memory()
        
        self.optimizer = optim.Adam(self.global_net.parameters(), lr=lr)
        self.gamma = gamma
    
    def worker(self, worker_id, env_fn, num_episodes):
        """
        Workerè¿›ç¨‹ï¼šç‹¬ç«‹é‡‡æ ·å’Œè®¡ç®—æ¢¯åº¦
        """
        # åˆ›å»ºæœ¬åœ°ç½‘ç»œ
        local_net = ActorCriticNetwork(state_dim, action_dim)
        env = env_fn()
        
        for episode in range(num_episodes):
            # åŒæ­¥å…¨å±€å‚æ•°åˆ°æœ¬åœ°
            local_net.load_state_dict(self.global_net.state_dict())
            
            # æ”¶é›†è½¨è¿¹
            states, actions, rewards = [], [], []
            state = env.reset()
            done = False
            
            while not done:
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                action_probs, _ = local_net(state_tensor)
                dist = Categorical(action_probs)
                action = dist.sample()
                
                next_state, reward, done, _ = env.step(action.item())
                
                states.append(state)
                actions.append(action.item())
                rewards.append(reward)
                
                state = next_state
            
            # è®¡ç®—æŸå¤±å¹¶æ›´æ–°å…¨å±€ç½‘ç»œ
            self.update_global(local_net, states, actions, rewards)
    
    def update_global(self, local_net, states, actions, rewards):
        """æ›´æ–°å…¨å±€ç½‘ç»œ"""
        # è®¡ç®—å›æŠ¥
        returns = []
        G = 0
        for reward in reversed(rewards):
            G = reward + self.gamma * G
            returns.insert(0, G)
        returns = torch.FloatTensor(returns)
        
        # è®¡ç®—æŸå¤±
        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        
        action_probs, values = local_net(states)
        dist = Categorical(action_probs)
        log_probs = dist.log_prob(actions)
        
        advantages = returns - values.squeeze().detach()
        
        actor_loss = -(log_probs * advantages).mean()
        critic_loss = advantages.pow(2).mean()
        entropy = dist.entropy().mean()
        
        loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy
        
        # è®¡ç®—æ¢¯åº¦
        self.optimizer.zero_grad()
        loss.backward()
        
        # å°†æœ¬åœ°æ¢¯åº¦åº”ç”¨åˆ°å…¨0
                for next_state in self.mdp.states:
                    trans_prob = self.mdp.get_transition_prob(state, action, next_state)
                    reward = self.mdp.get_reward(state, action, next_state)
                    q += trans_prob * (reward + self.mdp.gamma * V[next_state])
                q_values[action] = q
            
            # é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
            best_action = max(q_values, key=q_values.get)
            new_policy[state] = best_action
            
            # æ£€æŸ¥ç­–ç•¥æ˜¯å¦æ”¹å˜
            if state in new_policy and new_policy[state] != best_action:
                policy_stable = False
        
        return new_policy, policy_stable
```

### 3.3 ç­–ç•¥è¿­ä»£
```python
class PolicyIteration:
    """
    ç­–ç•¥è¿­ä»£ï¼šäº¤æ›¿è¿›è¡Œç­–ç•¥è¯„ä¼°å’Œç­–ç•¥æ”¹è¿›
    1. ç­–ç•¥è¯„ä¼°ï¼šè®¡ç®—V^Ï€
    2. ç­–ç•¥æ”¹è¿›ï¼šÏ€' = greedy(V^Ï€)
    3. é‡å¤ç›´åˆ°ç­–ç•¥æ”¶æ•›
    """
    def __init__(self, mdp, theta=1e-6):
        self.mdp = mdp
        self.theta = theta
    
    def iterate(self, max_iterations=100):
        """ç­–ç•¥è¿­ä»£ä¸»å¾ªç¯"""
        # åˆå§‹åŒ–éšæœºç­–ç•¥
        policy = {s: np.random.choice(self.mdp.actions) 
                 for s in self.mdp.states}
        V = {s: 0 for s in self.mdp.states}
        
        for iteration in range(max_iterations):
            # 1. ç­–ç•¥è¯„ä¼°
            V = self.policy_evaluation(policy, V)
            
            # 2. ç­–ç•¥æ”¹è¿›
            new_policy, policy_stable = self.policy_improvement(V)
            
            if policy_stable:
                print(f"ç­–ç•¥è¿­ä»£æ”¶æ•›äºç¬¬ {iteration+1} æ¬¡è¿­ä»£")
                break
            
            policy = new_policy
        
        return policy, V
    
    def policy_evaluation(self, policy, V):
        """ç­–ç•¥è¯„ä¼°å­è¿‡ç¨‹"""
        while True:
            delta = 0
            new_V = V.copy()
            
            for state in self.mdp.states:
                if self.mdp.is_terminal(state):
                    continue
                
                action = policy[state]
                new_value = 0
                
                for next_state in self.mdp.states:
                    trans_prob = self.mdp.get_transition_prob(state, action, next_state)
                    reward = self.mdp.get_reward(state, action, next_state)
                    new_value += trans_prob * (reward + self.mdp.gamma * V[next_state])
                
                new_V[state] = new_value
                delta = max(delta, abs(V[state] - new_value))
            
            V = new_V
            
            if delta < self.theta:
                break
        
        return V
    
    def policy_improvement(self, V):
        """ç­–ç•¥æ”¹è¿›å­è¿‡ç¨‹"""
        new_policy = {}
        policy_stable = True
        
        for state in self.mdp.states:
            if self.mdp.is_terminal(state):
                continue
            
            # è®¡ç®—Qå€¼
            q_values = {}
            for action in self.mdp.actions:
                q = 0
                for next_state in self.mdp.states:
                    trans_prob = self.mdp.get_transition_prob(state, action, next_state)
                    reward = self.mdp.get_reward(state, action, next_state)
                    q += trans_prob * (reward + self.mdp.gamma * V[next_state])
                q_values[action] = q
            
            best_action = max(q_values, key=q_values.get)
            new_policy[state] = best_action
        
        return new_policy, policy_stable
```

### 3.4 ä»·å€¼è¿­ä»£
```python
class ValueIteration:
    """
    ä»·å€¼è¿­ä»£ï¼šç›´æ¥è¿­ä»£æœ€ä¼˜è´å°”æ›¼æ–¹ç¨‹
    V_{k+1}(s) = max_a Î£_{s'} P(s'|s,a)[R(s,a,s') + Î³V_k(s')]
    
    ç›¸æ¯”ç­–ç•¥è¿­ä»£ï¼Œä»·å€¼è¿­ä»£æ›´ç®€å•é«˜æ•ˆ
    """
    def __init__(self, mdp, theta=1e-6):
        self.mdp = mdp
        self.theta = theta
    
    def iterate(self, max_iterations=1000):
        """ä»·å€¼è¿­ä»£ä¸»å¾ªç¯"""
        # åˆå§‹åŒ–ä»·å€¼å‡½æ•°
        V = {s: 0 for s in self.mdp.states}
        
        for iteration in range(max_iterations):
            delta = 0
            new_V = V.copy()
            
            for state in self.mdp.states:
                if self.mdp.is_terminal(state):
                    continue
                
                # è´å°”æ›¼æœ€ä¼˜æ›´æ–°
                max_value = float('-inf')
                
                for action in self.mdp.actions:
                    value = 0
                    for next_state in self.mdp.states:
                        trans_prob = self.mdp.get_transition_prob(state, action, next_state)
                        reward = self.mdp.get_reward(state, action, next_state)
                        value += trans_prob * (reward + self.mdp.gamma * V[next_state])
                    
                    max_value = max(max_value, value)
                
                new_V[state] = max_value
                delta = max(delta, abs(V[state] - new_value))
            
            V = new_V
            
            if delta < self.theta:
                print(f"ä»·å€¼è¿­ä»£æ”¶æ•›äºç¬¬ {iteration+1} æ¬¡è¿­ä»£")
                break
        
        # æå–æœ€ä¼˜ç­–ç•¥
        policy = self.extract_policy(V)
        
        return policy, V
    
    def extract_policy(self, V):
        """ä»ä»·å€¼å‡½æ•°æå–ç­–ç•¥"""
        policy = {}
        
        for state in self.mdp.states:
            if self.mdp.is_terminal(state):
                continue
            
            q_values = {}
            for action in self.mdp.actions:
                q = 0
                for next_state in self.mdp.states:
                    trans_prob = self.mdp.get_transition_prob(state, action, next_state)
                    reward = self.mdp.get_reward(state, action, next_state)
                    q += trans_prob * (reward + self.mdp.gamma * V[next_state])
                q_values[action] = q
            
            policy[state] = max(q_values, key=q_values.get)
        
        return policy
```

---

## ä¹ã€æ·±åº¦å¼ºåŒ–å­¦ä¹ 

### 9.1 DQN (Deep Q-Network)

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from collections import deque, namedtuple
import random

# ç»éªŒå›æ”¾ç¼“å†²åŒº
Transition = namedtuple('Transition', 
                       ('state', 'action', 'reward', 'next_state', 'done'))

class ReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        """æ·»åŠ ç»éªŒ"""
        self.buffer.append(Transition(state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        """éšæœºé‡‡æ ·æ‰¹é‡ç»éªŒ"""
        transitions = random.sample(self.buffer, batch_size)
        batch = Transition(*zip(*transitions))
        
        states = torch.FloatTensor(batch.state)
        actions = torch.LongTensor(batch.action)
        rewards = torch.FloatTensor(batch.reward)
        next_states = torch.FloatTensor(batch.next_state)
        dones = torch.FloatTensor(batch.done)
        
        return states, actions, rewards, next_states, dones
    
    def __len__(self):
        return len(self.buffer)

class DQNNetwork(nn.Module):
    """DQNç½‘ç»œæ¶æ„"""
    def __init__(self, state_dim, action_dim, hidden_dims=[128, 128]):
        super(DQNNetwork, self).__init__()
        
        layers = []
        input_dim = state_dim
        
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(input_dim, hidden_dim))
            layers.append(nn.ReLU())
            input_dim = hidden_dim
        
        layers.append(nn.Linear(input_dim, action_dim))
        self.network = nn.Sequential(*layers)
    
    def forward(self, state):
        return self.network(state)

class DQN:
    """
    Deep Q-Network
    
    å…³é”®æŠ€æœ¯ï¼š
    1. ç»éªŒå›æ”¾ (Experience Replay)
    2. ç›®æ ‡ç½‘ç»œ (Target Network)
    3. Îµ-è´ªå¿ƒæ¢ç´¢
    """
    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99,
                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,
                 buffer_size=10000, batch_size=64, target_update=10):
        
        # Qç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.q_net = DQNNetwork(state_dim, action_dim)
        self.target_net = DQNNetwork(state_dim, action_dim)
        self.target_net.load_state_dict(self.q_net.state_dict())
        
        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)
        
        # ç»éªŒå›æ”¾
        self.replay_buffer = ReplayBuffer(buffer_size)
        
        # è¶…å‚æ•°
        self.gamma = gamma
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        self.batch_size = batch_size
        self.target_update = target_update
        
        self.action_dim = action_dim
        self.update_counter = 0
    
    def select_action(self, state, training=True):
        """Îµ-è´ªå¿ƒåŠ¨ä½œé€‰æ‹©"""
        if training and random.random() < self.epsilon:
            return random.randrange(self.action_dim)
        
        with torch.no_grad():
            state = torch.FloatTensor(state).unsqueeze(0)
            q_values = self.q_net(state)
            return q_values.argmax().item()
    
    def train_step(self):
        """å•æ­¥è®­ç»ƒ"""
        if len(self.replay_buffer) < self.batch_size:
            return None
        
        # é‡‡æ ·æ‰¹é‡ç»éªŒ
        states, actions, rewards, next_states, dones = \
            self.replay_buffer.sample(self.batch_size)
        
        # å½“å‰Qå€¼
        q_values = self.q_net(states).gather(1, actions.unsqueeze(1)).squeeze()
        
        # ç›®æ ‡Qå€¼ï¼ˆä½¿ç”¨ç›®æ ‡ç½‘ç»œï¼‰
        with torch.no_grad():
            next_q_values = self.target_net(next_states).max(1)[0]
            target_q_values = rewards + self.gamma * next_q_values * (1 - dones)
        
        # è®¡ç®—æŸå¤±
        loss = nn.MSELoss()(q_values, target_q_values)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), 1.0)
        self.optimizer.step()
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.update_counter += 1
        if self.update_counter % self.target_update == 0:
            self.target_net.load_state_dict(self.q_net.state_dict())
        
        return loss.item()
    
    def train(self, env, num_episodes=1000):
        """è®­ç»ƒDQN"""
        episode_rewards = []
        
        for episode in range(num_episodes):
            state = env.reset()
            total_reward = 0
            done = False
            
            while not done:
                # é€‰æ‹©åŠ¨ä½œ
                action = self.select_action(state)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, _ = env.step(action)
                total_reward += reward
                
                # å­˜å‚¨ç»éªŒ
                self.replay_buffer.push(state, action, reward, next_state, done)
                
                # è®­ç»ƒ
                loss = self.train_step()
                
                state = next_state
            
            # Îµè¡°å‡
            self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)
            
            episode_rewards.append(total_reward)
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(episode_rewards[-100:])
                print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}, Epsilon: {self.epsilon:.3f}")
        
        return episode_rewards
```

### 9.2 Double DQN

```python
class DoubleDQN(DQN):
    """
    Double DQNï¼šè§£å†³Qå€¼è¿‡ä¼°è®¡é—®é¢˜
    
    ä½¿ç”¨åœ¨çº¿ç½‘ç»œé€‰æ‹©åŠ¨ä½œï¼Œç›®æ ‡ç½‘ç»œè¯„ä¼°Qå€¼
    Q_target = r + Î³ * Q_target(s', argmax_a Q(s',a))
    """
    def train_step(self):
        """Double DQNè®­ç»ƒæ­¥éª¤"""
        if len(self.replay_buffer) < self.batch_size:
            return None
        
        states, actions, rewards, next_states, dones = \
            self.replay_buffer.sample(self.batch_size)
        
        # å½“å‰Qå€¼
        q_values = self.q_net(states).gather(1, actions.unsqueeze(1)).squeeze()
        
        # Double DQNç›®æ ‡
        with torch.no_grad():
            # ä½¿ç”¨åœ¨çº¿ç½‘ç»œé€‰æ‹©åŠ¨ä½œ
            next_actions = self.q_net(next_states).argmax(1)
            # ä½¿ç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼°Qå€¼
            next_q_values = self.target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()
            target_q_values = rewards + self.gamma * next_q_values * (1 - dones)
        
        loss = nn.MSELoss()(q_values, target_q_values)
        
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), 1.0)
        self.optimizer.step()
        
        self.update_counter += 1
        if self.update_counter % self.target_update == 0:
            self.target_net.load_state_dict(self.q_net.state_dict())
        
        return loss.item()
```

### 9.3 Dueling DQN

```python
class DuelingDQNNetwork(nn.Module):
    """
    Dueling DQNç½‘ç»œ
    Q(s,a) = V(s) + A(s,a) - mean(A(s,Â·))
    """
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(DuelingDQNNetwork, self).__init__()
        
        # å…±äº«ç‰¹å¾æå–å±‚
        self.feature = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # ä»·å€¼æµ V(s)
        self.value_stream = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        # ä¼˜åŠ¿æµ A(s,a)
        self.advantage_stream = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
    
    def forward(self, state):
        features = self.feature(state)
        value = self.value_stream(features)
        advantages = self.advantage_stream(features)
        
        # ç»“åˆï¼šQ = V + (A - mean(A))
        q_values = value + (advantages - advantages.mean(dim=-1, keepdim=True))
        return q_values
```

### 9.4 Prioritized Experience Replay

```python
class SumTree:
    """æ±‚å’Œæ ‘ï¼šç”¨äºä¼˜å…ˆçº§é‡‡æ ·"""
    def __init__(self, capacity):
        self.capacity = capacity
        self.tree = np.zeros(2 * capacity - 1)
        self.data = np.zeros(capacity, dtype=object)
        self.write_idx = 0
        self.n_entries = 0
    
    def _propagate(self, idx, change):
        """å‘ä¸Šä¼ æ’­ä¼˜å…ˆçº§å˜åŒ–"""
        parent = (idx - 1) // 2
        self.tree[parent] += change
        if parent != 0:
            self._propagate(parent, change)
    
    def update(self, idx, priority):
        """æ›´æ–°ä¼˜å…ˆçº§"""
        change = priority - self.tree[idx]
        self.tree[idx] = priority
        self._propagate(idx, change)
    
    def add(self, priority, data):
        """æ·»åŠ æ•°æ®"""
        idx = self.write_idx + self.capacity - 1
        self.data[self.write_idx] = data
        self.update(idx, priority)
        
        self.write_idx = (self.write_idx + 1) % self.capacity
        self.n_entries = min(self.n_entries + 1, self.capacity)
    
    def get(self, s):
        """æ ¹æ®ä¼˜å…ˆçº§é‡‡æ ·"""
        idx = self._retrieve(0, s)
        data_idx = idx - self.capacity + 1
        return idx, self.tree[idx], self.data[data_idx]
    
    def _retrieve(self, idx, s):
        """æ£€ç´¢å¶èŠ‚ç‚¹"""
        left = 2 * idx + 1
        right = left + 1
        
        if left >= len(self.tree):
            return idx
        
        if s <= self.tree[left]:
            return self._retrieve(left, s)
        else:
            return self._retrieve(right, s - self.tree[left])
    
    @property
    def total_priority(self):
        return self.tree[0]

class PrioritizedReplayBuffer:
    """
    ä¼˜å…ˆçº§ç»éªŒå›æ”¾
    æ ¹æ®TDè¯¯å·®åˆ†é…é‡‡æ ·ä¼˜å…ˆçº§
    """
    def __init__(self, capacity, alpha=0.6, beta=0.4, beta_increment=0.001):
        self.tree = SumTree(capacity)
        self.capacity = capacity
        self.alpha = alpha  # ä¼˜å…ˆçº§æŒ‡æ•°
        self.beta = beta    # é‡è¦æ€§é‡‡æ ·æƒé‡
        self.beta_increment = beta_increment
        self.max_priority = 1.0
        self.epsilon = 0.01
    
    def push(self, state, action, reward, next_state, done):
        """æ·»åŠ ç»éªŒï¼ˆä½¿ç”¨æœ€å¤§ä¼˜å…ˆçº§ï¼‰"""
        data = Transition(state, action, reward, next_state, done)
        priority = self.max_priority
        self.tree.add(priority, data)
    
    def sample(self, batch_size):
        """æ ¹æ®ä¼˜å…ˆçº§é‡‡æ ·"""
        batch = []
        idxs = []
        priorities = []
        segment = self.tree.total_priority / batch_size
        
        # å¢åŠ Î²
        self.beta = min(1.0, self.beta + self.beta_increment)
        
        for i in range(batch_size):
            a = segment * i
            b = segment * (i + 1)
            s = random.uniform(a, b)
            
            idx, priority, data = self.tree.get(s)
            batch.append(data)
            idxs.append(idx)
            priorities.append(priority)
        
        # è®¡ç®—é‡è¦æ€§é‡‡æ ·æƒé‡
        sampling_probs = np.array(priorities) / self.tree.total_priority
        is_weights = np.power(self.tree.n_entries * sampling_probs, -self.beta)
        is_weights /= is_weights.max()
        
        # è½¬æ¢ä¸ºå¼ é‡
        transitions = Transition(*zip(*batch))
        states = torch.FloatTensor(transitions.state)
        actions = torch.LongTensor(transitions.action)
        rewards = torch.FloatTensor(transitions.reward)
        next_states = torch.FloatTensor(transitions.next_state)
        dones = torch.FloatTensor(transitions.done)
        is_weights = torch.FloatTensor(is_weights)
        
        return states, actions, rewards, next_states, dones, idxs, is_weights
    
    def update_priorities(self, idxs, td_errors):
        """æ›´æ–°ä¼˜å…ˆçº§"""
        for idx, td_error in zip(idxs, td_errors):
            priority = (abs(td_error) + self.epsilon) ** self.alpha
            self.max_priority = max(self.max_priority, priority)
            self.tree.update(idx, priority)
    
    def __len__(self):
        return self.tree.n_entries
```

---

## åã€é«˜çº§ç®—æ³•

### 10.1 PPO (Proximal Policy Optimization)

```python
class PPO:
    """
    PPOï¼šè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–
    
    ä½¿ç”¨è£å‰ªç›®æ ‡å‡½æ•°é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦
    L^CLIP(Î¸) = E[min(r_t(Î¸)A_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ)A_t)]
    
    å…¶ä¸­ r_t(Î¸) = Ï€_Î¸(a_t|s_t) / Ï€_Î¸_old(a_t|s_t)
    """
    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, 
                 epsilon=0.2, value_coef=0.5, entropy_coef=0.01,
                 gae_lambda=0.95, epochs=10, batch_size=64):
        
        self.actor_critic = ActorCriticNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)
        
        self.gamma = gamma
        self.epsilon = epsilon
        self.value_coef = value_coef
        self.entropy_coef = entropy_coef
        self.gae_lambda = gae_lambda
        self.epochs = epochs
        self.batch_size = batch_size
    
    def select_action(self, state):
        """é€‰æ‹©åŠ¨ä½œ"""
        state = torch.FloatTensor(state).unsqueeze(0)
        
        with torch.no_grad():
            action_probs, value = self.actor_critic(state)
            dist = Categorical(action_probs)
            action = dist.sample()
            log_prob = dist.log_prob(action)
        
        return action.item(), log_prob.item(), value.item()
    
    def compute_gae(self, rewards, values, dones):
        """
        è®¡ç®—GAE (Generalized Advantage Estimation)
        A_t = Î´_t + (Î³Î»)Î´_{t+1} + (Î³Î»)Â²Î´_{t+2} + ...
        å…¶ä¸­ Î´_t = r_t + Î³V(s_{t+1}) - V(s_t)
        """
        advantages = []
        gae = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]
            
            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]
            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae
            advantages.insert(0, gae)
        
        return advantages
    
    def update(self, states, actions, old_log_probs, returns, advantages):
        """PPOæ›´æ–°"""
        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        old_log_probs = torch.FloatTensor(old_log_probs)
        returns = torch.FloatTensor(returns)
        advantages = torch.FloatTensor(advantages)
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # å¤šæ¬¡æ›´æ–°
        for _ in range(self.epochs):
            # éšæœºæ‰“ä¹±
            indices = torch.randperm(len(states))
            
            for start in range(0, len(states), self.batch_size):
                end = start + self.batch_size
                batch_indices = indices[start:end]
                
                batch_states = states[batch_indices]
                batch_actions = actions[batch_indices]
                batch_old_log_probs = old_log_probs[batch_indices]
                batch_returns = returns[batch_indices]
                batch_advantages = advantages[batch_indices]
                
                # å‰å‘ä¼ æ’­
                action_probs, values = self.actor_critic(batch_states)
                dist = Categorical(action_probs)
                log_probs = dist.log_prob(batch_actions)
                entropy = dist.entropy().mean()
                
                # è®¡ç®—æ¯”ç‡
                ratio = torch.exp(log_probs - batch_old_log_probs)
                
                # PPOè£å‰ªç›®æ ‡
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * batch_advantages
                actor_loss = -torch.min(surr1, surr2).mean()
                
                # ä»·å€¼å‡½æ•°æŸå¤±
                critic_loss = F.mse_loss(values.squeeze(), batch_returns)
                
                # æ€»æŸå¤±
                loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy
                
                # æ›´æ–°
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.actor_critic.parameters(), 0.5)
                self.optimizer.step()
    
    def train(self, env, num_episodes=1000, max_steps=1000):
        """è®­ç»ƒPPO"""
        episode_rewards = []
        
        for episode in range(num_episodes):
            states, actions, rewards, log_probs, values, dones = [], [], [], [], [], []
            
            state = env.reset()
            
            for step in range(max_steps):
                action, log_prob, value = self.select_action(state)
                next_state, reward, done, _ = env.step(action)
                
                states.append(state)
                actions.append(action)
                rewards.append(reward)
                log_probs.append(log_prob)
                values.append(value)
                dones.append(done)
                
                state = next_state
                
                if done:
                    break
            
            # è®¡ç®—å›æŠ¥å’Œä¼˜åŠ¿
            advantages = self.compute_gae(rewards, values, dones)
            returns = [adv + val for adv, val in zip(advantages, values)]
            
            # æ›´æ–°ç­–ç•¥
            self.update(states, actions, log_probs, returns, advantages)
            
            episode_rewards.append(sum(rewards))
            
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(episode_rewards[-100:])
                print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}")
        
        return episode_rewards
```

### 10.2 DDPG (Deep Deterministic Policy Gradient)

```python
class DDPGActor(nn.Module):
    """DDPG Actorç½‘ç»œï¼ˆç¡®å®šæ€§ç­–ç•¥ï¼‰"""
    def __init__(self, state_dim, action_dim, max_action, hidden_dim=256):
        super(DDPGActor, self).__init__()
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Tanh()
        )
        
        self.max_action = max_action
    
    def forward(self, state):
        return self.max_action * self.network(state)

class DDPGCritic(nn.Module):
    """DDPG Criticç½‘ç»œï¼ˆQå‡½æ•°ï¼‰"""
    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super(DDPGCritic, self).__init__()
        
        self.network = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, state, action):
        return self.network(torch.cat([state, action], dim=-1))

class DDPG:
    """
    DDPGï¼šæ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦
    é€‚ç”¨äºè¿ç»­åŠ¨ä½œç©ºé—´
    
    å…³é”®æŠ€æœ¯ï¼š
    1. Actor-Criticæ¶æ„
    2. ç›®æ ‡ç½‘ç»œ
    3. ç»éªŒå›æ”¾
    4. OUå™ªå£°æ¢ç´¢
    """
    def __init__(self, state_dim, action_dim, max_action,
                 actor_lr=1e-4, critic_lr=1e-3, gamma=0.99, tau=0.005,
                 buffer_size=100000, batch_size=64):
        
        # Actorç½‘ç»œ
        self.actor = DDPGActor(state_dim, action_dim, max_action)
        self.actor_target = DDPGActor(state_dim, action_dim, max_action)
        self.actor_target.load_state_dict(self.actor.state_dict())
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)
        
        # Criticç½‘ç»œ
        self.critic = DDPGCritic(state_dim, action_dim)
        self.critic_target = DDPGCritic(state_dim, action_dim)
        self.critic_target.load_state_dict(self.critic.state_dict())
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)
        
        self.replay_buffer = ReplayBuffer(buffer_size)
        
        self.gamma = gamma
        self.tau = tau
        self.batch_size = batch_size
        self.max_action = max_action
    
    def select_action(self, state, noise=0.1):
        """é€‰æ‹©åŠ¨ä½œï¼ˆå¸¦å™ªå£°ï¼‰"""
        state = torch.FloatTensor(state).unsqueeze(0)
        action = self.actor(state).detach().numpy()[0]
        
        # æ·»åŠ æ¢ç´¢å™ªå£°
        if noise > 0:
            action += np.random.normal(0, noise, size=action.shape)
            action = np.clip(action, -self.max_action, self.max_action)
        
        return action
    
    def train_step(self):
        """è®­ç»ƒæ­¥éª¤"""
        if len(self.replay_buffer) < self.batch_size:
            return None, None
        
        # é‡‡æ ·
        states, actions, rewards, next_states, dones = \
            self.replay_buffer.sample(self.batch_size)
        
        # æ›´æ–°Critic
        with torch.no_grad():
            next_actions = self.actor_target(next_states)
            target_q = self.critic_target(next_states, next_actions)
            target_q = rewards.unsqueeze(1) + self.gamma * target_q * (1 - dones.unsqueeze(1))
        
        current_q = self.critic(states, actions)
        critic_loss = F.mse_loss(current_q, target_q)
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()
        
        # æ›´æ–°Actor
        actor_loss = -self.critic(states, self.actor(states)).mean()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.soft_update(self.actor, self.actor_target)
        self.soft_update(self.critic, self.critic_target)
        
        return actor_loss.item(), critic_loss.item()
    
    def soft_update(self, source, target):
        """è½¯æ›´æ–°ï¼šÎ¸' â† Ï„Î¸ + (1-Ï„)Î¸'"""
        for param, target_param in zip(source.parameters(), target.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
```

### 10.3 SAC (Soft Actor-Critic)

```python
class SAC:
    """
    SACï¼šè½¯Actor-Critic
    æœ€å¤§åŒ–ç†µæ­£åˆ™åŒ–çš„ç›®æ ‡ï¼šJ = E[Î£(r_t + Î±*H(Ï€(Â·|s_t)))]
    
    å…³é”®ç‰¹ç‚¹ï¼š
    1. æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ 
    2. è‡ªåŠ¨æ¸©åº¦è°ƒèŠ‚
    3. åŒQç½‘ç»œï¼ˆå‡å°‘è¿‡ä¼°è®¡ï¼‰
    """
    def __init__(self, state_dim, action_dim, max_action,
                 actor_lr=3e-4, critic_lr=3e-4, alpha_lr=3e-4,
                 gamma=0.99, tau=0.005, alpha=0.2, auto_entropy=True):
        
        # Actorç½‘ç»œï¼ˆè¾“å‡ºé«˜æ–¯åˆ†å¸ƒå‚æ•°ï¼‰
        self.actor = GaussianPolicy(state_dim, action_dim, max_action)
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)
        
        # åŒCriticç½‘ç»œ
        self.critic1 = DDPGCritic(state_dim, action_dim)
        self.critic2 = DDPGCritic(state_dim, action_dim)
        self.critic1_target = DDPGCritic(state_dim, action_dim)
        self.critic2_target = DDPGCritic(state_dim, action_dim)
        
        self.critic1_target.load_state_dict(self.critic1.state_dict())
        self.critic2_target.load_state_dict(self.critic2.state_dict())
        
        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=critic_lr)
        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=critic_lr)
        
        # æ¸©åº¦å‚æ•°
        self.alpha = alpha
        self.auto_entropy = auto_entropy
        if auto_entropy:
            self.target_entropy = -action_dim
            self.log_alpha = torch.zeros(1, requires_grad=True)
            self.alpha_optimizer = optim.Adam([self.log_alpha], lr=alpha_lr)
        
        self.gamma = gamma
        self.tau = tau
        self.replay_buffer = ReplayBuffer(100000)
        self.batch_size = 256
    
    def select_action(self, state, deterministic=False):
        """é€‰æ‹©åŠ¨ä½œ"""
        state = torch.FloatTensor(state).unsqueeze(0)
        
        if deterministic:
            _, _, action = self.actor.sample(state)
        else:
            action, _, _ = self.actor.sample(state)
        
        return action.detach().numpy()[0]
    
    def train_step(self):