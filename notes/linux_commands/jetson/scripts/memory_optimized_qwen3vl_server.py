#!/usr/bin/env python
# vllm_style_jetson_qwen3vl.py
import asyncio
import json
import logging
import time
import gc
import os
import threading
from typing import Dict, Any, List, Optional, Tuple, Union
from queue import Queue, Empty, PriorityQueue
from dataclasses import dataclass
import hashlib
from concurrent.futures import ThreadPoolExecutor
import numpy as np

import click
import torch
import torch.nn.functional as F
from torch.nn.attention import SDPBackend, sdpa_kernel
import uvicorn
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, Response
from modelscope import Qwen3VLForConditionalGeneration, AutoProcessor

# vLLM é£æ ¼çš„ç¯å¢ƒä¼˜åŒ–
os.environ.update({
    "CUDA_LAUNCH_BLOCKING": "0",
    "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:128,roundup_power2_divisions:16,garbage_collection_threshold:0.8",
    "CUDA_DEVICE_MAX_CONNECTIONS": "32",
    "NCCL_NVLS_ENABLE": "0",
    "CUDA_MODULE_LOADING": "LAZY",
    "TORCH_CUDNN_SDPA_ENABLED": "1"
})

# æè‡´æ€§èƒ½è®¾ç½®
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False
torch.backends.cudnn.enabled = True
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.set_float32_matmul_precision('high')  # ä½¿ç”¨æœ€é«˜ç²¾åº¦çš„ TF32

# ä¼˜åŒ–çº¿ç¨‹è®¾ç½®
torch.set_num_threads(8)
torch.set_num_interop_threads(4)

@dataclass
class GenerationRequest:
    """ç”Ÿæˆè¯·æ±‚"""
    request_id: str
    messages: List[Dict]
    max_tokens: int
    temperature: float
    priority: int
    created_time: float
    future: asyncio.Future
    
    def __lt__(self, other):
        return self.priority < other.priority

class VLLMStyleKVCache:
    """vLLM é£æ ¼çš„ KV ç¼“å­˜ç®¡ç†"""
    
    def __init__(self, max_blocks: int = 1000, block_size: int = 16):
        self.max_blocks = max_blocks
        self.block_size = block_size
        self.free_blocks = list(range(max_blocks))
        self.allocated_blocks = {}
        self.block_tables = {}
        self.lock = threading.Lock()
    
    def allocate_blocks(self, seq_id: str, num_blocks: int) -> List[int]:
        """ä¸ºåºåˆ—åˆ†é…å—"""
        with self.lock:
            if len(self.free_blocks) < num_blocks:
                # å›æ”¶æœ€è€çš„å—
                self._evict_oldest_blocks(num_blocks - len(self.free_blocks))
            
            allocated = []
            for _ in range(min(num_blocks, len(self.free_blocks))):
                block_id = self.free_blocks.pop(0)
                allocated.append(block_id)
            
            self.allocated_blocks[seq_id] = allocated
            self.block_tables[seq_id] = allocated.copy()
            return allocated
    
    def free_blocks(self, seq_id: str):
        """é‡Šæ”¾åºåˆ—çš„å—"""
        with self.lock:
            if seq_id in self.allocated_blocks:
                blocks = self.allocated_blocks.pop(seq_id)
                self.free_blocks.extend(blocks)
                self.block_tables.pop(seq_id, None)
    
    def _evict_oldest_blocks(self, num_blocks: int):
        """å›æ”¶æœ€è€çš„å—"""
        # ç®€å•çš„ LRU ç­–ç•¥
        if len(self.allocated_blocks) > 0:
            oldest_seq = list(self.allocated_blocks.keys())[0]
            self.free_blocks(oldest_seq)

class ContinuousBatchProcessor:
    """è¿ç»­æ‰¹å¤„ç†å™¨ - vLLM é£æ ¼"""
    
    def __init__(self, model, processor, max_batch_size: int = 8):
        self.model = model
        self.processor = processor
        self.max_batch_size = max_batch_size
        self.device = next(model.parameters()).device
        
        # è¯·æ±‚é˜Ÿåˆ—
        self.pending_requests = PriorityQueue()
        self.running_requests = {}
        self.completed_requests = {}
        
        # KV ç¼“å­˜ç®¡ç†
        self.kv_cache = VLLMStyleKVCache()
        
        # æ‰¹å¤„ç†çŠ¶æ€
        self.current_batch = []
        self.batch_lock = threading.Lock()
        
        # å¯åŠ¨è¿ç»­æ‰¹å¤„ç†å¾ªç¯
        self.processing_thread = threading.Thread(target=self._continuous_batching_loop, daemon=True)
        self.processing_thread.start()
        
        # é¢„ç¼–è¯‘çš„ç”Ÿæˆé…ç½®
        self.generation_configs = self._prepare_optimized_configs()
    
    def _prepare_optimized_configs(self):
        """é¢„å‡†å¤‡ä¼˜åŒ–çš„ç”Ÿæˆé…ç½®"""
        configs = {}
        
        # ä¸åŒåœºæ™¯çš„ä¼˜åŒ–é…ç½®
        configs['greedy'] = {
            "do_sample": False,
            "num_beams": 1,
            "use_cache": True,
            "pad_token_id": self.processor.tokenizer.eos_token_id,
            "repetition_penalty": 1.02,
            "length_penalty": 1.0,
            "early_stopping": True,
        }
        
        configs['sampling'] = {
            "do_sample": True,
            "num_beams": 1,
            "use_cache": True,
            "pad_token_id": self.processor.tokenizer.eos_token_id,
            "repetition_penalty": 1.02,
            "length_penalty": 1.0,
            "early_stopping": True,
            "top_p": 0.9,
            "top_k": 50,
        }
        
        return configs
    
    async def add_request(self, request: GenerationRequest) -> Dict[str, Any]:
        """æ·»åŠ è¯·æ±‚åˆ°é˜Ÿåˆ—"""
        # æ·»åŠ åˆ°å¾…å¤„ç†é˜Ÿåˆ—
        await asyncio.get_event_loop().run_in_executor(
            None, self.pending_requests.put, request
        )
        
        # ç­‰å¾…ç»“æœ
        try:
            result = await asyncio.wait_for(request.future, timeout=300)
            return result
        except asyncio.TimeoutError:
            # æ¸…ç†è¶…æ—¶è¯·æ±‚
            self.kv_cache.free_blocks(request.request_id)
            raise Exception("Request timeout")
    
    def _continuous_batching_loop(self):
        """è¿ç»­æ‰¹å¤„ç†ä¸»å¾ªç¯"""
        while True:
            try:
                # æ”¶é›†å¾…å¤„ç†è¯·æ±‚
                self._collect_pending_requests()
                
                # å¦‚æœæœ‰è¯·æ±‚éœ€è¦å¤„ç†
                if self.current_batch:
                    self._process_current_batch()
                else:
                    time.sleep(0.001)  # çŸ­æš‚ä¼‘çœ 
                    
            except Exception as e:
                logging.error(f"Continuous batching error: {e}")
                time.sleep(0.01)
    
    def _collect_pending_requests(self):
        """æ”¶é›†å¾…å¤„ç†çš„è¯·æ±‚"""
        with self.batch_lock:
            # ç§»é™¤å·²å®Œæˆçš„è¯·æ±‚
            self.current_batch = [req for req in self.current_batch 
                                if not req.future.done()]
            
            # æ·»åŠ æ–°è¯·æ±‚åˆ°å½“å‰æ‰¹æ¬¡
            while (len(self.current_batch) < self.max_batch_size and 
                   not self.pending_requests.empty()):
                try:
                    request = self.pending_requests.get_nowait()
                    self.current_batch.append(request)
                    self.running_requests[request.request_id] = request
                except Empty:
                    break
    
    def _process_current_batch(self):
        """å¤„ç†å½“å‰æ‰¹æ¬¡"""
        if not self.current_batch:
            return
        
        try:
            # æŒ‰è¯·æ±‚ç±»å‹åˆ†ç»„å¤„ç†
            text_only_requests = []
            vision_requests = []
            
            for request in self.current_batch:
                text_prompt, image_urls = self._extract_content(request.messages)
                if image_urls:
                    vision_requests.append(request)
                else:
                    text_only_requests.append(request)
            
            # ä¼˜å…ˆå¤„ç†çº¯æ–‡æœ¬è¯·æ±‚ï¼ˆæ›´å¿«ï¼‰
            if text_only_requests:
                self._process_text_batch(text_only_requests)
            
            # å¤„ç†è§†è§‰è¯·æ±‚
            if vision_requests:
                self._process_vision_batch(vision_requests)
                
        except Exception as e:
            logging.error(f"Batch processing error: {e}")
            # æ ‡è®°æ‰€æœ‰è¯·æ±‚ä¸ºå¤±è´¥
            for request in self.current_batch:
                if not request.future.done():
                    request.future.set_exception(e)
    
    def _process_vision_batch(self, requests: List[GenerationRequest]):
        """å¤„ç†è§†è§‰è¯·æ±‚æ‰¹æ¬¡"""
        if not requests:
            return
        
        # ä¸ºäº†ç®€åŒ–ï¼Œé€ä¸ªå¤„ç†è§†è§‰è¯·æ±‚ï¼ˆè§†è§‰æ¨¡å‹æ‰¹å¤„ç†è¾ƒå¤æ‚ï¼‰
        for request in requests:
            try:
                result = self._process_single_vision_request(request)
                if not request.future.done():
                    request.future.set_result(result)
            except Exception as e:
                if not request.future.done():
                    request.future.set_exception(e)
    
    def _process_single_vision_request(self, request: GenerationRequest) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªè§†è§‰è¯·æ±‚ - æè‡´ä¼˜åŒ–"""
        start_time = time.time()
        
        # æå–å†…å®¹
        text_prompt, image_urls = self._extract_content(request.messages)
        
        # æ„å»º Qwen æ ¼å¼æ¶ˆæ¯
        content_list = []
        for img_url in image_urls:
            content_list.append({"type": "image", "image": img_url})
        content_list.append({"type": "text", "text": text_prompt})
        qwen_messages = [{"role": "user", "content": content_list}]
        
        # å‡†å¤‡è¾“å…¥ - ä½¿ç”¨æµæ°´çº¿ä¼˜åŒ–
        inputs = self.processor.apply_chat_template(
            qwen_messages,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        ).to(self.device, non_blocking=True)
        
        # é€‰æ‹©ä¼˜åŒ–çš„ç”Ÿæˆé…ç½®
        if request.temperature <= 0.01:
            generation_kwargs = self.generation_configs['greedy'].copy()
        else:
            generation_kwargs = self.generation_configs['sampling'].copy()
            generation_kwargs['temperature'] = max(request.temperature, 0.1)
        
        generation_kwargs['max_new_tokens'] = min(request.max_tokens, 512)
        
        # ä½¿ç”¨ä¼˜åŒ–çš„ç”Ÿæˆç­–ç•¥
        with torch.no_grad():
            # å¯ç”¨æ‰€æœ‰å¯èƒ½çš„ä¼˜åŒ–
            with torch.cuda.amp.autocast(enabled=True):
                with sdpa_kernel(SDPBackend.FLASH_ATTENTION):  # ä½¿ç”¨ Flash Attention
                    generated_ids = self.model.generate(**inputs, **generation_kwargs)
        
        # å¿«é€Ÿè§£ç 
        input_length = inputs["input_ids"].shape[1]
        generated_ids_trimmed = generated_ids[0][input_length:]
        
        output_text = self.processor.tokenizer.decode(
            generated_ids_trimmed, 
            skip_special_tokens=True, 
            clean_up_tokenization_spaces=False
        )
        
        # ç«‹å³æ¸…ç†å†…å­˜
        del inputs, generated_ids
        torch.cuda.empty_cache()
        
        processing_time = time.time() - start_time
        
        return {
            "text": output_text,
            "prompt_tokens": input_length,
            "completion_tokens": len(generated_ids_trimmed),
            "total_tokens": input_length + len(generated_ids_trimmed),
            "processing_time": processing_time,
            "cache_hit": False
        }
    
    def _process_text_batch(self, requests: List[GenerationRequest]):
        """å¤„ç†çº¯æ–‡æœ¬è¯·æ±‚æ‰¹æ¬¡ï¼ˆå¯ä»¥çœŸæ­£æ‰¹å¤„ç†ï¼‰"""
        # è¿™é‡Œå¯ä»¥å®ç°çœŸæ­£çš„æ‰¹å¤„ç†
        # ä¸ºäº†ç®€åŒ–ï¼Œæš‚æ—¶é€ä¸ªå¤„ç†
        for request in requests:
            try:
                result = self._process_single_vision_request(request)
                if not request.future.done():
                    request.future.set_result(result)
            except Exception as e:
                if not request.future.done():
                    request.future.set_exception(e)
    
    def _extract_content(self, messages: List[Dict]) -> Tuple[str, List[str]]:
        """æå–æ¶ˆæ¯å†…å®¹"""
        text_parts = []
        image_urls = []
        
        for message in messages:
            if message.get("role") == "user":
                content = message.get("content", [])
                
                if isinstance(content, str):
                    text_parts.append(content)
                elif isinstance(content, list):
                    for item in content:
                        if item.get("type") == "text":
                            text_parts.append(item.get("text", ""))
                        elif item.get("type") == "image_url":
                            image_url_data = item.get("image_url", {})
                            if isinstance(image_url_data, dict):
                                image_urls.append(image_url_data.get("url", ""))
                            else:
                                image_urls.append(str(image_url_data))
                        elif item.get("type") == "image":
                            image_urls.append(item.get("image", ""))
        
        return " ".join(text_parts).strip(), image_urls

class VLLMStyleJetsonEngine:
    """vLLM é£æ ¼çš„ Jetson å¼•æ“"""
    
    def __init__(self, model_dir: str, dtype=torch.float16):
        self.model_dir = model_dir
        self.dtype = dtype
        
        logging.info(f"ğŸš€ Loading vLLM-Style Qwen3-VL for Jetson Orin")
        
        # GPU å†…å­˜ä¼˜åŒ–
        if torch.cuda.is_available():
            gpu_props = torch.cuda.get_device_properties(0)
            gpu_memory = gpu_props.total_memory / 1024**3
            logging.info(f"ğŸ“Š Jetson GPU: {gpu_props.name}, Memory: {gpu_memory:.1f} GB")
            
            # æ¿€è¿›çš„å†…å­˜è®¾ç½®
            torch.cuda.set_per_process_memory_fraction(0.9)
            torch.cuda.empty_cache()
        
        # æ¨¡å‹åŠ è½½ - vLLM é£æ ¼ä¼˜åŒ–
        model_kwargs = {
            "torch_dtype": dtype,
            "trust_remote_code": True,
            "low_cpu_mem_usage": True,
            "device_map": "cuda:0",
            "use_safetensors": True,
            "attn_implementation": "flash_attention_2",  # å°è¯•ä½¿ç”¨ Flash Attention
        }
        
        try:
            logging.info("ğŸ“¦ Loading model with Flash Attention...")
            self.model = Qwen3VLForConditionalGeneration.from_pretrained(
                model_dir, **model_kwargs
            )
        except Exception as e:
            logging.warning(f"Flash Attention failed: {e}")
            # å›é€€åˆ°æ ‡å‡†æ³¨æ„åŠ›
            model_kwargs.pop("attn_implementation", None)
            self.model = Qwen3VLForConditionalGeneration.from_pretrained(
                model_dir, **model_kwargs
            )
        
        logging.info("âœ… Model loaded successfully")
        
        # åŠ è½½å¤„ç†å™¨
        self.processor = AutoProcessor.from_pretrained(model_dir)
        self.device = self.model.device
        
        # åº”ç”¨ vLLM é£æ ¼ä¼˜åŒ–
        self._apply_vllm_optimizations()
        
        # åˆ›å»ºè¿ç»­æ‰¹å¤„ç†å™¨
        self.batch_processor = ContinuousBatchProcessor(
            self.model, self.processor, max_batch_size=4
        )
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_tokens_generated": 0,
            "total_latency": 0.0,
        }
        
        # é¢„çƒ­
        self._vllm_warmup()
        
        logging.info(f"âœ… vLLM-Style engine ready!")

    def _apply_vllm_optimizations(self):
        """åº”ç”¨ vLLM é£æ ¼ä¼˜åŒ–"""
        try:
            logging.info("ğŸ”¥ Applying vLLM-style optimizations...")
            
            # 1. æ¨¡å‹ç¼–è¯‘ - æœ€æ¿€è¿›æ¨¡å¼
            if hasattr(torch, 'compile'):
                try:
                    self.model = torch.compile(
                        self.model,
                        mode="max-autotune-no-cudagraphs",  # vLLM é£æ ¼ç¼–è¯‘
                        fullgraph=False,
                        dynamic=True  # æ”¯æŒåŠ¨æ€å½¢çŠ¶
                    )
                    logging.info("âœ… Model compiled with vLLM-style settings")
                except Exception as e:
                    logging.warning(f"vLLM-style compilation failed: {e}")
            
            # 2. è®¾ç½®è¯„ä¼°æ¨¡å¼
            self.model.eval()
            
            # 3. å†»ç»“å‚æ•°
            for param in self.model.parameters():
                param.requires_grad = False
            
            # 4. å¯ç”¨æ‰€æœ‰ç¼“å­˜ä¼˜åŒ–
            if hasattr(self.model.config, 'use_cache'):
                self.model.config.use_cache = True
            
            # 5. å¯ç”¨èåˆæ“ä½œ
            torch.backends.cuda.enable_flash_sdp(True)
            
            logging.info("âœ… vLLM-style optimizations applied")
            
        except Exception as e:
            logging.warning(f"Some vLLM optimizations failed: {e}")

    def _vllm_warmup(self):
        """vLLM é£æ ¼é¢„çƒ­"""
        try:
            logging.info("ğŸ”¥ vLLM-style warmup starting...")
            
            # é¢„çƒ­ä¸åŒé•¿åº¦çš„åºåˆ—
            warmup_lengths = [32, 64, 128, 256]
            
            for length in warmup_lengths:
                text = "Hello " * (length // 6)
                messages = [{"role": "user", "content": [{"type": "text", "text": text}]}]
                
                try:
                    inputs = self.processor.apply_chat_template(
                        messages,
                        tokenize=True,
                        add_generation_prompt=True,
                        return_dict=True,
                        return_tensors="pt"
                    ).to(self.device)
                    
                    with torch.no_grad():
                        with torch.cuda.amp.autocast(enabled=True):
                            _ = self.model.generate(
                                **inputs,
                                max_new_tokens=10,
                                do_sample=False,
                                use_cache=True,
                                pad_token_id=self.processor.tokenizer.eos_token_id,
                                num_beams=1
                            )
                    
                    del inputs
                    torch.cuda.empty_cache()
                    
                except Exception as e:
                    logging.warning(f"Warmup failed for length {length}: {e}")
            
            logging.info("âœ… vLLM-style warmup completed")
            
        except Exception as e:
            logging.warning(f"vLLM warmup failed: {e}")

    async def generate_async(self, messages: List[Dict], max_tokens: int = 128, 
                           temperature: float = 0.7) -> Dict[str, Any]:
        """å¼‚æ­¥ç”Ÿæˆ - vLLM é£æ ¼"""
        request_id = f"req_{int(time.time() * 1000000)}"
        future = asyncio.Future()
        
        request = GenerationRequest(
            request_id=request_id,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
            priority=1,  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ä¼˜å…ˆçº§
            created_time=time.time(),
            future=future
        )
        
        try:
            result = await self.batch_processor.add_request(request)
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats["successful_requests"] += 1
            self.stats["total_tokens_generated"] += result["completion_tokens"]
            self.stats["total_latency"] += result["processing_time"]
            
            return result
            
        except Exception as e:
            self.stats["failed_requests"] += 1
            raise e
        finally:
            self.stats["total_requests"] += 1

    async def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        avg_latency = 0.0
        if self.stats["successful_requests"] > 0:
            avg_latency = self.stats["total_latency"] / self.stats["successful_requests"]
        
        gpu_memory_info = {}
        if torch.cuda.is_available():
            gpu_memory_info = {
                "allocated_gb": torch.cuda.memory_allocated() / 1024**3,
                "reserved_gb": torch.cuda.memory_reserved() / 1024**3,
                "max_allocated_gb": torch.cuda.max_memory_allocated() / 1024**3,
            }
        
        return {
            "engine_info": {
                "model_dir": self.model_dir,
                "device": str(self.device),
                "precision": "FP16",
                "platform": "vLLM-Style Jetson Orin"
            },
            "runtime_stats": {
                "total_requests": self.stats["total_requests"],
                "successful_requests": self.stats["successful_requests"],
                "failed_requests": self.stats["failed_requests"],
                "total_tokens_generated": self.stats["total_tokens_generated"],
                "average_latency_ms": avg_latency * 1000,
                "tokens_per_second": self.stats["total_tokens_generated"] / max(self.stats["total_latency"], 0.001)
            },
            "memory_info": gpu_memory_info
        }

class VLLMStyleServer:
    """vLLM é£æ ¼æœåŠ¡å™¨"""
    
    def __init__(self, engine: VLLMStyleJetsonEngine):
        self.engine = engine
        self.app = FastAPI(
            title="vLLM-Style Jetson Qwen3-VL Server", 
            version="1.0.0",
            docs_url=None,
            redoc_url=None
        )
        self.register_routes()

    def register_routes(self):
        self.app.add_api_route("/stats", self.stats, methods=["GET"])
        self.app.add_api_route("/health", self.health, methods=["GET"])
        self.app.add_api_route("/v1/chat/completions", self.chat_completions, methods=["POST"])

    async def stats(self) -> Response:
        stats = await self.engine.get_stats()
        return JSONResponse(stats)

    async def health(self) -> Response:
        return Response(status_code=200)

    async def chat_completions(self, request: Request) -> Response:
        try:
            request_dict = await request.json()
            messages = request_dict.get("messages", [])
            max_tokens = min(request_dict.get("max_tokens", 128), 512)
            temperature = request_dict.get("temperature", 0.7)
            
            result = await self.engine.generate_async(messages, max_tokens, temperature)
            
            response_data = {
                "id": f"chatcmpl-{int(time.time())}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": "Qwen3-VL-4B-Instruct-vLLM-Style",
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": result["text"]
                    },
                    "finish_reason": "stop"
                }],
                "usage": {
                    "prompt_tokens": result["prompt_tokens"],
                    "completion_tokens": result["completion_tokens"],
                    "total_tokens": result["total_tokens"]
                },
                "processing_time": result["processing_time"]
            }
            
            return JSONResponse(response_data)
            
        except Exception as e:
            logging.error(f"Request failed: {e}")
            return JSONResponse({
                "error": {"message": str(e), "type": "internal_server_error"}
            }, status_code=500)

    async def __call__(self, host: str, port: int):
        config = uvicorn.Config(
            self.app, 
            host=host, 
            port=port, 
            log_level="warning",
            access_log=False,
            workers=1
        )
        await uvicorn.Server(config).serve()

@click.command()
@click.argument("model_dir")
@click.option("--host", type=str, default="0.0.0.0")
@click.option("--port", type=int, default=8001)
@click.option("--dtype", type=click.Choice(["float16", "bfloat16"]), default="float16")
def entrypoint(model_dir: str, host: str, port: int, dtype: str):
    """
    å¯åŠ¨ vLLM é£æ ¼çš„ Jetson Orin Qwen3-VL API æœåŠ¡å™¨
    """
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    dtype_map = {
        "float16": torch.float16,
        "bfloat16": torch.bfloat16,
    }
    torch_dtype = dtype_map[dtype]
    
    logging.info(f"ğŸš€ Starting vLLM-Style Jetson Qwen3-VL Server")
    logging.info(f"ğŸ“ Model: {model_dir}")
    logging.info(f"ğŸŒ Server: http://{host}:{port}")
    logging.info(f"ğŸ’¾ Precision: {dtype}")
    
    # åˆ›å»ºå¼•æ“
    engine = VLLMStyleJetsonEngine(
        model_dir=model_dir,
        dtype=torch_dtype
    )
    
    # åˆ›å»ºæœåŠ¡å™¨
    server = VLLMStyleServer(engine)
    
    # å¯åŠ¨æœåŠ¡å™¨
    asyncio.run(server(host, port))

if __name__ == "__main__":
    entrypoint()
