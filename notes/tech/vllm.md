## **ç»¼åˆæŒ‡å—ï¼švLLM ä»æ¦‚å¿µåˆ°ç”Ÿäº§éƒ¨ç½²**

### **ä¸€ã€vLLM æ˜¯ä»€ä¹ˆï¼Ÿ**

**vLLM** æ˜¯ä¸€ä¸ªä¸ºç”Ÿäº§ç¯å¢ƒè®¾è®¡çš„é«˜æ€§èƒ½ã€é«˜ååé‡çš„å¤§è¯­è¨€æ¨¡å‹ (LLM) **æ¨ç†å’ŒæœåŠ¡æ¡†æ¶**ã€‚å®ƒçš„æ ¸å¿ƒç›®æ ‡æ˜¯è®©å¤§æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹æ›´å¿«ã€æ›´èŠ‚çœæ˜¾å­˜ã€‚

#### **æ ¸å¿ƒç‰¹æ€§**

| ç‰¹æ€§ | æè¿° |
| :--- | :--- |
| **ğŸš€ é«˜ååé‡** | é€šè¿‡ PagedAttention å’Œè¿ç»­æ‰¹å¤„ç†æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡ GPU åˆ©ç”¨ç‡å’Œå¹¶å‘å¤„ç†èƒ½åŠ›ã€‚ |
| **ğŸ§  å†…å­˜æ•ˆç‡** | æå¤§å‡å°‘å†…å­˜ç¢ç‰‡ï¼Œå…è®¸åœ¨ç›¸åŒç¡¬ä»¶ä¸Šè¿è¡Œæ›´å¤§çš„æ¨¡å‹æˆ–å¤„ç†æ›´é•¿çš„åºåˆ—ã€‚ |
| **ğŸŒ OpenAI å…¼å®¹ API** | åªéœ€ä¸€è¡Œå‘½ä»¤ï¼Œå³å¯å°†ä»»ä½•å…¼å®¹æ¨¡å‹éƒ¨ç½²ä¸º OpenAI é£æ ¼çš„ API æœåŠ¡ï¼Œæ— ç¼å¯¹æ¥ç°æœ‰åº”ç”¨ã€‚ |
| **ğŸ§© å¹¿æ³›çš„æ¨¡å‹æ”¯æŒ** | æ”¯æŒåŒ…æ‹¬ Llama, Mixtral, InternVL, Qwen ç­‰åœ¨å†…çš„å¤§é‡ä¸»æµå¼€æºæ¨¡å‹ã€‚ |

---

### **äºŒã€æ ¸å¿ƒæŠ€æœ¯æ­ç§˜ (vLLM çš„â€œé­”æ³•â€)**

#### **1. PagedAttentionâ„¢**
è¿™æ˜¯ vLLM çš„å…³é”®åˆ›æ–°ï¼Œå®ƒå€Ÿé‰´äº†æ“ä½œç³»ç»Ÿä¸­**è™šæ‹Ÿå†…å­˜**å’Œ**åˆ†é¡µ**çš„æ€æƒ³æ¥ç®¡ç† Attention æœºåˆ¶ä¸­çš„ KV Cacheã€‚

*   **ä¼ ç»Ÿæ–¹å¼**: ä¸ºæ¯ä¸ªè¯·æ±‚é¢„å…ˆåˆ†é…ä¸€å—å·¨å¤§çš„ã€è¿ç»­çš„æ˜¾å­˜å—ï¼Œå¯¼è‡´å¤§é‡æµªè´¹å’Œç¢ç‰‡ã€‚
*   **vLLM æ–¹å¼**: å°† KV Cache å­˜å‚¨åœ¨éè¿ç»­çš„ã€åˆ†é¡µçš„â€œå†…å­˜å—â€ä¸­ã€‚
    *   **ä¼˜ç‚¹**: å‡ ä¹æ¶ˆé™¤äº†å†…å­˜ç¢ç‰‡ï¼Œæ˜¾å­˜åˆ©ç”¨ç‡å¯è¾¾ 90% ä»¥ä¸Šï¼Œè¿œè¶…ä¼ ç»Ÿæ–¹æ³•çš„ 60%ã€‚

#### **2. è¿ç»­æ‰¹å¤„ç† (Continuous Batching)**
vLLM å¯¹ä¼ å…¥çš„è¯·æ±‚è¿›è¡Œæ›´æ™ºèƒ½çš„æ‰¹å¤„ç†ã€‚

*   **ä¼ ç»Ÿæ‰¹å¤„ç†**: å¿…é¡»ç­‰å¾…æ‰¹æ¬¡ä¸­æ‰€æœ‰è¯·æ±‚éƒ½å¤„ç†å®Œæ¯•åï¼Œæ‰èƒ½å¼€å§‹ä¸‹ä¸€æ‰¹ã€‚
*   **vLLM æ–¹å¼**: è¯·æ±‚é˜Ÿåˆ—ä¸­çš„è¯·æ±‚ä¸€æ—¦å®Œæˆï¼Œç«‹å³å°†å…¶ç§»å‡ºï¼Œå¹¶åŠ¨æ€åœ°å°†æ–°è¯·æ±‚æ’å…¥åˆ°æ‰¹æ¬¡ä¸­ï¼Œè®© GPU æ°¸è¿œâ€œæœ‰æ´»å¹²â€ã€‚

---

### **ä¸‰. å®æˆ˜æ¼”ç»ƒï¼šéƒ¨ç½² InternVL å¤šæ¨¡æ€æ¨¡å‹**

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ã€ä»é›¶å¼€å§‹çš„éƒ¨ç½²æµç¨‹ã€‚

#### **æ­¥éª¤ 1ï¼šç¯å¢ƒå‡†å¤‡**

```bash
# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install vllm modelscope transformers torch

# éªŒè¯ vLLM å’Œ CUDA æ˜¯å¦å¯ç”¨
python -c "import vllm; print('vLLM installed successfully')"
python -c "import torch; print('CUDA available:', torch.cuda.is_available())"
```

#### **æ­¥éª¤ 2ï¼šä¸‹è½½æ¨¡å‹**
ä½¿ç”¨ `modelscope` ä»ç¤¾åŒºä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°æŒ‡å®šç›®å½•ã€‚

```bash
# ä¸‹è½½ InternVL2-8B æ¨¡å‹
modelscope download --model=OpenGVLab/InternVL2-8B --local_dir /path/to/your/models/InternVL2-8B
```
> **æç¤º**: å°† `/path/to/your/models/` æ›¿æ¢ä¸ºä½ è‡ªå·±çš„æ¨¡å‹å­˜å‚¨è·¯å¾„ã€‚

#### **æ­¥éª¤ 3ï¼šå¯åŠ¨ vLLM æœåŠ¡ (åå°è¿è¡Œ)**
è¿™æ˜¯æœ€å…³é”®çš„ä¸€æ­¥ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ `nohup` å‘½ä»¤è®©æœåŠ¡åœ¨åå°æŒç»­è¿è¡Œï¼Œå¹¶å°†æ—¥å¿—è¾“å‡ºåˆ°æ–‡ä»¶ã€‚

```bash
nohup vllm serve /path/to/your/models/InternVL2-8B \
    --served-model-name=InternVL2-8B \
    --api-key your-secret-key-123 \
    --tensor-parallel-size 2 \
    --gpu-memory-utilization 0.9 \
    --dtype half \
    --max-model-len 4096 \
    --trust_remote_code \
    --port 8000 > /path/to/your/logs/vllm.log 2>&1 &
```
**å‘½ä»¤è§£é‡Š:**
*   `nohup ... &`: `nohup` (No Hang Up) å’Œ `&` ç»„åˆï¼Œç¡®ä¿å³ä½¿ä½ å…³é—­ç»ˆç«¯ï¼ŒæœåŠ¡ä¹Ÿèƒ½åœ¨åå°ç»§ç»­è¿è¡Œã€‚
*   `> vllm.log 2>&1`: å°†æ ‡å‡†è¾“å‡º (`>`) å’Œæ ‡å‡†é”™è¯¯ (`2>&1`) éƒ½é‡å®šå‘åˆ° `vllm.log` æ–‡ä»¶ä¸­ã€‚

#### **æ­¥éª¤ 4ï¼šéªŒè¯æœåŠ¡çŠ¶æ€**

```bash
# å®æ—¶æŸ¥çœ‹æ—¥å¿—è¾“å‡º
tail -f /path/to/your/logs/vllm.log

# æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å­˜åœ¨
ps -ef | grep "vllm serve"
```
å½“ä½ åœ¨æ—¥å¿—ä¸­çœ‹åˆ° `Uvicorn running on http://...` å¹¶ä¸”æ²¡æœ‰é”™è¯¯æ—¶ï¼ŒæœåŠ¡å°±å¯åŠ¨æˆåŠŸäº†ã€‚

---

### **å››ã€`vllm serve` å‘½ä»¤å‚æ•°æ·±åº¦è§£æ**

| å‚æ•°åˆ†ç±» | å‚æ•° | ä½œç”¨ |
| :--- | :--- | :--- |
| **æ ¸å¿ƒå‚æ•°** | `[MODEL_PATH]` | **å¿…éœ€**ã€‚æŒ‡å‘åŒ…å«æ¨¡å‹æƒé‡å’Œé…ç½®çš„æœ¬åœ°ç›®å½•ã€‚ |
| | `--served-model-name` | å®šä¹‰åœ¨ API è¯·æ±‚ä¸­ä½¿ç”¨çš„æ¨¡å‹åç§°ã€‚ |
| | `--api-key` | è®¾ç½®ä¸€ä¸ª API è®¿é—®å¯†é’¥ï¼Œç”¨äºç®€å•çš„èº«ä»½éªŒè¯ã€‚ |
| | `--port` / `--host` | æŒ‡å®šæœåŠ¡ç›‘å¬çš„ç«¯å£å’Œä¸»æœºåœ°å€ã€‚ |
| | `--trust_remote_code` | **å¿…éœ€**ã€‚å…è®¸ vLLM æ‰§è¡Œæ¨¡å‹ä»“åº“ä¸­è‡ªå®šä¹‰çš„ Python ä»£ç ï¼Œå¯¹äºéæ ‡å‡†æ¨¡å‹ç»“æ„ï¼ˆå¦‚å¤šæ¨¡æ€ï¼‰è‡³å…³é‡è¦ã€‚ |
| **æ€§èƒ½ä¸å†…å­˜** | `--gpu-memory-utilization` | GPU æ˜¾å­˜ä½¿ç”¨ç‡ï¼Œå»ºè®®è®¾ç½®ä¸º `0.9` åˆ° `0.95`ã€‚ |
| | `--max-model-len` | æ¨¡å‹æ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦ (ä¸Šä¸‹æ–‡é•¿åº¦)ã€‚ |
| | `--max-num-seqs` | æœ€å¤§å¹¶å‘è¯·æ±‚æ•°ã€‚ |
| | `--max-num-batched-tokens`| ä¸€ä¸ªæ‰¹æ¬¡ä¸­å¤„ç†çš„æœ€å¤§ token æ€»æ•°ã€‚ |
| **å¹¶è¡ŒåŒ–** | `--tensor-parallel-size` | **å¼ é‡å¹¶è¡Œåº¦**ã€‚é€šå¸¸è®¾ç½®ä¸ºä½ çš„ GPU æ•°é‡ï¼Œç”¨äºå°†æ¨¡å‹åˆ‡åˆ†åˆ°å¤šå¼ å¡ä¸Šã€‚ |
| | `--pipeline-parallel-size`| **æµæ°´çº¿å¹¶è¡Œåº¦**ã€‚ç”¨äºæ›´å¤æ‚çš„è·¨èŠ‚ç‚¹å¹¶è¡Œã€‚ |
| **æ•°æ®ç±»å‹** | `--dtype` | è®¾ç½®è®¡ç®—ç²¾åº¦ã€‚`auto` (è‡ªåŠ¨é€‰æ‹©), `half` (fp16), `bfloat16` (bf16)ã€‚ |
| **é‡åŒ–** | `--quantization` | ä½¿ç”¨é‡åŒ–æ¨¡å‹ï¼Œå¦‚ `awq` æˆ– `gptq`ï¼Œä»¥å‡å°‘æ˜¾å­˜å ç”¨ã€‚ |

---

### **äº”ã€å¦‚ä½•è°ƒç”¨ä½ çš„ API æœåŠ¡**

ä¸€æ—¦æœåŠ¡å¯åŠ¨ï¼Œä½ å°±å¯ä»¥åƒè°ƒç”¨ OpenAI API ä¸€æ ·ä¸ä½ çš„æœ¬åœ°æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚

```python
from openai import OpenAI
import base64

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = OpenAI(
    base_url="http://localhost:8000/v1", # æ›¿æ¢ä¸ºä½ çš„æœåŠ¡å™¨ IP å’Œç«¯å£
    api_key="your-secret-key-123"      # æ›¿æ¢ä¸ºå¯åŠ¨æ—¶è®¾ç½®çš„ API Key
)

# --- åœºæ™¯ä¸€ï¼šçº¯æ–‡æœ¬èŠå¤© ---
print("--- Text Chat Example ---")
response = client.chat.completions.create(
    model="InternVL2-8B",  # ä½¿ç”¨ --served-model-name å®šä¹‰çš„åç§°
    messages=[
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
    ]
)
print(response.choices[0].message.content)


# --- åœºæ™¯äºŒï¼šå¤šæ¨¡æ€èŠå¤© (å›¾åƒ + æ–‡æœ¬) ---
print("\n--- Multimodal Chat Example ---")

# å°†æœ¬åœ°å›¾ç‰‡ç¼–ç ä¸º Base64
def image_to_base64(image_path):
    with open(image_path, "rb") as img_file:
        return base64.b64encode(img_file.read()).decode('utf-8')

image_path = "your_image.jpg" # æ›¿æ¢ä¸ºä½ è‡ªå·±çš„å›¾ç‰‡è·¯å¾„
base64_image = image_to_base64(image_path)

response = client.chat.completions.create(
    model="InternVL2-8B",
    messages=[{
        "role": "user",
        "content": [
            {"type": "text", "text": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
        ]
    }],
    max_tokens=1024
)
print(response.choices[0].message.content)
```

---

### **å…­ã€æ€§èƒ½è°ƒä¼˜ä¸æ•…éšœæ’æŸ¥**

| åœºæ™¯ | å»ºè®® |
| :--- | :--- |
| **æ˜¾å­˜ä¸è¶³ (CUDA out of memory)** | 1. é™ä½ `--gpu-memory-utilization` (å¦‚ `0.85`)ã€‚<br>2. å‡å° `--max-model-len`ã€‚<br>3. ä½¿ç”¨é‡åŒ–æ¨¡å‹ (`--quantization awq`)ã€‚ |
| **é«˜å¹¶å‘éœ€æ±‚** | é€‚å½“å¢åŠ  `--max-num-seqs` å’Œ `--max-num-batched-tokens` çš„å€¼ã€‚ |
| **ä½å»¶è¿Ÿéœ€æ±‚** | é€‚å½“å‡å° `--max-num-seqs`ï¼Œç¡®ä¿å•ä¸ªè¯·æ±‚èƒ½è¢«æ›´å¿«å¤„ç†ã€‚ |
| **æ¨¡å‹åŠ è½½å¤±è´¥** | 1. ç¡®è®¤æ¨¡å‹è·¯å¾„æ­£ç¡®ã€‚<br>2. ç¡®ä¿å·²æ·»åŠ  `--trust_remote_code` å‚æ•°ã€‚<br>3. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´ã€‚ |